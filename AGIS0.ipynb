{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Unified Cognitive Architecture — single drop-in cell (Colab)\n",
        "# If you restart runtime, just re-run this one cell.\n",
        "\n",
        "# ============ 0) Environment: pin a consistent PyTorch trio (CUDA 12.6) ============\n",
        "import sys, subprocess, textwrap, time, os, pathlib, json, math, random\n",
        "def _sh(cmd): subprocess.check_call(cmd, shell=True)\n",
        "# Install the PyTorch *trio* on the cu126 index to avoid version skew.\n",
        "_sh(\"pip -q install 'torch==2.8.*' 'torchvision==0.23.*' 'torchaudio==2.8.*' --index-url https://download.pytorch.org/whl/cu126\")\n",
        "\n",
        "# ============ 1) Write full module to uca.py ======================================\n",
        "from pathlib import Path\n",
        "module_src = r'''\n",
        "\"\"\"\n",
        "UNIFIED COGNITIVE ARCHITECTURE - Google Colab Edition (FIXED)\n",
        "===============================================================\n",
        "\n",
        "Complete production-ready cognitive system in a single module.\n",
        "\n",
        "Architecture:\n",
        "  INPUT → L0: Perception → L1: Representation → L2: Dynamics\n",
        "           ↓                                        ↓\n",
        "          L3: Memory ← L4: Meta-Controller ← Loop?\n",
        "\n",
        "This module preserves the structure of the notebook version while\n",
        "providing a programmatic interface that can be tested automatically.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from enum import IntEnum\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 2: LAYER 0 - ADAPTIVE PERCEPTION\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "class AdaptivePerception(nn.Module):\n",
        "    \"\"\"Resolution-aware perception front-end.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels: int = 3,\n",
        "        hidden_dim: int = 256,\n",
        "        resolutions: Optional[List[int]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if resolutions is None:\n",
        "            resolutions = [64, 256, 512]\n",
        "        self.resolutions = sorted(resolutions)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.encoders = nn.ModuleDict(\n",
        "            {\n",
        "                str(res): self._make_encoder(input_channels, hidden_dim)\n",
        "                for res in self.resolutions\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Budget thresholds (0-10 scale)\n",
        "        self.budget_thresholds = torch.linspace(0, 10, len(self.resolutions) + 1)[1:]\n",
        "\n",
        "    def _make_encoder(self, in_channels: int, hidden_dim: int) -> nn.Module:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def select_resolution(self, budget: float) -> int:\n",
        "        budget = torch.clamp(torch.tensor(budget), 0, 10)\n",
        "        for res, thresh in zip(self.resolutions, self.budget_thresholds):\n",
        "            if budget <= thresh:\n",
        "                return res\n",
        "        return self.resolutions[-1]\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        budget: float = 5.0,\n",
        "        return_info: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "        resolution = self.select_resolution(budget)\n",
        "\n",
        "        if x.shape[-1] != resolution:\n",
        "            x = F.interpolate(\n",
        "                x,\n",
        "                size=(resolution, resolution),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "\n",
        "        encoded = self.encoders[str(resolution)](x)\n",
        "\n",
        "        if return_info:\n",
        "            info = {\n",
        "                \"resolution\": resolution,\n",
        "                \"budget\": float(budget),\n",
        "                \"speedup\": self.resolutions[-1] / resolution,\n",
        "            }\n",
        "            return encoded, info\n",
        "        return encoded, {}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 3: LAYER 1 - SET TRANSFORMER REPRESENTATION\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "class SetTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int = 256,\n",
        "        hidden_dim: int = 256,\n",
        "        num_heads: int = 8,\n",
        "        num_layers: int = 4,\n",
        "        dropout: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.pool_query = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        B, N, _ = x.shape\n",
        "        x = self.input_proj(x)\n",
        "        transformed = self.transformer(x, src_key_padding_mask=mask)\n",
        "\n",
        "        query = self.pool_query.expand(B, 1, -1)\n",
        "        pooled = torch.matmul(query, transformed.transpose(1, 2))\n",
        "\n",
        "        if mask is not None:\n",
        "            pooled = pooled.masked_fill(mask.unsqueeze(1), -1e9)\n",
        "\n",
        "        weights = torch.softmax(pooled, dim=-1)\n",
        "        aggregated = torch.matmul(weights, transformed)\n",
        "        return self.output_proj(aggregated.squeeze(1))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 4: LAYER 2 - ACTIVE INFERENCE DYNAMICS\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "class ActiveInferenceModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int = 256,\n",
        "        hidden_dim: int = 256,\n",
        "        latent_dim: int = 64,\n",
        "        num_layers: int = 2,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.prior_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim * 2),\n",
        "        )\n",
        "\n",
        "        self.posterior_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim * 2),\n",
        "        )\n",
        "\n",
        "        self.generative_net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, obs_dim),\n",
        "        )\n",
        "\n",
        "        self.dynamics = nn.LSTM(latent_dim, latent_dim, num_layers, batch_first=True)\n",
        "        self.output_proj = nn.Linear(latent_dim, latent_dim)\n",
        "\n",
        "    def encode(self, obs: torch.Tensor, use_prior: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        net = self.prior_net if use_prior else self.posterior_net\n",
        "        params = net(obs)\n",
        "        mean, logvar = torch.chunk(params, 2, dim=-1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def decode(self, latent: torch.Tensor) -> torch.Tensor:\n",
        "        return self.generative_net(latent)\n",
        "\n",
        "    def compute_free_energy(\n",
        "        self,\n",
        "        obs: torch.Tensor,\n",
        "        post_mean: torch.Tensor,\n",
        "        post_logvar: torch.Tensor,\n",
        "        prior_mean: torch.Tensor,\n",
        "        prior_logvar: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        latent = self.reparameterize(post_mean, post_logvar)\n",
        "        recon = self.decode(latent)\n",
        "        accuracy = -torch.mean((obs - recon) ** 2, dim=-1)\n",
        "\n",
        "        complexity = -0.5 * torch.sum(\n",
        "            1\n",
        "            + post_logvar\n",
        "            - prior_logvar\n",
        "            - ((post_mean - prior_mean) ** 2 + torch.exp(post_logvar))\n",
        "            / torch.exp(prior_logvar),\n",
        "            dim=-1,\n",
        "        )\n",
        "        return (complexity - accuracy).mean()\n",
        "\n",
        "    def forward(self, obs: torch.Tensor, num_iterations: int = 10) -> Dict[str, object]:\n",
        "        prior_mean, prior_logvar = self.encode(obs, use_prior=True)\n",
        "\n",
        "        free_energies: List[float] = []\n",
        "        latent_history: List[torch.Tensor] = []\n",
        "        current_latent = self.reparameterize(prior_mean, prior_logvar)\n",
        "\n",
        "        for _ in range(num_iterations):\n",
        "            post_mean, post_logvar = self.encode(obs)\n",
        "            fe = self.compute_free_energy(obs, post_mean, post_logvar, prior_mean, prior_logvar)\n",
        "            free_energies.append(float(fe.item()))\n",
        "            current_latent = self.reparameterize(post_mean, post_logvar)\n",
        "            latent_history.append(current_latent)\n",
        "            prior_mean = 0.9 * prior_mean + 0.1 * post_mean\n",
        "            prior_logvar = 0.9 * prior_logvar + 0.1 * post_logvar\n",
        "\n",
        "        if len(latent_history) > 1:\n",
        "            latent_seq = torch.stack(latent_history, dim=1)\n",
        "            _, (h_n, _) = self.dynamics(latent_seq)\n",
        "            final_latent = self.output_proj(h_n[-1])\n",
        "        else:\n",
        "            final_latent = self.output_proj(current_latent)\n",
        "\n",
        "        return {\n",
        "            \"latent\": final_latent,\n",
        "            \"free_energy\": free_energies[-1],\n",
        "            \"free_energy_history\": free_energies,\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 5: LAYER 3 - MEMORY SYSTEM\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "class TitansWorkingMemory(nn.Module):\n",
        "    def __init__(self, hidden_dim: int = 256, num_slots: int = 1024, lr: float = 0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_slots = num_slots\n",
        "        self.lr = lr\n",
        "\n",
        "        self.register_buffer(\"memory\", torch.zeros(num_slots, hidden_dim))\n",
        "        self.register_buffer(\"access_count\", torch.zeros(num_slots))\n",
        "\n",
        "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.slot_idx = 0\n",
        "\n",
        "    def forward(self, query: torch.Tensor, update: bool = False) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "        B = query.shape[0]\n",
        "\n",
        "        Q = self.query_proj(query)\n",
        "        scores = torch.matmul(Q, self.memory.T) / (self.hidden_dim ** 0.5)\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, self.memory)\n",
        "\n",
        "        if update:\n",
        "            V = self.value_proj(query)\n",
        "            for i in range(B):\n",
        "                if self.slot_idx < self.num_slots:\n",
        "                    slot = self.slot_idx\n",
        "                    self.slot_idx += 1\n",
        "                else:\n",
        "                    slot = self.access_count.argmin().item()\n",
        "\n",
        "                self.memory[slot] = self.memory[slot] * (1 - self.lr) + V[i] * self.lr\n",
        "                self.access_count[slot] += 1\n",
        "\n",
        "        info = {\n",
        "            \"memory_usage\": self.slot_idx / self.num_slots,\n",
        "            \"avg_access\": float(self.access_count.mean().item()),\n",
        "        }\n",
        "        return output, info\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.memory.zero_()\n",
        "        self.access_count.zero_()\n",
        "        self.slot_idx = 0\n",
        "\n",
        "\n",
        "class MAPElitesArchive:\n",
        "    def __init__(self, descriptor_dim: int = 2, grid_bins: int = 10) -> None:\n",
        "        self.descriptor_dim = descriptor_dim\n",
        "        self.grid_bins = grid_bins\n",
        "        self.archive: Dict[Tuple[int, ...], Dict[str, object]] = {}\n",
        "        self.descriptor_min: Optional[np.ndarray] = None\n",
        "        self.descriptor_max: Optional[np.ndarray] = None\n",
        "\n",
        "    def _discretize(self, descriptor: np.ndarray) -> Tuple[int, ...]:\n",
        "        if self.descriptor_min is None:\n",
        "            self.descriptor_min = descriptor.copy()\n",
        "            self.descriptor_max = descriptor.copy()\n",
        "        else:\n",
        "            self.descriptor_min = np.minimum(self.descriptor_min, descriptor)\n",
        "            self.descriptor_max = np.maximum(self.descriptor_max, descriptor)\n",
        "\n",
        "        ranges = self.descriptor_max - self.descriptor_min\n",
        "        ranges = np.where(ranges == 0, 1, ranges)\n",
        "        normalized = (descriptor - self.descriptor_min) / ranges\n",
        "        bins = (normalized * (self.grid_bins - 1)).astype(int)\n",
        "        bins = np.clip(bins, 0, self.grid_bins - 1)\n",
        "        return tuple(bins)\n",
        "\n",
        "    def add(self, solution: torch.Tensor, fitness: float, descriptor: np.ndarray) -> bool:\n",
        "        cell = self._discretize(descriptor)\n",
        "        if cell not in self.archive or fitness > self.archive[cell][\"fitness\"]:\n",
        "            self.archive[cell] = {\n",
        "                \"solution\": solution.detach().cpu(),\n",
        "                \"fitness\": fitness,\n",
        "                \"descriptor\": descriptor,\n",
        "            }\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def get_statistics(self) -> Dict[str, float]:\n",
        "        if len(self.archive) == 0:\n",
        "            return {\"size\": 0, \"coverage\": 0.0, \"avg_fitness\": 0.0}\n",
        "\n",
        "        fitnesses = [entry[\"fitness\"] for entry in self.archive.values()]\n",
        "        return {\n",
        "            \"size\": len(self.archive),\n",
        "            \"coverage\": len(self.archive) / (self.grid_bins ** self.descriptor_dim),\n",
        "            \"avg_fitness\": float(np.mean(fitnesses)),\n",
        "            \"max_fitness\": float(np.max(fitnesses)),\n",
        "        }\n",
        "\n",
        "    def retrieve(self, query_descriptor: np.ndarray, k: int = 5):\n",
        "        if len(self.archive) == 0:\n",
        "            return []\n",
        "        descs, cells = [], []\n",
        "        for cell, entry in self.archive.items():\n",
        "            descs.append(entry[\"descriptor\"])\n",
        "            cells.append(cell)\n",
        "        descs = np.stack(descs, axis=0)\n",
        "        dists = np.linalg.norm(descs - query_descriptor[None, :], axis=1)\n",
        "        order = np.argsort(dists)[:k]\n",
        "        return [self.archive[cells[i]] for i in order]\n",
        "\n",
        "\n",
        "class MemorySystem(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int = 256,\n",
        "        working_slots: int = 1024,\n",
        "        archive_bins: int = 10,\n",
        "        descriptor_dim: int = 2,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.working_memory = TitansWorkingMemory(hidden_dim, working_slots)\n",
        "        self.long_term_memory = MAPElitesArchive(descriptor_dim, archive_bins)\n",
        "        self.descriptor_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, descriptor_dim),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def compute_descriptor(self, latent: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"Project a latent vector into descriptor space.\n",
        "\n",
        "        Supports both batched and 1D inputs.\n",
        "        \"\"\"\n",
        "        if latent.dim() == 1:\n",
        "            latent = latent.unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            desc = self.descriptor_net(latent)\n",
        "        return desc.cpu().numpy()\n",
        "\n",
        "    def forward(\n",
        "        self, query: torch.Tensor, update_working: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "        return self.working_memory(query, update=update_working)\n",
        "\n",
        "    def store_solution(self, solution: torch.Tensor, fitness: float) -> bool:\n",
        "        descriptor = self.compute_descriptor(solution)\n",
        "        return self.long_term_memory.add(solution, float(fitness), descriptor[0])\n",
        "\n",
        "    def retrieve_from_archive(self, query_latent: torch.Tensor, k: int = 5):\n",
        "        if query_latent.dim() == 1:\n",
        "            query_latent = query_latent.unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            qd = self.descriptor_net(query_latent).cpu().numpy()\n",
        "        return self.long_term_memory.retrieve(qd[0], k=k)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 6: LAYER 4 - META-CONTROLLER\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "class Action(IntEnum):\n",
        "    THINK = 0\n",
        "    RETRIEVE = 1\n",
        "    PERCEIVE_UP = 2\n",
        "    PERCEIVE_DOWN = 3\n",
        "    VERIFY = 4\n",
        "    STORE = 5\n",
        "    EXIT = 6\n",
        "\n",
        "\n",
        "class MetaController(nn.Module):\n",
        "    def __init__(self, state_dim: int = 512, hidden_dim: int = 256, num_actions: int = 7) -> None:\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.policy_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_actions),\n",
        "        )\n",
        "\n",
        "        self.value_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "    def encode_state(\n",
        "        self,\n",
        "        task_embedding: torch.Tensor,\n",
        "        progress: float,\n",
        "        budget: float,\n",
        "        confidence: float,\n",
        "        memory_usage: float,\n",
        "        iteration: int,\n",
        "    ) -> torch.Tensor:\n",
        "        B = task_embedding.shape[0]\n",
        "        scalars = torch.tensor(\n",
        "            [progress, budget / 10.0, confidence, memory_usage, iteration / 50.0],\n",
        "            device=task_embedding.device,\n",
        "        ).unsqueeze(0)\n",
        "        scalars = scalars.expand(B, -1)\n",
        "\n",
        "        state = torch.cat([task_embedding, scalars], dim=-1)\n",
        "        if state.shape[-1] < self.state_dim:\n",
        "            padding = torch.zeros(B, self.state_dim - state.shape[-1], device=state.device)\n",
        "            state = torch.cat([state, padding], dim=-1)\n",
        "        return state[:, : self.state_dim]\n",
        "\n",
        "    def forward(self, state: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        logits = self.policy_net(state)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        if deterministic:\n",
        "            actions = torch.argmax(probs, dim=-1)\n",
        "            log_probs = torch.log(probs.gather(1, actions.unsqueeze(1))).squeeze(1)\n",
        "        else:\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            actions = dist.sample()\n",
        "            log_probs = dist.log_prob(actions)\n",
        "        return actions, log_probs\n",
        "\n",
        "    def get_value(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        return self.value_net(state).squeeze(-1)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 7: UNIFIED SYSTEM (FIXED)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "class UnifiedCognitiveSystem(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels: int = 3,\n",
        "        obs_dim: int = 256,\n",
        "        hidden_dim: int = 256,\n",
        "        latent_dim: int = 64,\n",
        "        resolutions: Optional[List[int]] = None,\n",
        "        memory_slots: int = 1024,\n",
        "        archive_bins: int = 10,\n",
        "        use_meta_controller: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if resolutions is None:\n",
        "            resolutions = [64, 256, 512]\n",
        "\n",
        "        self.obs_dim = obs_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.use_meta_controller = use_meta_controller\n",
        "\n",
        "        self.perception = AdaptivePerception(input_channels, obs_dim, resolutions)\n",
        "        self.representation = SetTransformer(obs_dim, hidden_dim, num_heads=8, num_layers=4)\n",
        "        self.dynamics = ActiveInferenceModule(hidden_dim, hidden_dim, latent_dim)\n",
        "        self.memory = MemorySystem(latent_dim, memory_slots, archive_bins, descriptor_dim=2)\n",
        "\n",
        "        if use_meta_controller:\n",
        "            self.meta_controller = MetaController(state_dim=512, hidden_dim=256)\n",
        "\n",
        "        self.output_proj = nn.Linear(latent_dim, obs_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        inputs: Dict[str, torch.Tensor],\n",
        "        budget: float = 5.0,\n",
        "        num_iterations: int = 10,\n",
        "        update_memory: bool = True,\n",
        "        return_info: bool = True,\n",
        "    ) -> Dict[str, object]:\n",
        "        info: Dict[str, object] = {}\n",
        "\n",
        "        # Perception for image if present\n",
        "        if \"image\" in inputs:\n",
        "            perceived, perc_info = self.perception(inputs[\"image\"], budget, return_info=True)\n",
        "            info[\"perception\"] = perc_info\n",
        "        else:\n",
        "            # Fallback: use the first provided modality directly\n",
        "            perceived = next(iter(inputs.values()))\n",
        "            info[\"perception\"] = {\"modalities\": len(inputs)}\n",
        "\n",
        "        # Treat available modalities as a set (image + others), projecting to obs_dim if needed\n",
        "        set_elems = [perceived]\n",
        "        for k, v in inputs.items():\n",
        "            if k == \"image\":\n",
        "                continue\n",
        "            t = v\n",
        "            if t.dim() == 1:\n",
        "                t = t.unsqueeze(0)\n",
        "            if t.shape[-1] != self.obs_dim:\n",
        "                proj = getattr(self, f\"_proj_{k}\", None)\n",
        "                if proj is None:\n",
        "                    proj = nn.Linear(t.shape[-1], self.obs_dim).to(t.device)\n",
        "                    setattr(self, f\"_proj_{k}\", proj)\n",
        "                t = proj(t)\n",
        "            set_elems.append(t)\n",
        "        perceived_set = torch.stack(set_elems, dim=1)  # [B, N, obs_dim]\n",
        "\n",
        "        represented = self.representation(perceived_set)\n",
        "        info[\"representation\"] = {\"hidden_dim\": represented.shape[-1], \"set_size\": perceived_set.shape[1]}\n",
        "\n",
        "        dynamics_result = self.dynamics(represented, num_iterations)\n",
        "        latent = dynamics_result[\"latent\"]\n",
        "        info[\"dynamics\"] = {\n",
        "            \"free_energy\": dynamics_result[\"free_energy\"],\n",
        "            \"iterations\": num_iterations,\n",
        "        }\n",
        "\n",
        "        memory_out, memory_info = self.memory(latent, update_working=update_memory)\n",
        "        info[\"memory\"] = memory_info\n",
        "\n",
        "        combined = latent + memory_out\n",
        "        output = self.output_proj(combined)\n",
        "\n",
        "        result: Dict[str, object] = {\"output\": output}\n",
        "        if return_info:\n",
        "            result[\"info\"] = info\n",
        "            result[\"latent\"] = latent\n",
        "        return result\n",
        "\n",
        "    def store_in_archive(self, solution: torch.Tensor, fitness: float) -> bool:\n",
        "        return self.memory.store_solution(solution, fitness)\n",
        "\n",
        "    def get_statistics(self) -> Dict[str, float]:\n",
        "        archive_stats = self.memory.long_term_memory.get_statistics()\n",
        "        return {\n",
        "            \"perception_resolutions\": self.perception.resolutions,\n",
        "            \"archive_size\": archive_stats[\"size\"],\n",
        "            \"archive_coverage\": archive_stats[\"coverage\"],\n",
        "            \"avg_fitness\": archive_stats.get(\"avg_fitness\", 0.0),\n",
        "            \"hidden_dim\": self.hidden_dim,\n",
        "            \"latent_dim\": self.latent_dim,\n",
        "        }\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration presets for different operating regimes.\"\"\"\n",
        "\n",
        "    input_channels: int = 3\n",
        "    obs_dim: int = 256\n",
        "    hidden_dim: int = 256\n",
        "    latent_dim: int = 64\n",
        "    resolutions: Optional[List[int]] = None\n",
        "    memory_slots: int = 1024\n",
        "    archive_bins: int = 10\n",
        "    use_meta_controller: bool = False\n",
        "\n",
        "    @staticmethod\n",
        "    def mvp() -> Dict[str, object]:\n",
        "        return {\n",
        "            \"input_channels\": 3,\n",
        "            \"obs_dim\": 64,\n",
        "            \"hidden_dim\": 128,\n",
        "            \"latent_dim\": 32,\n",
        "            \"resolutions\": [64, 256],\n",
        "            \"memory_slots\": 256,\n",
        "            \"archive_bins\": 10,\n",
        "            \"use_meta_controller\": False,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def production() -> Dict[str, object]:\n",
        "        return {\n",
        "            \"input_channels\": 3,\n",
        "            \"obs_dim\": 256,\n",
        "            \"hidden_dim\": 256,\n",
        "            \"latent_dim\": 64,\n",
        "            \"resolutions\": [64, 256, 512],\n",
        "            \"memory_slots\": 1024,\n",
        "            \"archive_bins\": 20,\n",
        "            \"use_meta_controller\": False,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def research() -> Dict[str, object]:\n",
        "        return {\n",
        "            \"input_channels\": 3,\n",
        "            \"obs_dim\": 512,\n",
        "            \"hidden_dim\": 512,\n",
        "            \"latent_dim\": 128,\n",
        "            \"resolutions\": [64, 256, 512, 1024],\n",
        "            \"memory_slots\": 4096,\n",
        "            \"archive_bins\": 50,\n",
        "            \"use_meta_controller\": False,\n",
        "        }\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"AdaptivePerception\",\n",
        "    \"SetTransformer\",\n",
        "    \"ActiveInferenceModule\",\n",
        "    \"TitansWorkingMemory\",\n",
        "    \"MAPElitesArchive\",\n",
        "    \"MemorySystem\",\n",
        "    \"MetaController\",\n",
        "    \"UnifiedCognitiveSystem\",\n",
        "    \"Config\",\n",
        "    \"Action\",\n",
        "]\n",
        "'''\n",
        "Path(\"uca.py\").write_text(module_src)\n",
        "\n",
        "# ============ 2) Import + quick smoke ==========================================\n",
        "import torch, numpy as np, time\n",
        "from uca import UnifiedCognitiveSystem\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device, \"| torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda)\n",
        "\n",
        "cfg = dict(input_channels=3, obs_dim=256, hidden_dim=256, latent_dim=64, resolutions=[64,256])\n",
        "sysmod = UnifiedCognitiveSystem(**cfg).to(device).eval()\n",
        "\n",
        "# multimodal toy inputs\n",
        "B = 2\n",
        "inputs = {\n",
        "    \"image\": torch.randn(B,3,224,224, device=device),\n",
        "    \"text\":  torch.randn(B,256, device=device),\n",
        "    \"audio\": torch.randn(B,256, device=device),\n",
        "}\n",
        "\n",
        "# forward and iteration sweep (test-time scaling)\n",
        "for iters in [1, 5, 10, 20]:\n",
        "    t0=time.time()\n",
        "    out = sysmod.forward(inputs, budget=5.0, num_iterations=iters, update_memory=True, return_info=True)\n",
        "    ms = (time.time()-t0)*1000\n",
        "    fe = out[\"info\"][\"dynamics\"][\"free_energy\"]\n",
        "    set_sz = out[\"info\"][\"representation\"][\"set_size\"]\n",
        "    print(f\"{iters:>2} iters | {ms:6.1f} ms | FE {fe:8.4f} | set_size={set_sz}\")\n",
        "\n",
        "# archive round-trip\n",
        "latent = out[\"latent\"].detach()\n",
        "stored = sysmod.store_in_archive(latent[0], fitness=float(np.random.rand()))\n",
        "neighbors = sysmod.memory.retrieve_from_archive(latent[0], k=3)\n",
        "print(f\"Archive store: {stored} | retrieved: {len(neighbors)} | stats:\", sysmod.get_statistics())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "fCDbEh-vGfy1",
        "outputId": "41a603fd-e4a8-452f-f6bc-ebe8abe2bd61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'UnifiedCognitiveSystem' from 'uca' (/content/uca.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1529534155.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;31m# ============ 2) Import + quick smoke ==========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0muca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnifiedCognitiveSystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'UnifiedCognitiveSystem' from 'uca' (/content/uca.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}