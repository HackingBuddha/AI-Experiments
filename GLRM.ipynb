{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":212576,"status":"ok","timestamp":1758210894621,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"},"user_tz":-180},"id":"Nh-BkMKYhbxv","outputId":"77dbd371-fc99-45ad-fe25-bbc92d40b131"},"outputs":[{"name":"stdout","output_type":"stream","text":["FAISS available: False | GPU support: False\n","Using device: cuda\n","CUDA available: True\n","GPU: NVIDIA A100-SXM4-40GB\n","Output directory: graph_world_runs/run_sparse_knn_vec_faiss\n","FAISS: available=False, GPU=False\n","\n","=== Episode 1/30 ===\n","  Step 10/50 | Loss: 0.000179 | Memory: 640 nodes, 4608 edges | Time: 0.039s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000090 | Memory: 1280 nodes, 9728 edges | Time: 0.041s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000057 | Memory: 1920 nodes, 14848 edges | Time: 0.044s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000037 | Memory: 2560 nodes, 19968 edges | Time: 0.043s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000033 | Memory: 3200 nodes, 25088 edges | Time: 0.045s | GPU: 0.02G/0.02G\n","Episode 1 complete:\n","  Mean loss: 0.000187\n","  Memory: 3200 nodes, 25088 edges\n","  Time: 3.4s\n","  Checkpoint saved\n","\n","=== Episode 2/30 ===\n","  Step 10/50 | Loss: 0.000031 | Memory: 3840 nodes, 30208 edges | Time: 0.046s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000036 | Memory: 4480 nodes, 35328 edges | Time: 0.047s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000011 | Memory: 5120 nodes, 40448 edges | Time: 0.050s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000020 | Memory: 5760 nodes, 45568 edges | Time: 0.051s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000026 | Memory: 6400 nodes, 50688 edges | Time: 0.052s | GPU: 0.02G/0.02G\n","Episode 2 complete:\n","  Mean loss: 0.000028\n","  Memory: 6400 nodes, 50688 edges\n","  Time: 2.4s\n","  Checkpoint saved\n","\n","=== Episode 3/30 ===\n","  Step 10/50 | Loss: 0.000035 | Memory: 7040 nodes, 55808 edges | Time: 0.053s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000016 | Memory: 7680 nodes, 60928 edges | Time: 0.055s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000019 | Memory: 8320 nodes, 66048 edges | Time: 0.056s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000018 | Memory: 8960 nodes, 71168 edges | Time: 0.056s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000022 | Memory: 9600 nodes, 76288 edges | Time: 0.060s | GPU: 0.02G/0.02G\n","Episode 3 complete:\n","  Mean loss: 0.000022\n","  Memory: 9600 nodes, 76288 edges\n","  Time: 2.8s\n","  Checkpoint saved\n","\n","=== Episode 4/30 ===\n","  Step 10/50 | Loss: 0.000019 | Memory: 10240 nodes, 81408 edges | Time: 0.060s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000016 | Memory: 10880 nodes, 86528 edges | Time: 0.060s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000013 | Memory: 11520 nodes, 91648 edges | Time: 0.062s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000012 | Memory: 12160 nodes, 96768 edges | Time: 0.064s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000010 | Memory: 12800 nodes, 101888 edges | Time: 0.064s | GPU: 0.02G/0.02G\n","Episode 4 complete:\n","  Mean loss: 0.000017\n","  Memory: 12800 nodes, 101888 edges\n","  Time: 3.1s\n","  Checkpoint saved\n","\n","=== Episode 5/30 ===\n","  Step 10/50 | Loss: 0.000010 | Memory: 13440 nodes, 107008 edges | Time: 0.066s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000016 | Memory: 14080 nodes, 112128 edges | Time: 0.067s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000013 | Memory: 14720 nodes, 117248 edges | Time: 0.067s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000011 | Memory: 15360 nodes, 122368 edges | Time: 0.068s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000016 | Memory: 16000 nodes, 127488 edges | Time: 0.069s | GPU: 0.02G/0.02G\n","Episode 5 complete:\n","  Mean loss: 0.000014\n","  Memory: 16000 nodes, 127488 edges\n","  Time: 3.4s\n","  Checkpoint saved\n","\n","=== Episode 6/30 ===\n","  Step 10/50 | Loss: 0.000013 | Memory: 16640 nodes, 132608 edges | Time: 0.072s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000009 | Memory: 17280 nodes, 137728 edges | Time: 0.071s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000016 | Memory: 17920 nodes, 142848 edges | Time: 0.074s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000013 | Memory: 18560 nodes, 147968 edges | Time: 0.075s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000010 | Memory: 19200 nodes, 153088 edges | Time: 0.077s | GPU: 0.02G/0.02G\n","Episode 6 complete:\n","  Mean loss: 0.000013\n","  Memory: 19200 nodes, 153088 edges\n","  Time: 3.7s\n","  Checkpoint saved\n","\n","=== Episode 7/30 ===\n","  Step 10/50 | Loss: 0.000012 | Memory: 19840 nodes, 158208 edges | Time: 0.078s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000018 | Memory: 20480 nodes, 163328 edges | Time: 0.079s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000013 | Memory: 21120 nodes, 168448 edges | Time: 0.080s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000011 | Memory: 21760 nodes, 173568 edges | Time: 0.082s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000009 | Memory: 22400 nodes, 178688 edges | Time: 0.085s | GPU: 0.02G/0.02G\n","Episode 7 complete:\n","  Mean loss: 0.000012\n","  Memory: 22400 nodes, 178688 edges\n","  Time: 4.0s\n","  Checkpoint saved\n","\n","=== Episode 8/30 ===\n","  Step 10/50 | Loss: 0.000011 | Memory: 23040 nodes, 183808 edges | Time: 0.087s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000011 | Memory: 23680 nodes, 188928 edges | Time: 0.088s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000009 | Memory: 24320 nodes, 194048 edges | Time: 0.087s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000009 | Memory: 24960 nodes, 199168 edges | Time: 0.089s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000012 | Memory: 25600 nodes, 204288 edges | Time: 0.090s | GPU: 0.02G/0.02G\n","Episode 8 complete:\n","  Mean loss: 0.000010\n","  Memory: 25600 nodes, 204288 edges\n","  Time: 4.4s\n","  Checkpoint saved\n","\n","=== Episode 9/30 ===\n","  Step 10/50 | Loss: 0.000008 | Memory: 26240 nodes, 209408 edges | Time: 0.092s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000009 | Memory: 26880 nodes, 214528 edges | Time: 0.091s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000007 | Memory: 27520 nodes, 219648 edges | Time: 0.094s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000009 | Memory: 28160 nodes, 224768 edges | Time: 0.094s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000007 | Memory: 28800 nodes, 229888 edges | Time: 0.096s | GPU: 0.02G/0.02G\n","Episode 9 complete:\n","  Mean loss: 0.000009\n","  Memory: 28800 nodes, 229888 edges\n","  Time: 4.7s\n","  Checkpoint saved\n","\n","=== Episode 10/30 ===\n","  Step 10/50 | Loss: 0.000009 | Memory: 29440 nodes, 235008 edges | Time: 0.098s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000008 | Memory: 30080 nodes, 240128 edges | Time: 0.098s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000008 | Memory: 30720 nodes, 245248 edges | Time: 0.101s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000009 | Memory: 31360 nodes, 250368 edges | Time: 0.100s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000006 | Memory: 32000 nodes, 255488 edges | Time: 0.103s | GPU: 0.02G/0.02G\n","Episode 10 complete:\n","  Mean loss: 0.000009\n","  Memory: 32000 nodes, 255488 edges\n","  Time: 5.0s\n","  Checkpoint saved\n","\n","=== Episode 11/30 ===\n","  Step 10/50 | Loss: 0.000007 | Memory: 32640 nodes, 260608 edges | Time: 0.104s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000007 | Memory: 33280 nodes, 265728 edges | Time: 0.106s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000005 | Memory: 33920 nodes, 270848 edges | Time: 0.107s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000006 | Memory: 34560 nodes, 275968 edges | Time: 0.109s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000007 | Memory: 35200 nodes, 281088 edges | Time: 0.109s | GPU: 0.02G/0.02G\n","Episode 11 complete:\n","  Mean loss: 0.000007\n","  Memory: 35200 nodes, 281088 edges\n","  Time: 5.3s\n","  Checkpoint saved\n","\n","=== Episode 12/30 ===\n","  Step 10/50 | Loss: 0.000004 | Memory: 35840 nodes, 286208 edges | Time: 0.110s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000007 | Memory: 36480 nodes, 291328 edges | Time: 0.113s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000005 | Memory: 37120 nodes, 296448 edges | Time: 0.113s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000007 | Memory: 37760 nodes, 301568 edges | Time: 0.115s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000005 | Memory: 38400 nodes, 306688 edges | Time: 0.116s | GPU: 0.02G/0.02G\n","Episode 12 complete:\n","  Mean loss: 0.000006\n","  Memory: 38400 nodes, 306688 edges\n","  Time: 5.6s\n","  Checkpoint saved\n","\n","=== Episode 13/30 ===\n","  Step 10/50 | Loss: 0.000005 | Memory: 39040 nodes, 311808 edges | Time: 0.118s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000006 | Memory: 39680 nodes, 316928 edges | Time: 0.119s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000007 | Memory: 40320 nodes, 322048 edges | Time: 0.120s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000004 | Memory: 40960 nodes, 327168 edges | Time: 0.121s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000005 | Memory: 41600 nodes, 332288 edges | Time: 0.122s | GPU: 0.02G/0.02G\n","Episode 13 complete:\n","  Mean loss: 0.000006\n","  Memory: 41600 nodes, 332288 edges\n","  Time: 6.0s\n","  Checkpoint saved\n","\n","=== Episode 14/30 ===\n","  Step 10/50 | Loss: 0.000007 | Memory: 42240 nodes, 337408 edges | Time: 0.125s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000006 | Memory: 42880 nodes, 342528 edges | Time: 0.126s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000005 | Memory: 43520 nodes, 347648 edges | Time: 0.127s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000004 | Memory: 44160 nodes, 352768 edges | Time: 0.131s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000004 | Memory: 44800 nodes, 357888 edges | Time: 0.129s | GPU: 0.02G/0.02G\n","Episode 14 complete:\n","  Mean loss: 0.000006\n","  Memory: 44800 nodes, 357888 edges\n","  Time: 6.4s\n","  Checkpoint saved\n","\n","=== Episode 15/30 ===\n","  Step 10/50 | Loss: 0.000005 | Memory: 45440 nodes, 363008 edges | Time: 0.129s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000005 | Memory: 46080 nodes, 368128 edges | Time: 0.132s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000006 | Memory: 46720 nodes, 373248 edges | Time: 0.134s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000005 | Memory: 47360 nodes, 378368 edges | Time: 0.135s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000006 | Memory: 48000 nodes, 383488 edges | Time: 0.135s | GPU: 0.02G/0.02G\n","Episode 15 complete:\n","  Mean loss: 0.000005\n","  Memory: 48000 nodes, 383488 edges\n","  Time: 6.6s\n","  Checkpoint saved\n","\n","=== Episode 16/30 ===\n","  Step 10/50 | Loss: 0.000006 | Memory: 48640 nodes, 388608 edges | Time: 0.135s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000005 | Memory: 49280 nodes, 393728 edges | Time: 0.137s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000004 | Memory: 49920 nodes, 398848 edges | Time: 0.139s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000004 | Memory: 50560 nodes, 403968 edges | Time: 0.138s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000003 | Memory: 51200 nodes, 409088 edges | Time: 0.138s | GPU: 0.02G/0.02G\n","Episode 16 complete:\n","  Mean loss: 0.000004\n","  Memory: 51200 nodes, 409088 edges\n","  Time: 6.9s\n","  Checkpoint saved\n","\n","=== Episode 17/30 ===\n","  Step 10/50 | Loss: 0.000004 | Memory: 51840 nodes, 414208 edges | Time: 0.141s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000006 | Memory: 52480 nodes, 419328 edges | Time: 0.143s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000006 | Memory: 53120 nodes, 424448 edges | Time: 0.147s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000003 | Memory: 53760 nodes, 429568 edges | Time: 0.144s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000005 | Memory: 54400 nodes, 434688 edges | Time: 0.148s | GPU: 0.02G/0.02G\n","Episode 17 complete:\n","  Mean loss: 0.000004\n","  Memory: 54400 nodes, 434688 edges\n","  Time: 7.2s\n","  Checkpoint saved\n","\n","=== Episode 18/30 ===\n","  Step 10/50 | Loss: 0.000004 | Memory: 55040 nodes, 439808 edges | Time: 0.147s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000005 | Memory: 55680 nodes, 444928 edges | Time: 0.149s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000003 | Memory: 56320 nodes, 450048 edges | Time: 0.150s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000003 | Memory: 56960 nodes, 455168 edges | Time: 0.147s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000005 | Memory: 57600 nodes, 460288 edges | Time: 0.153s | GPU: 0.02G/0.02G\n","Episode 18 complete:\n","  Mean loss: 0.000004\n","  Memory: 57600 nodes, 460288 edges\n","  Time: 7.4s\n","  Checkpoint saved\n","\n","=== Episode 19/30 ===\n","  Step 10/50 | Loss: 0.000005 | Memory: 58240 nodes, 465408 edges | Time: 0.155s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000005 | Memory: 58880 nodes, 470528 edges | Time: 0.156s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000003 | Memory: 59520 nodes, 475648 edges | Time: 0.159s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000004 | Memory: 60160 nodes, 480768 edges | Time: 0.158s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000004 | Memory: 60800 nodes, 485888 edges | Time: 0.158s | GPU: 0.02G/0.02G\n","Episode 19 complete:\n","  Mean loss: 0.000004\n","  Memory: 60800 nodes, 485888 edges\n","  Time: 7.8s\n","  Checkpoint saved\n","\n","=== Episode 20/30 ===\n","  Step 10/50 | Loss: 0.000003 | Memory: 61440 nodes, 491008 edges | Time: 0.160s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000003 | Memory: 62080 nodes, 496128 edges | Time: 0.162s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000003 | Memory: 62720 nodes, 501248 edges | Time: 0.162s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000003 | Memory: 63360 nodes, 506368 edges | Time: 0.168s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000003 | Memory: 64000 nodes, 511488 edges | Time: 0.165s | GPU: 0.02G/0.02G\n","Episode 20 complete:\n","  Mean loss: 0.000003\n","  Memory: 64000 nodes, 511488 edges\n","  Time: 8.1s\n","  Checkpoint saved\n","\n","=== Episode 21/30 ===\n","  Step 10/50 | Loss: 0.000005 | Memory: 64640 nodes, 516608 edges | Time: 0.168s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000003 | Memory: 65280 nodes, 521728 edges | Time: 0.170s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000004 | Memory: 65920 nodes, 526848 edges | Time: 0.168s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000003 | Memory: 66560 nodes, 531968 edges | Time: 0.172s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000003 | Memory: 67200 nodes, 537088 edges | Time: 0.174s | GPU: 0.02G/0.02G\n","Episode 21 complete:\n","  Mean loss: 0.000003\n","  Memory: 67200 nodes, 537088 edges\n","  Time: 8.4s\n","  Checkpoint saved\n","\n","=== Episode 22/30 ===\n","  Step 10/50 | Loss: 0.000003 | Memory: 67840 nodes, 542208 edges | Time: 0.173s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000002 | Memory: 68480 nodes, 547328 edges | Time: 0.175s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000003 | Memory: 69120 nodes, 552448 edges | Time: 0.177s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000003 | Memory: 69760 nodes, 557568 edges | Time: 0.177s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000004 | Memory: 70400 nodes, 562688 edges | Time: 0.181s | GPU: 0.02G/0.02G\n","Episode 22 complete:\n","  Mean loss: 0.000003\n","  Memory: 70400 nodes, 562688 edges\n","  Time: 8.8s\n","  Checkpoint saved\n","\n","=== Episode 23/30 ===\n","  Step 10/50 | Loss: 0.000004 | Memory: 71040 nodes, 567808 edges | Time: 0.180s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000003 | Memory: 71680 nodes, 572928 edges | Time: 0.180s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000002 | Memory: 72320 nodes, 578048 edges | Time: 0.182s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000004 | Memory: 72960 nodes, 583168 edges | Time: 0.184s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000002 | Memory: 73600 nodes, 588288 edges | Time: 0.185s | GPU: 0.02G/0.02G\n","Episode 23 complete:\n","  Mean loss: 0.000003\n","  Memory: 73600 nodes, 588288 edges\n","  Time: 9.1s\n","  Checkpoint saved\n","\n","=== Episode 24/30 ===\n","  Step 10/50 | Loss: 0.000002 | Memory: 74240 nodes, 593408 edges | Time: 0.184s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000002 | Memory: 74880 nodes, 598528 edges | Time: 0.186s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000002 | Memory: 75520 nodes, 603648 edges | Time: 0.189s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000002 | Memory: 76160 nodes, 608768 edges | Time: 0.187s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000003 | Memory: 76800 nodes, 613888 edges | Time: 0.192s | GPU: 0.02G/0.02G\n","Episode 24 complete:\n","  Mean loss: 0.000002\n","  Memory: 76800 nodes, 613888 edges\n","  Time: 9.4s\n","  Checkpoint saved\n","\n","=== Episode 25/30 ===\n","  Step 10/50 | Loss: 0.000003 | Memory: 77440 nodes, 619008 edges | Time: 0.192s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000003 | Memory: 78080 nodes, 624128 edges | Time: 0.195s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000002 | Memory: 78720 nodes, 629248 edges | Time: 0.194s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000003 | Memory: 79360 nodes, 634368 edges | Time: 0.199s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000002 | Memory: 80000 nodes, 639488 edges | Time: 0.199s | GPU: 0.02G/0.02G\n","Episode 25 complete:\n","  Mean loss: 0.000003\n","  Memory: 80000 nodes, 639488 edges\n","  Time: 9.7s\n","  Checkpoint saved\n","\n","=== Episode 26/30 ===\n","  Step 10/50 | Loss: 0.000002 | Memory: 80640 nodes, 644608 edges | Time: 0.198s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000003 | Memory: 81280 nodes, 649728 edges | Time: 0.201s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000003 | Memory: 81920 nodes, 654848 edges | Time: 0.199s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000003 | Memory: 82560 nodes, 659968 edges | Time: 0.202s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000003 | Memory: 83200 nodes, 665088 edges | Time: 0.205s | GPU: 0.02G/0.02G\n","Episode 26 complete:\n","  Mean loss: 0.000002\n","  Memory: 83200 nodes, 665088 edges\n","  Time: 10.0s\n","  Checkpoint saved\n","\n","=== Episode 27/30 ===\n","  Step 10/50 | Loss: 0.000003 | Memory: 83840 nodes, 670208 edges | Time: 0.203s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000002 | Memory: 84480 nodes, 675328 edges | Time: 0.208s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000003 | Memory: 85120 nodes, 680448 edges | Time: 0.206s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000002 | Memory: 85760 nodes, 685568 edges | Time: 0.205s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000002 | Memory: 86400 nodes, 690688 edges | Time: 0.208s | GPU: 0.02G/0.02G\n","Episode 27 complete:\n","  Mean loss: 0.000002\n","  Memory: 86400 nodes, 690688 edges\n","  Time: 10.3s\n","  Checkpoint saved\n","\n","=== Episode 28/30 ===\n","  Step 10/50 | Loss: 0.000002 | Memory: 87040 nodes, 695808 edges | Time: 0.210s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000003 | Memory: 87680 nodes, 700928 edges | Time: 0.211s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000002 | Memory: 88320 nodes, 706048 edges | Time: 0.212s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000002 | Memory: 88960 nodes, 711168 edges | Time: 0.215s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000002 | Memory: 89600 nodes, 716288 edges | Time: 0.217s | GPU: 0.02G/0.02G\n","Episode 28 complete:\n","  Mean loss: 0.000002\n","  Memory: 89600 nodes, 716288 edges\n","  Time: 10.6s\n","  Checkpoint saved\n","\n","=== Episode 29/30 ===\n","  Step 10/50 | Loss: 0.000002 | Memory: 90240 nodes, 721408 edges | Time: 0.218s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000002 | Memory: 90880 nodes, 726528 edges | Time: 0.218s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000002 | Memory: 91520 nodes, 731648 edges | Time: 0.219s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000002 | Memory: 92160 nodes, 736768 edges | Time: 0.221s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000002 | Memory: 92800 nodes, 741888 edges | Time: 0.226s | GPU: 0.02G/0.02G\n","Episode 29 complete:\n","  Mean loss: 0.000002\n","  Memory: 92800 nodes, 741888 edges\n","  Time: 11.0s\n","  Checkpoint saved\n","\n","=== Episode 30/30 ===\n","  Step 10/50 | Loss: 0.000002 | Memory: 93440 nodes, 747008 edges | Time: 0.224s | GPU: 0.02G/0.02G\n","  Step 20/50 | Loss: 0.000002 | Memory: 94080 nodes, 752128 edges | Time: 0.224s | GPU: 0.02G/0.02G\n","  Step 30/50 | Loss: 0.000002 | Memory: 94720 nodes, 757248 edges | Time: 0.227s | GPU: 0.02G/0.02G\n","  Step 40/50 | Loss: 0.000002 | Memory: 95360 nodes, 762368 edges | Time: 0.227s | GPU: 0.02G/0.02G\n","  Step 50/50 | Loss: 0.000002 | Memory: 96000 nodes, 767488 edges | Time: 0.228s | GPU: 0.02G/0.02G\n","Episode 30 complete:\n","  Mean loss: 0.000002\n","  Memory: 96000 nodes, 767488 edges\n","  Time: 11.3s\n","  Checkpoint saved\n","\n","Training complete! Total time: 203.0s\n","Final stats: {}\n","Artifacts saved in: graph_world_runs/run_sparse_knn_vec_faiss\n"]}],"source":["# @title\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","GRLM: Graph Representation Learning Model\n","Debugged and cleaned version\n","\"\"\"\n","\n","import os\n","import math\n","import time\n","import sys\n","import json\n","import random\n","import tempfile\n","import shutil\n","import warnings\n","from dataclasses import dataclass\n","from pathlib import Path\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# ============================================================================\n","# PERFORMANCE OPTIMIZATIONS\n","# ============================================================================\n","\n","# Enable TF32 for faster training on Ampere+ GPUs\n","if torch.cuda.is_available():\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    try:\n","        torch.set_float32_matmul_precision(\"high\")\n","    except AttributeError:\n","        # Older PyTorch versions don't have this\n","        pass\n","\n","# AMP settings\n","USE_AMP = torch.cuda.is_available()\n","AMP_DTYPE = torch.bfloat16\n","\n","# Device setup\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# ============================================================================\n","# FAISS SETUP (OPTIONAL)\n","# ============================================================================\n","\n","try:\n","    import faiss\n","    FAISS_AVAILABLE = True\n","    # Check GPU support\n","    try:\n","        FAISS_GPU_OK = hasattr(faiss, \"StandardGpuResources\") and faiss.get_num_gpus() > 0\n","    except (AttributeError, RuntimeError):\n","        FAISS_GPU_OK = False\n","except ImportError:\n","    faiss = None\n","    FAISS_AVAILABLE = False\n","    FAISS_GPU_OK = False\n","\n","print(f\"FAISS available: {FAISS_AVAILABLE} | GPU support: {FAISS_GPU_OK}\")\n","\n","def build_faiss_index(dim: int,\n","                      metric: str = \"l2\",\n","                      kind: str = \"flat\",\n","                      nlist: int = 4096,\n","                      nprobe: int = 64,\n","                      force_cpu: bool = False):\n","    \"\"\"Build FAISS index with GPU support if available.\"\"\"\n","    if not FAISS_AVAILABLE:\n","        return None\n","\n","    use_gpu = (not force_cpu) and FAISS_GPU_OK\n","    metric_id = faiss.METRIC_L2 if metric.lower() == \"l2\" else faiss.METRIC_INNER_PRODUCT\n","\n","    if kind == \"flat\":\n","        if use_gpu:\n","            res = faiss.StandardGpuResources()\n","            if metric_id == faiss.METRIC_L2:\n","                index = faiss.GpuIndexFlatL2(res, dim)\n","            else:\n","                index = faiss.GpuIndexFlatIP(res, dim)\n","            backend = \"gpu-flat\"\n","        else:\n","            if metric_id == faiss.METRIC_L2:\n","                index = faiss.IndexFlatL2(dim)\n","            else:\n","                index = faiss.IndexFlatIP(dim)\n","            backend = \"cpu-flat\"\n","        print(f\"[FAISS] Built {backend} index (dim={dim})\")\n","        return index\n","\n","    elif kind == \"ivf_flat\":\n","        # Create IVF index\n","        if metric_id == faiss.METRIC_L2:\n","            quantizer = faiss.IndexFlatL2(dim)\n","        else:\n","            quantizer = faiss.IndexFlatIP(dim)\n","\n","        cpu_ivf = faiss.IndexIVFFlat(quantizer, dim, int(nlist), metric_id)\n","\n","        if use_gpu:\n","            res = faiss.StandardGpuResources()\n","            index = faiss.index_cpu_to_gpu(res, 0, cpu_ivf)\n","            backend = \"gpu-ivf\"\n","        else:\n","            index = cpu_ivf\n","            backend = \"cpu-ivf\"\n","\n","        # Set nprobe\n","        index.nprobe = int(nprobe)\n","        print(f\"[FAISS] Built {backend} index (dim={dim}, nlist={nlist}, nprobe={nprobe})\")\n","        return index\n","\n","    else:\n","        raise ValueError(\"kind must be 'flat' or 'ivf_flat'\")\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","# Paths\n","RUN_ROOT = Path(os.getenv(\"RUN_ROOT\", \"./graph_world_runs\")).expanduser()\n","RUN_NAME = os.getenv(\"RUN_NAME\", \"run_sparse_knn_vec_faiss\")\n","ROOT = RUN_ROOT / RUN_NAME\n","ROOT.mkdir(parents=True, exist_ok=True)\n","\n","# Random seed\n","SEED = 1337\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","# Training parameters\n","EPISODES = 30\n","STEPS_PER_EP = 50\n","\n","# Memory/retrieval parameters\n","EMB_DIM = 128\n","K_NEI = 4\n","CAND_RECENT = 384\n","CAND_RANDOM = 128\n","MAX_NODES = 200_000\n","\n","# World/model parameters\n","N_OBJECTS_PER_STEP = 64\n","LR = 2.99e-3\n","\n","# FAISS configuration\n","FAISS_KIND = os.environ.get(\"FAISS_KIND\", \"flat\")  # Changed default to \"flat\" for reliability\n","FAISS_METRIC = os.environ.get(\"FAISS_METRIC\", \"l2\")\n","FAISS_NLIST = int(os.environ.get(\"FAISS_NLIST\", \"4096\"))\n","FAISS_NPROBE = int(os.environ.get(\"FAISS_NPROBE\", \"64\"))\n","\n","# ============================================================================\n","# UTILITY FUNCTIONS\n","# ============================================================================\n","\n","def atomic_save(obj, path: Path):\n","    \"\"\"Atomically save object to path.\"\"\"\n","    path.parent.mkdir(parents=True, exist_ok=True)\n","    with tempfile.NamedTemporaryFile(dir=path.parent, suffix=\".tmp\", delete=False) as tmp:\n","        tmp_path = Path(tmp.name)\n","\n","    try:\n","        torch.save(obj, tmp_path)\n","        if os.name == 'nt':  # Windows\n","            if path.exists():\n","                path.unlink()\n","        tmp_path.replace(path)\n","    except Exception:\n","        if tmp_path.exists():\n","            tmp_path.unlink()\n","        raise\n","\n","def gpu_mem_str():\n","    \"\"\"Get GPU memory usage string.\"\"\"\n","    if torch.cuda.is_available():\n","        reserved = torch.cuda.memory_reserved() / (1024**3)\n","        allocated = torch.cuda.max_memory_allocated() / (1024**3)\n","        return f\"{reserved:.2f}G/{allocated:.2f}G\"\n","    return \"CPU\"\n","\n","@torch.no_grad()\n","def torch_topk_cosine(query: np.ndarray, corpus: np.ndarray, topk: int):\n","    \"\"\"Fast cosine similarity top-k using PyTorch.\"\"\"\n","    device = torch.device(DEVICE)\n","\n","    Q = torch.from_numpy(query).to(device, dtype=torch.float32)\n","    C = torch.from_numpy(corpus).to(device, dtype=torch.float32)\n","\n","    # Normalize for cosine similarity\n","    Q = F.normalize(Q, dim=1)\n","    C = F.normalize(C, dim=1)\n","\n","    # Compute similarities\n","    similarities = Q @ C.T  # [query_count, corpus_count]\n","\n","    # Get top-k\n","    topk = min(topk, C.shape[0])\n","    _, indices = torch.topk(similarities, k=topk, dim=1, largest=True)\n","\n","    return indices.detach().cpu().numpy()\n","\n","# ============================================================================\n","# GRAPH MEMORY\n","# ============================================================================\n","\n","class GraphMemory:\n","    \"\"\"Graph-based memory with FAISS acceleration.\"\"\"\n","\n","    def __init__(self, dim: int, max_nodes: int = MAX_NODES):\n","        self.dim = dim\n","        self.max_nodes = max_nodes\n","        self.nodes = []  # List of node embeddings\n","        self.edges = []  # List of (source, target) tuples\n","        self._faiss_index = None\n","        self._needs_faiss_rebuild = True\n","\n","    def _trim_if_needed(self):\n","        \"\"\"Remove oldest nodes if over capacity.\"\"\"\n","        if len(self.nodes) <= self.max_nodes:\n","            return\n","\n","        # Keep most recent nodes\n","        keep_count = self.max_nodes\n","        trim_count = len(self.nodes) - keep_count\n","\n","        # Remove old nodes\n","        self.nodes = self.nodes[trim_count:]\n","\n","        # Update edges (remove references to trimmed nodes)\n","        new_edges = []\n","        for src, dst in self.edges:\n","            if src >= trim_count and dst >= trim_count:\n","                new_edges.append((src - trim_count, dst - trim_count))\n","        self.edges = new_edges\n","\n","        # Force FAISS rebuild\n","        self._needs_faiss_rebuild = True\n","        self._faiss_index = None\n","\n","    def _build_faiss_index(self):\n","        \"\"\"Build or rebuild FAISS index.\"\"\"\n","        if not FAISS_AVAILABLE or len(self.nodes) == 0:\n","            return\n","\n","        # Convert nodes to numpy array\n","        node_array = np.array(self.nodes, dtype=np.float32)\n","\n","        # Build index\n","        self._faiss_index = build_faiss_index(\n","            dim=self.dim,\n","            metric=FAISS_METRIC,\n","            kind=FAISS_KIND,\n","            nlist=FAISS_NLIST,\n","            nprobe=FAISS_NPROBE\n","        )\n","\n","        if self._faiss_index is not None:\n","            # Train if needed (for IVF indices)\n","            if hasattr(self._faiss_index, 'is_trained') and not self._faiss_index.is_trained:\n","                self._faiss_index.train(node_array)\n","\n","            # Add vectors\n","            self._faiss_index.add(node_array)\n","\n","        self._needs_faiss_rebuild = False\n","\n","    def _faiss_search(self, query_vecs: np.ndarray, k: int):\n","        \"\"\"Search using FAISS index.\"\"\"\n","        if self._needs_faiss_rebuild or self._faiss_index is None:\n","            self._build_faiss_index()\n","\n","        if self._faiss_index is None:\n","            return None\n","\n","        query_vecs = query_vecs.astype(np.float32)\n","        k = min(k, len(self.nodes))\n","\n","        try:\n","            distances, indices = self._faiss_index.search(query_vecs, k)\n","            return indices\n","        except Exception as e:\n","            print(f\"FAISS search failed: {e}\")\n","            return None\n","\n","    def add_batch(self, batch_vecs: np.ndarray,\n","                  k_neighbors: int = K_NEI,\n","                  recent_candidates: int = CAND_RECENT,\n","                  random_candidates: int = CAND_RANDOM):\n","        \"\"\"Add a batch of vectors and connect them to existing nodes.\"\"\"\n","\n","        start_idx = len(self.nodes)\n","\n","        # Add new nodes\n","        for vec in batch_vecs:\n","            self.nodes.append(vec.astype(np.float32))\n","\n","        # Connect new nodes to existing ones\n","        if start_idx > 0:  # Only if we have existing nodes\n","            all_nodes = np.array(self.nodes, dtype=np.float32)\n","\n","            for new_idx in range(start_idx, len(self.nodes)):\n","                # Build candidate pool\n","                recent_start = max(0, new_idx - recent_candidates)\n","                recent_candidates_list = list(range(recent_start, new_idx))\n","\n","                # Add some random older candidates\n","                older_pool = list(range(0, recent_start))\n","                if random_candidates > 0 and older_pool:\n","                    random_count = min(random_candidates, len(older_pool))\n","                    random_candidates_list = random.sample(older_pool, random_count)\n","                else:\n","                    random_candidates_list = []\n","\n","                candidates = recent_candidates_list + random_candidates_list\n","\n","                if not candidates:\n","                    continue\n","\n","                # Find k nearest neighbors\n","                query = all_nodes[new_idx:new_idx+1]  # Shape: (1, dim)\n","                candidate_nodes = all_nodes[candidates]\n","\n","                try:\n","                    # Try FAISS first\n","                    if FAISS_AVAILABLE and len(candidates) >= k_neighbors:\n","                        # Create temporary index for candidates\n","                        temp_index = build_faiss_index(self.dim, FAISS_METRIC, \"flat\")\n","                        if temp_index is not None:\n","                            if hasattr(temp_index, 'is_trained') and not temp_index.is_trained:\n","                                temp_index.train(candidate_nodes)\n","                            temp_index.add(candidate_nodes)\n","                            _, neighbor_indices = temp_index.search(query, k_neighbors)\n","                            neighbors = [candidates[idx] for idx in neighbor_indices[0]]\n","                        else:\n","                            raise RuntimeError(\"FAISS index creation failed\")\n","                    else:\n","                        raise RuntimeError(\"Using PyTorch fallback\")\n","\n","                except Exception:\n","                    # Fallback to PyTorch\n","                    neighbor_indices = torch_topk_cosine(query, candidate_nodes, k_neighbors)[0]\n","                    neighbors = [candidates[idx] for idx in neighbor_indices]\n","\n","                # Add edges (bidirectional)\n","                for neighbor_idx in neighbors:\n","                    self.edges.append((new_idx, neighbor_idx))\n","                    self.edges.append((neighbor_idx, new_idx))\n","\n","        # Trim if necessary\n","        self._trim_if_needed()\n","\n","        # Mark FAISS for rebuild\n","        self._needs_faiss_rebuild = True\n","\n","# ============================================================================\n","# WORLD SIMULATION\n","# ============================================================================\n","\n","def create_world_step(batch_size: int, n_objects: int, embedding_dim: int, device: str):\n","    \"\"\"Create a synthetic world step with random features and positions.\"\"\"\n","    # Split embedding into feature and position components\n","    feat_dim = embedding_dim // 2\n","    pos_dim = embedding_dim - feat_dim\n","\n","    features = torch.randn(batch_size, n_objects, feat_dim, device=device)\n","    positions = torch.randn(batch_size, n_objects, pos_dim, device=device)\n","\n","    # Concatenate features and positions\n","    world_state = torch.cat([features, positions], dim=-1)\n","\n","    return world_state\n","\n","# ============================================================================\n","# MODEL ARCHITECTURE\n","# ============================================================================\n","\n","class Encoder(nn.Module):\n","    \"\"\"Encode world state to embeddings.\"\"\"\n","\n","    def __init__(self, input_dim: int, embedding_dim: int):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(input_dim, embedding_dim),\n","            nn.ReLU(),\n","            nn.Linear(embedding_dim, embedding_dim),\n","        )\n","\n","    def forward(self, x):\n","        # x shape: [batch_size, num_objects, input_dim]\n","        embeddings = self.network(x)\n","        # Normalize embeddings\n","        embeddings = F.normalize(embeddings, dim=-1)\n","        return embeddings\n","\n","class ReadoutHead(nn.Module):\n","    \"\"\"Readout head for final prediction.\"\"\"\n","\n","    def __init__(self, embedding_dim: int):\n","        super().__init__()\n","        self.projection = nn.Linear(embedding_dim, embedding_dim)\n","\n","    def forward(self, embeddings):\n","        # embeddings shape: [batch_size, num_objects, embedding_dim]\n","        # Average pool over objects and project\n","        pooled = embeddings.mean(dim=1)  # [batch_size, embedding_dim]\n","        output = self.projection(pooled)\n","        return output\n","\n","class WorldModel(nn.Module):\n","    \"\"\"Complete world model with encoder and readout.\"\"\"\n","\n","    def __init__(self, input_dim: int, embedding_dim: int):\n","        super().__init__()\n","        self.encoder = Encoder(input_dim, embedding_dim)\n","        self.readout = ReadoutHead(embedding_dim)\n","\n","    def forward(self, world_state):\n","        embeddings = self.encoder(world_state)\n","        prediction = self.readout(embeddings)\n","        return prediction, embeddings\n","\n","# ============================================================================\n","# AGENT\n","# ============================================================================\n","\n","@dataclass\n","class Agent:\n","    \"\"\"Agent with world model and graph memory.\"\"\"\n","    world_model: WorldModel\n","    memory: GraphMemory\n","\n","# ============================================================================\n","# OPTIMIZER SETUP\n","# ============================================================================\n","\n","def create_optimizer(parameters, learning_rate: float):\n","    \"\"\"Create optimized optimizer.\"\"\"\n","    if DEVICE == \"cuda\":\n","        try:\n","            return torch.optim.AdamW(parameters, lr=learning_rate, fused=True)\n","        except TypeError:\n","            pass\n","        try:\n","            return torch.optim.AdamW(parameters, lr=learning_rate, foreach=True)\n","        except TypeError:\n","            pass\n","\n","    return torch.optim.AdamW(parameters, lr=learning_rate)\n","\n","# ============================================================================\n","# CHECKPOINT MANAGEMENT\n","# ============================================================================\n","\n","def save_checkpoint(agent: Agent, optimizer, episode: int, stats: dict, checkpoint_path: Path):\n","    \"\"\"Save training checkpoint.\"\"\"\n","    state = {\n","        \"episode\": episode,\n","        \"model_state\": agent.world_model.state_dict(),\n","        \"optimizer_state\": optimizer.state_dict(),\n","        \"stats\": stats,\n","        \"config\": {\n","            \"EMB_DIM\": EMB_DIM,\n","            \"K_NEI\": K_NEI,\n","            \"CAND_RECENT\": CAND_RECENT,\n","            \"CAND_RANDOM\": CAND_RANDOM,\n","            \"LR\": LR,\n","        },\n","    }\n","    atomic_save(state, checkpoint_path)\n","\n","def load_checkpoint(agent: Agent, optimizer, checkpoint_path: Path):\n","    \"\"\"Load training checkpoint.\"\"\"\n","    if not checkpoint_path.exists():\n","        return 0, {}\n","\n","    state = torch.load(checkpoint_path, map_location=DEVICE)\n","    agent.world_model.load_state_dict(state[\"model_state\"])\n","    optimizer.load_state_dict(state[\"optimizer_state\"])\n","\n","    return state.get(\"episode\", 0), state.get(\"stats\", {})\n","\n","# ============================================================================\n","# MAIN TRAINING LOOP\n","# ============================================================================\n","\n","def main():\n","    \"\"\"Main training function.\"\"\"\n","    print(f\"Using device: {DEVICE}\")\n","    print(f\"CUDA available: {torch.cuda.is_available()}\")\n","    if torch.cuda.is_available():\n","        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Output directory: {ROOT}\")\n","    print(f\"FAISS: available={FAISS_AVAILABLE}, GPU={FAISS_GPU_OK}\")\n","\n","    # Initialize agent\n","    input_dim = EMB_DIM  # World step produces EMB_DIM features per object\n","    agent = Agent(\n","        world_model=WorldModel(input_dim, EMB_DIM).to(DEVICE),\n","        memory=GraphMemory(dim=EMB_DIM, max_nodes=MAX_NODES)\n","    )\n","\n","    # Initialize optimizer\n","    optimizer = create_optimizer(agent.world_model.parameters(), LR)\n","\n","    # Checkpoint management\n","    checkpoint_path = ROOT / \"checkpoint_latest.pt\"\n","    start_episode, stats = load_checkpoint(agent, optimizer, checkpoint_path)\n","\n","    if start_episode > 0:\n","        print(f\"Resumed from episode {start_episode}\")\n","\n","    # Training loop\n","    start_time = time.time()\n","\n","    for episode in range(max(1, start_episode + 1), EPISODES + 1):\n","        episode_losses = []\n","        episode_start = time.time()\n","\n","        print(f\"\\n=== Episode {episode}/{EPISODES} ===\")\n","\n","        for step in range(1, STEPS_PER_EP + 1):\n","            step_start = time.time()\n","\n","            # Create world step\n","            world_state = create_world_step(\n","                batch_size=1,\n","                n_objects=N_OBJECTS_PER_STEP,\n","                embedding_dim=EMB_DIM,\n","                device=DEVICE\n","            )\n","\n","            # Create dummy target (replace with your actual target)\n","            target = torch.zeros(1, EMB_DIM, device=DEVICE)\n","\n","            # Forward pass with optional AMP\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            if USE_AMP:\n","                with torch.autocast(device_type='cuda', dtype=AMP_DTYPE):\n","                    prediction, embeddings = agent.world_model(world_state)\n","                    loss = F.mse_loss(prediction, target)\n","            else:\n","                prediction, embeddings = agent.world_model(world_state)\n","                loss = F.mse_loss(prediction, target)\n","\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Add embeddings to memory\n","            with torch.no_grad():\n","                # Extract embeddings from first batch item\n","                step_embeddings = embeddings[0].detach().float().cpu().numpy()\n","                agent.memory.add_batch(\n","                    step_embeddings,\n","                    k_neighbors=K_NEI,\n","                    recent_candidates=CAND_RECENT,\n","                    random_candidates=CAND_RANDOM\n","                )\n","\n","            episode_losses.append(loss.item())\n","            step_time = time.time() - step_start\n","\n","            # Periodic logging\n","            if step % 10 == 0:\n","                print(f\"  Step {step:2d}/{STEPS_PER_EP} | \"\n","                      f\"Loss: {loss.item():.6f} | \"\n","                      f\"Memory: {len(agent.memory.nodes)} nodes, {len(agent.memory.edges)} edges | \"\n","                      f\"Time: {step_time:.3f}s | \"\n","                      f\"GPU: {gpu_mem_str()}\")\n","\n","        # Episode summary\n","        mean_loss = float(np.mean(episode_losses))\n","        episode_time = time.time() - episode_start\n","\n","        # Save checkpoint\n","        episode_stats = {\"mean_loss\": mean_loss}\n","        save_checkpoint(agent, optimizer, episode, episode_stats, checkpoint_path)\n","\n","        print(f\"Episode {episode} complete:\")\n","        print(f\"  Mean loss: {mean_loss:.6f}\")\n","        print(f\"  Memory: {len(agent.memory.nodes)} nodes, {len(agent.memory.edges)} edges\")\n","        print(f\"  Time: {episode_time:.1f}s\")\n","        print(f\"  Checkpoint saved\")\n","\n","    total_time = time.time() - start_time\n","    print(f\"\\nTraining complete! Total time: {total_time:.1f}s\")\n","    print(f\"Final stats: {stats}\")\n","    print(f\"Artifacts saved in: {ROOT}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2590,"status":"ok","timestamp":1758211837840,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"},"user_tz":-180},"id":"Mnr4JOc5kdOl","outputId":"83fda4ee-4000-4144-a23f-1728215667ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Choose an option:\n","1. Run enhanced training\n","2. Show migration strategy\n","Enter choice (1 or 2): 2\n","=== GRADUAL MIGRATION STRATEGY ===\n","\n","1. Adding Hierarchical Memory to existing system...\n","2. Adding Contrastive Learning...\n","3. Upgrading to Transformer Encoder...\n","4. Adding Graph Neural Networks...\n","5. Complete Advanced System Integration...\n","Migration strategy complete!\n"]}],"source":["# @title\n","# Complete Integration Example: Advanced GRLM with Original System\n","# This shows how to integrate the advanced components with your working base system\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import time\n","from pathlib import Path\n","from dataclasses import dataclass\n","\n","# Import the advanced components (from the previous artifact)\n","# from advanced_grlm_improvements import AdvancedGRLM, HierarchicalMemory, DualMemorySystem\n","\n","# ============================================================================\n","# ENHANCED AGENT WITH ADVANCED COMPONENTS\n","# ============================================================================\n","\n","@dataclass\n","class AdvancedAgent:\n","    \"\"\"Enhanced agent with advanced architectural components.\"\"\"\n","    world_model: nn.Module\n","    hierarchical_memory: 'HierarchicalMemory'\n","    dual_memory: 'DualMemorySystem'\n","\n","    # Advanced components\n","    transformer_encoder: nn.Module = None\n","    contrastive_learner: nn.Module = None\n","    meta_learner: nn.Module = None\n","\n","class AdvancedTrainingLoop:\n","    \"\"\"Enhanced training loop with advanced features.\"\"\"\n","\n","    def __init__(self, agent: AdvancedAgent, config: dict):\n","        self.agent = agent\n","        self.config = config\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Initialize optimizers\n","        self.world_optimizer = self._create_optimizer(agent.world_model.parameters())\n","\n","        if agent.transformer_encoder:\n","            self.transformer_optimizer = self._create_optimizer(agent.transformer_encoder.parameters())\n","\n","        # Loss tracking\n","        self.loss_history = {\n","            'world_loss': [],\n","            'contrastive_loss': [],\n","            'meta_loss': [],\n","            'total_loss': []\n","        }\n","\n","        # Memory consolidation tracking\n","        self.consolidation_counter = 0\n","        self.consolidation_frequency = config.get('consolidation_frequency', 100)\n","\n","    def _create_optimizer(self, parameters):\n","        \"\"\"Create optimized optimizer.\"\"\"\n","        if self.device.type == \"cuda\":\n","            try:\n","                return torch.optim.AdamW(parameters, lr=self.config['learning_rate'], fused=True)\n","            except TypeError:\n","                pass\n","        return torch.optim.AdamW(parameters, lr=self.config['learning_rate'])\n","\n","    def train_step(self, batch_data: dict, step: int, episode: int) -> dict:\n","        \"\"\"Advanced training step with multiple components.\"\"\"\n","\n","        # Extract batch components\n","        world_states = batch_data['states']  # [B, N_objects, state_dim]\n","        actions = batch_data.get('actions', None)\n","        targets = batch_data.get('targets', torch.zeros(world_states.shape[0], self.config['embedding_dim'], device=self.device))\n","\n","        # ============================================================================\n","        # 1. WORLD MODEL PREDICTION\n","        # ============================================================================\n","\n","        self.world_optimizer.zero_grad(set_to_none=True)\n","\n","        # Forward pass through world model\n","        if hasattr(self.agent.world_model, 'forward') and len(torch.signature(self.agent.world_model.forward).parameters) > 1:\n","            # Advanced model with multiple inputs\n","            world_outputs = self.agent.world_model(world_states, actions)\n","            prediction = world_outputs.get('prediction', world_outputs)\n","            embeddings = world_outputs.get('embeddings', world_states)\n","        else:\n","            # Simple model\n","            prediction, embeddings = self.agent.world_model(world_states)\n","\n","        # World model loss\n","        world_loss = F.mse_loss(prediction, targets)\n","\n","        # ============================================================================\n","        # 2. CONTRASTIVE LEARNING (if available)\n","        # ============================================================================\n","\n","        contrastive_loss = torch.tensor(0.0, device=self.device)\n","        if self.agent.contrastive_learner and self.config.get('use_contrastive', True):\n","            # Generate positive/negative pairs\n","            batch_size = embeddings.shape[0]\n","\n","            # Simple positive pairs: same sample with noise\n","            positives = embeddings + 0.1 * torch.randn_like(embeddings)\n","\n","            # Negative samples: other samples in batch\n","            if batch_size > 1:\n","                indices = torch.randperm(batch_size, device=self.device)\n","                negatives = embeddings[indices]\n","\n","                contrastive_loss = self.agent.contrastive_learner.info_nce_loss(\n","                    embeddings, positives, negatives\n","                )\n","\n","        # ============================================================================\n","        # 3. MEMORY OPERATIONS\n","        # ============================================================================\n","\n","        with torch.no_grad():\n","            # Compute importance score\n","            importance = self._compute_importance_score(\n","                prediction, targets, embeddings, step, episode\n","            )\n","\n","            # Add to hierarchical memory\n","            embeddings_np = embeddings.detach().cpu().numpy()\n","            for i in range(embeddings_np.shape[0]):\n","                self.agent.hierarchical_memory.add_experience(\n","                    embeddings_np[i:i+1], importance\n","                )\n","\n","            # Add to dual memory system\n","            context = {\n","                'episode': episode,\n","                'step': step,\n","                'prediction_error': float(world_loss.item()),\n","                'action_taken': actions.cpu().numpy().tolist() if actions is not None else None\n","            }\n","\n","            # Add most important sample to episodic memory\n","            if embeddings_np.shape[0] > 0:\n","                best_idx = 0  # Could be based on importance scoring\n","                self.agent.dual_memory.add_episodic(\n","                    embeddings_np[best_idx],\n","                    context,\n","                    episode,\n","                    importance\n","                )\n","\n","        # ============================================================================\n","        # 4. COMBINED LOSS AND BACKPROPAGATION\n","        # ============================================================================\n","\n","        # Combine losses\n","        total_loss = world_loss\n","        if contrastive_loss.item() > 0:\n","            total_loss += self.config.get('contrastive_weight', 0.1) * contrastive_loss\n","\n","        # Backward pass\n","        total_loss.backward()\n","\n","        # Gradient clipping\n","        if self.config.get('grad_clip', 0) > 0:\n","            torch.nn.utils.clip_grad_norm_(\n","                self.agent.world_model.parameters(),\n","                self.config['grad_clip']\n","            )\n","\n","        # Optimizer steps\n","        self.world_optimizer.step()\n","\n","        if self.agent.transformer_encoder and hasattr(self, 'transformer_optimizer'):\n","            self.transformer_optimizer.step()\n","\n","        # ============================================================================\n","        # 5. MEMORY CONSOLIDATION\n","        # ============================================================================\n","\n","        self.consolidation_counter += 1\n","        if self.consolidation_counter >= self.consolidation_frequency:\n","            self._trigger_memory_consolidation()\n","            self.consolidation_counter = 0\n","\n","        # ============================================================================\n","        # 6. TRACKING AND METRICS\n","        # ============================================================================\n","\n","        step_metrics = {\n","            'world_loss': float(world_loss.item()),\n","            'contrastive_loss': float(contrastive_loss.item()),\n","            'total_loss': float(total_loss.item()),\n","            'importance_score': importance,\n","            'memory_stats': self._get_memory_stats(),\n","            'gradient_norm': self._get_gradient_norm()\n","        }\n","\n","        # Update loss history\n","        self.loss_history['world_loss'].append(step_metrics['world_loss'])\n","        self.loss_history['contrastive_loss'].append(step_metrics['contrastive_loss'])\n","        self.loss_history['total_loss'].append(step_metrics['total_loss'])\n","\n","        return step_metrics\n","\n","    def _compute_importance_score(self, prediction: torch.Tensor, target: torch.Tensor,\n","                                 embeddings: torch.Tensor, step: int, episode: int) -> float:\n","        \"\"\"Compute importance score for memory storage.\"\"\"\n","\n","        # Prediction error component\n","        pred_error = F.mse_loss(prediction, target).item()\n","        error_importance = 1.0 / (1.0 + pred_error)\n","\n","        # Novelty component (based on embedding variance)\n","        embedding_var = torch.var(embeddings).item()\n","        novelty_importance = min(1.0, embedding_var / 0.1)  # Normalized novelty\n","\n","        # Temporal component (early episodes more important)\n","        temporal_importance = max(0.3, 1.0 - episode / 100.0)\n","\n","        # Combined importance\n","        importance = 0.5 * error_importance + 0.3 * novelty_importance + 0.2 * temporal_importance\n","\n","        return float(np.clip(importance, 0.0, 1.0))\n","\n","    def _trigger_memory_consolidation(self):\n","        \"\"\"Trigger memory consolidation processes.\"\"\"\n","        print(f\"[Memory Consolidation] Triggered at step {self.consolidation_counter}\")\n","\n","        # Consolidate dual memory\n","        if hasattr(self.agent.dual_memory, '_consolidate_to_semantic'):\n","            self.agent.dual_memory._consolidate_to_semantic()\n","\n","        # Optionally rebuild FAISS indices\n","        for level in self.agent.hierarchical_memory.levels:\n","            level['faiss_index'] = None  # Force rebuild\n","\n","    def _get_memory_stats(self) -> dict:\n","        \"\"\"Get current memory system statistics.\"\"\"\n","        hierarchical_stats = {\n","            f'level_{i}_nodes': len(level['nodes'])\n","            for i, level in enumerate(self.agent.hierarchical_memory.levels)\n","        }\n","\n","        dual_stats = {\n","            'episodic_memories': len(self.agent.dual_memory.episodic['embeddings']),\n","            'semantic_prototypes': len(self.agent.dual_memory.semantic['prototypes'])\n","        }\n","\n","        return {**hierarchical_stats, **dual_stats}\n","\n","    def _get_gradient_norm(self) -> float:\n","        \"\"\"Get gradient norm for monitoring.\"\"\"\n","        total_norm = 0.0\n","        for p in self.agent.world_model.parameters():\n","            if p.grad is not None:\n","                param_norm = p.grad.data.norm(2)\n","                total_norm += param_norm.item() ** 2\n","        return total_norm ** (1. / 2)\n","\n","# ============================================================================\n","# INTEGRATION EXAMPLE: ENHANCED VERSION OF ORIGINAL TRAINING\n","# ============================================================================\n","\n","def create_enhanced_grlm_system():\n","    \"\"\"Create the complete enhanced GRLM system.\"\"\"\n","\n","    # Configuration\n","    config = {\n","        'embedding_dim': 256,\n","        'learning_rate': 2.99e-3,\n","        'contrastive_weight': 0.1,\n","        'grad_clip': 1.0,\n","        'use_contrastive': True,\n","        'consolidation_frequency': 100,\n","\n","        # Advanced model config\n","        'd_model': 256,\n","        'n_heads': 8,\n","        'n_layers': 4,  # Start smaller for testing\n","        'graph_layers': 2,\n","        'num_modules': 4,\n","        'action_dim': 64,\n","        'dropout': 0.1,\n","        'temperature': 0.07\n","    }\n","\n","    # Create advanced world model\n","    from your_advanced_grlm import AdvancedGRLM, HierarchicalMemory, DualMemorySystem, ContrastiveLearning\n","\n","    advanced_world_model = AdvancedGRLM(config)\n","\n","    # Create memory systems\n","    hierarchical_memory = HierarchicalMemory(\n","        dim=config['embedding_dim'],\n","        max_nodes_per_level=[50000, 10000, 2000]\n","    )\n","\n","    dual_memory = DualMemorySystem(\n","        dim=config['embedding_dim'],\n","        episodic_capacity=100000,\n","        semantic_capacity=10000\n","    )\n","\n","    # Create contrastive learner\n","    contrastive_learner = ContrastiveLearning(\n","        temperature=config['temperature']\n","    )\n","\n","    # Create enhanced agent\n","    enhanced_agent = AdvancedAgent(\n","        world_model=advanced_world_model,\n","        hierarchical_memory=hierarchical_memory,\n","        dual_memory=dual_memory,\n","        contrastive_learner=contrastive_learner\n","    )\n","\n","    return enhanced_agent, config\n","\n","def run_enhanced_training():\n","    \"\"\"Run enhanced training loop.\"\"\"\n","\n","    print(\"Creating enhanced GRLM system...\")\n","    agent, config = create_enhanced_grlm_system()\n","\n","    # Initialize training loop\n","    training_loop = AdvancedTrainingLoop(agent, config)\n","\n","    print(\"Starting enhanced training...\")\n","\n","    # Training parameters\n","    episodes = 10  # Start with fewer episodes for testing\n","    steps_per_episode = 50\n","    n_objects_per_step = 64\n","\n","    for episode in range(1, episodes + 1):\n","        episode_start = time.time()\n","        episode_metrics = []\n","\n","        print(f\"\\n=== Enhanced Episode {episode}/{episodes} ===\")\n","\n","        for step in range(1, steps_per_episode + 1):\n","            # Create batch data (similar to original)\n","            batch_data = {\n","                'states': torch.randn(1, n_objects_per_step, config['embedding_dim']),\n","                'actions': torch.randn(1, config['action_dim']),\n","                'targets': torch.zeros(1, config['embedding_dim'])\n","            }\n","\n","            # Enhanced training step\n","            step_metrics = training_loop.train_step(batch_data, step, episode)\n","            episode_metrics.append(step_metrics)\n","\n","            # Periodic logging\n","            if step % 10 == 0:\n","                print(f\"  Step {step:2d}/{steps_per_episode} | \"\n","                      f\"World Loss: {step_metrics['world_loss']:.6f} | \"\n","                      f\"Contrastive: {step_metrics['contrastive_loss']:.6f} | \"\n","                      f\"Importance: {step_metrics['importance_score']:.3f} | \"\n","                      f\"Memory: {step_metrics['memory_stats']['level_0_nodes']} nodes\")\n","\n","        # Episode summary\n","        episode_time = time.time() - episode_start\n","        avg_world_loss = np.mean([m['world_loss'] for m in episode_metrics])\n","        avg_contrastive_loss = np.mean([m['contrastive_loss'] for m in episode_metrics])\n","\n","        print(f\"Episode {episode} Complete:\")\n","        print(f\"  Avg World Loss: {avg_world_loss:.6f}\")\n","        print(f\"  Avg Contrastive Loss: {avg_contrastive_loss:.6f}\")\n","        print(f\"  Memory Stats: {episode_metrics[-1]['memory_stats']}\")\n","        print(f\"  Time: {episode_time:.1f}s\")\n","\n","    print(\"\\nEnhanced training complete!\")\n","    return agent, training_loop\n","\n","# ============================================================================\n","# GRADUAL MIGRATION STRATEGY\n","# ============================================================================\n","\n","def migrate_existing_system():\n","    \"\"\"Step-by-step migration from basic to advanced system.\"\"\"\n","\n","    print(\"=== GRADUAL MIGRATION STRATEGY ===\")\n","\n","    # Step 1: Keep existing world model, add hierarchical memory\n","    print(\"\\n1. Adding Hierarchical Memory to existing system...\")\n","    # Your existing agent setup here\n","    # agent.memory = HierarchicalMemory(dim=EMB_DIM)\n","\n","    # Step 2: Add contrastive learning\n","    print(\"2. Adding Contrastive Learning...\")\n","    # contrastive_learner = ContrastiveLearning()\n","    # Add contrastive loss to your training loop\n","\n","    # Step 3: Replace encoder with transformer\n","    print(\"3. Upgrading to Transformer Encoder...\")\n","    # Replace your Encoder with TransformerEncoder\n","\n","    # Step 4: Add graph neural networks\n","    print(\"4. Adding Graph Neural Networks...\")\n","    # Add GraphAttentionLayer or GraphConvolution\n","\n","    # Step 5: Full advanced system\n","    print(\"5. Complete Advanced System Integration...\")\n","    # Full AdvancedGRLM integration\n","\n","    print(\"Migration strategy complete!\")\n","\n","if __name__ == \"__main__\":\n","    # Option 1: Run enhanced system\n","    print(\"Choose an option:\")\n","    print(\"1. Run enhanced training\")\n","    print(\"2. Show migration strategy\")\n","\n","    choice = input(\"Enter choice (1 or 2): \").strip()\n","\n","    if choice == \"1\":\n","        try:\n","            agent, training_loop = run_enhanced_training()\n","            print(\"Enhanced system ran successfully!\")\n","        except Exception as e:\n","            print(f\"Error running enhanced system: {e}\")\n","            print(\"Make sure to import the advanced components correctly.\")\n","\n","    elif choice == \"2\":\n","        migrate_existing_system()\n","\n","    else:\n","        print(\"Running basic test of advanced components...\")\n","\n","        # Basic test without full training\n","        from your_advanced_grlm import create_advanced_model\n","        model, memory, dual_memory = create_advanced_model()\n","\n","        # Test forward pass\n","        x = torch.randn(2, 50, 256)\n","        outputs = model(x)\n","\n","        print(f\"Model test successful!\")\n","        print(f\"Output shape: {outputs['prediction'].shape}\")\n","        print(f\"Embeddings shape: {outputs['embeddings'].shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1063305,"status":"ok","timestamp":1758214096646,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"},"user_tz":-180},"id":"KHayNg0HqcUU","outputId":"bb45c643-dd66-41c3-ea6b-4a8293f8e10a"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Enhanced GRLM Starting\n","Device: cuda\n","GPU: NVIDIA A100-SXM4-40GB\n","GPU Memory: 42.5GB\n","\n","============================================================\n"," ENHANCED GRLM TRAINING STARTING\n","============================================================\n","Episodes: 25 | Steps per episode: 50\n","Embedding dim: 256 | Objects per step: 64\n","Memory capacity: 100,000 nodes\n","Device: cuda\n"," Enhanced system initialized\n","Model parameters: 1,249,792\n","\n","==================================================\n"," Episode 1/25\n","==================================================\n","  Step 10/50 | World: 0.000035 | Contrast: 4.263485 | Import: 0.701 | LR: 3.0e-03 | Mem: 640(640w/0e/0s)\n","  Step 20/50 | World: 0.000012 | Contrast: 4.151185 | Import: 0.701 | LR: 3.0e-03 | Mem: 1280(1280w/0e/0s)\n","  Step 30/50 | World: 0.000011 | Contrast: 4.747889 | Import: 0.701 | LR: 3.0e-03 | Mem: 1920(1920w/0e/0s)\n","  Step 40/50 | World: 0.000003 | Contrast: 4.456876 | Import: 0.701 | LR: 3.0e-03 | Mem: 2560(2560w/0e/0s)\n","  Step 50/50 | World: 0.000001 | Contrast: 4.203395 | Import: 0.701 | LR: 3.0e-03 | Mem: 3200(3200w/0e/0s)\n","\n"," Episode 1 Summary:\n","   World Loss: 0.000067 ()\n","   Contrastive Loss: 4.517619\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 3200 total (3200 working, 0 episodic, 0 semantic)\n","   Time: 8.2s\n","\n","==================================================\n"," Episode 2/25\n","==================================================\n","  Step 10/50 | World: 0.000001 | Contrast: 5.167976 | Import: 0.701 | LR: 3.0e-03 | Mem: 3840(3840w/0e/0s)\n","  Step 20/50 | World: 0.000001 | Contrast: 4.232287 | Import: 0.701 | LR: 3.0e-03 | Mem: 4480(4480w/0e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.224020 | Import: 0.701 | LR: 3.0e-03 | Mem: 5120(5120w/0e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.305248 | Import: 0.701 | LR: 3.0e-03 | Mem: 5760(5760w/0e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.877451 | Import: 0.701 | LR: 3.0e-03 | Mem: 6400(6400w/0e/0s)\n","\n"," Episode 2 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.361810\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 6400 total (6400 working, 0 episodic, 0 semantic)\n","   Time: 9.1s\n","\n","==================================================\n"," Episode 3/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.283861 | Import: 0.701 | LR: 3.0e-03 | Mem: 7040(7040w/0e/0s)\n","  Step 20/50 | World: 0.000008 | Contrast: 10.989684 | Import: 0.701 | LR: 3.0e-03 | Mem: 7680(7680w/0e/0s)\n","  Step 30/50 | World: 0.000008 | Contrast: 5.314898 | Import: 0.701 | LR: 3.0e-03 | Mem: 8320(8320w/50e/0s)\n","  Step 40/50 | World: 0.000003 | Contrast: 4.734019 | Import: 0.701 | LR: 3.0e-03 | Mem: 8960(8960w/50e/0s)\n","  Step 50/50 | World: 0.000002 | Contrast: 4.538345 | Import: 0.701 | LR: 3.0e-03 | Mem: 9600(9600w/50e/0s)\n","\n"," Episode 3 Summary:\n","   World Loss: 0.000004 ()\n","   Contrastive Loss: 4.693595\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 9600 total (9600 working, 50 episodic, 0 semantic)\n","   Time: 10.2s\n","\n","==================================================\n"," Episode 4/25\n","==================================================\n","  Step 10/50 | World: 0.000001 | Contrast: 4.275846 | Import: 0.701 | LR: 3.0e-03 | Mem: 10240(10240w/50e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.198131 | Import: 0.701 | LR: 3.0e-03 | Mem: 10880(10880w/50e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.612385 | Import: 0.701 | LR: 3.0e-03 | Mem: 11520(11520w/50e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.248272 | Import: 0.701 | LR: 3.0e-03 | Mem: 12160(12160w/50e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.298498 | Import: 0.701 | LR: 3.0e-03 | Mem: 12800(12800w/50e/0s)\n","\n"," Episode 4 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.349191\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 12800 total (12800 working, 50 episodic, 0 semantic)\n","   Time: 11.3s\n","\n","==================================================\n"," Episode 5/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.426512 | Import: 0.701 | LR: 3.0e-03 | Mem: 13440(13440w/50e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.388881 | Import: 0.701 | LR: 3.0e-03 | Mem: 14080(14080w/50e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.403708 | Import: 0.701 | LR: 3.0e-03 | Mem: 14720(14720w/50e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.367402 | Import: 0.701 | LR: 3.0e-03 | Mem: 15360(15360w/50e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.480053 | Import: 0.701 | LR: 3.0e-03 | Mem: 16000(16000w/100e/0s)\n","\n"," Episode 5 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.503550\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 16000 total (16000 working, 100 episodic, 0 semantic)\n","   Time: 12.3s\n","\n","==================================================\n"," Episode 6/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.388365 | Import: 0.701 | LR: 3.0e-03 | Mem: 16640(16640w/100e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.356927 | Import: 0.701 | LR: 3.0e-03 | Mem: 17280(17280w/100e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.416345 | Import: 0.701 | LR: 3.0e-03 | Mem: 17920(17920w/100e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.462387 | Import: 0.701 | LR: 3.0e-03 | Mem: 18560(18560w/100e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.395585 | Import: 0.701 | LR: 3.0e-03 | Mem: 19200(19200w/100e/0s)\n","\n"," Episode 6 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.490352\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 19200 total (19200 working, 100 episodic, 0 semantic)\n","   Time: 13.4s\n","\n","==================================================\n"," Episode 7/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.510229 | Import: 0.701 | LR: 3.0e-03 | Mem: 19840(19840w/100e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.462812 | Import: 0.701 | LR: 3.0e-03 | Mem: 20480(20480w/100e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.433072 | Import: 0.701 | LR: 3.0e-03 | Mem: 21120(21120w/100e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.412513 | Import: 0.701 | LR: 3.0e-03 | Mem: 21760(21760w/100e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.449988 | Import: 0.701 | LR: 3.0e-03 | Mem: 22400(22400w/100e/0s)\n","\n"," Episode 7 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.481709\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 22400 total (22400 working, 100 episodic, 0 semantic)\n","   Time: 14.5s\n","\n","==================================================\n"," Episode 8/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.380123 | Import: 0.701 | LR: 3.0e-03 | Mem: 23040(23040w/100e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.335961 | Import: 0.701 | LR: 3.0e-03 | Mem: 23680(23680w/100e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.277328 | Import: 0.701 | LR: 3.0e-03 | Mem: 24320(24320w/150e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.281003 | Import: 0.701 | LR: 3.0e-03 | Mem: 24960(24960w/150e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.619826 | Import: 0.701 | LR: 3.0e-03 | Mem: 25600(25600w/150e/0s)\n","\n"," Episode 8 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.368561\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 25600 total (25600 working, 150 episodic, 0 semantic)\n","   Time: 15.6s\n","\n","==================================================\n"," Episode 9/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.272056 | Import: 0.701 | LR: 3.0e-03 | Mem: 26240(26240w/150e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.281710 | Import: 0.701 | LR: 3.0e-03 | Mem: 26880(26880w/150e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.346395 | Import: 0.701 | LR: 3.0e-03 | Mem: 27520(27520w/150e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.315937 | Import: 0.701 | LR: 3.0e-03 | Mem: 28160(28160w/150e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.337190 | Import: 0.701 | LR: 3.0e-03 | Mem: 28800(28800w/150e/0s)\n","\n"," Episode 9 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.391685\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 28800 total (28800 working, 150 episodic, 0 semantic)\n","   Time: 16.6s\n","\n","==================================================\n"," Episode 10/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.395876 | Import: 0.701 | LR: 3.0e-03 | Mem: 29440(29440w/150e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.385872 | Import: 0.701 | LR: 3.0e-03 | Mem: 30080(30080w/150e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.369643 | Import: 0.701 | LR: 3.0e-03 | Mem: 30720(30720w/150e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.362574 | Import: 0.701 | LR: 3.0e-03 | Mem: 31360(31360w/150e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.332620 | Import: 0.701 | LR: 3.0e-03 | Mem: 32000(32000w/200e/0s)\n","\n"," Episode 10 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.351771\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 32000 total (32000 working, 200 episodic, 0 semantic)\n","   Time: 17.7s\n","\n","==================================================\n"," Episode 11/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.374773 | Import: 0.701 | LR: 3.0e-03 | Mem: 32640(32640w/200e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.369340 | Import: 0.701 | LR: 3.0e-03 | Mem: 33280(33280w/200e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.317102 | Import: 0.701 | LR: 3.0e-03 | Mem: 33920(33920w/200e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.298215 | Import: 0.701 | LR: 3.0e-03 | Mem: 34560(34560w/200e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.312336 | Import: 0.701 | LR: 3.0e-03 | Mem: 35200(35200w/200e/0s)\n","\n"," Episode 11 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.341948\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 35200 total (35200 working, 200 episodic, 0 semantic)\n","   Time: 18.9s\n","\n","==================================================\n"," Episode 12/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.304917 | Import: 0.701 | LR: 3.0e-03 | Mem: 35840(35840w/200e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.355551 | Import: 0.701 | LR: 3.0e-03 | Mem: 36480(36480w/200e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.305123 | Import: 0.701 | LR: 3.0e-03 | Mem: 37120(37120w/200e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.298703 | Import: 0.701 | LR: 3.0e-03 | Mem: 37760(37760w/200e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.269867 | Import: 0.701 | LR: 3.0e-03 | Mem: 38400(38400w/200e/0s)\n","\n"," Episode 12 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.352659\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 38400 total (38400 working, 200 episodic, 0 semantic)\n","   Time: 20.3s\n","\n","==================================================\n"," Episode 13/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.212049 | Import: 0.701 | LR: 3.0e-03 | Mem: 39040(39040w/200e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.181470 | Import: 0.701 | LR: 3.0e-03 | Mem: 39680(39680w/200e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.234773 | Import: 0.701 | LR: 3.0e-03 | Mem: 40320(40320w/250e/10s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.231924 | Import: 0.701 | LR: 3.0e-03 | Mem: 40960(40960w/250e/10s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.270447 | Import: 0.701 | LR: 3.0e-03 | Mem: 41600(41600w/250e/10s)\n","\n"," Episode 13 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.247011\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 41600 total (41600 working, 250 episodic, 10 semantic)\n","   Time: 21.3s\n","\n","==================================================\n"," Episode 14/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.285048 | Import: 0.701 | LR: 3.0e-03 | Mem: 42240(42240w/250e/10s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.679658 | Import: 0.701 | LR: 3.0e-03 | Mem: 42880(42880w/250e/10s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.381091 | Import: 0.701 | LR: 3.0e-03 | Mem: 43520(43520w/250e/10s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.506091 | Import: 0.701 | LR: 3.0e-03 | Mem: 44160(44160w/250e/10s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.514270 | Import: 0.701 | LR: 3.0e-03 | Mem: 44800(44800w/250e/10s)\n","\n"," Episode 14 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.376946\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 44800 total (44800 working, 250 episodic, 10 semantic)\n","   Time: 22.5s\n","\n","==================================================\n"," Episode 15/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.577037 | Import: 0.701 | LR: 3.0e-03 | Mem: 45440(45440w/250e/10s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.624230 | Import: 0.701 | LR: 3.0e-03 | Mem: 46080(46080w/250e/10s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.653390 | Import: 0.701 | LR: 3.0e-03 | Mem: 46720(46720w/250e/10s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.829224 | Import: 0.701 | LR: 3.0e-03 | Mem: 47360(47360w/250e/10s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.686236 | Import: 0.701 | LR: 3.0e-03 | Mem: 48000(48000w/300e/20s)\n","\n"," Episode 15 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.670437\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 48000 total (48000 working, 300 episodic, 20 semantic)\n","   Time: 23.5s\n","\n","==================================================\n"," Episode 16/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.727494 | Import: 0.701 | LR: 2.4e-03 | Mem: 48640(48640w/300e/20s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.812451 | Import: 0.701 | LR: 2.4e-03 | Mem: 49280(49280w/300e/20s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.785791 | Import: 0.701 | LR: 2.4e-03 | Mem: 49920(49920w/300e/20s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.775002 | Import: 0.701 | LR: 2.4e-03 | Mem: 50560(50560w/300e/20s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.874274 | Import: 0.701 | LR: 2.4e-03 | Mem: 51200(51200w/300e/20s)\n","\n"," Episode 16 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.791385\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 51200 total (51200 working, 300 episodic, 20 semantic)\n","   Time: 24.7s\n","\n","==================================================\n"," Episode 17/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.878686 | Import: 0.701 | LR: 2.4e-03 | Mem: 51840(51840w/300e/20s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.829051 | Import: 0.701 | LR: 2.4e-03 | Mem: 52480(52480w/300e/20s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.820272 | Import: 0.701 | LR: 2.4e-03 | Mem: 53120(53120w/300e/20s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.852652 | Import: 0.701 | LR: 2.4e-03 | Mem: 53760(53760w/300e/20s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.837387 | Import: 0.701 | LR: 2.4e-03 | Mem: 54400(54400w/300e/20s)\n","\n"," Episode 17 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.842531\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 54400 total (54400 working, 300 episodic, 20 semantic)\n","   Time: 25.8s\n","\n","==================================================\n"," Episode 18/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.819549 | Import: 0.701 | LR: 2.4e-03 | Mem: 55040(55040w/300e/20s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.748913 | Import: 0.701 | LR: 2.4e-03 | Mem: 55680(55680w/300e/20s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.870566 | Import: 0.701 | LR: 2.4e-03 | Mem: 56320(56320w/350e/30s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.832967 | Import: 0.701 | LR: 2.4e-03 | Mem: 56960(56960w/350e/30s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.885463 | Import: 0.701 | LR: 2.4e-03 | Mem: 57600(57600w/350e/30s)\n","\n"," Episode 18 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.833854\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 57600 total (57600 working, 350 episodic, 30 semantic)\n","   Time: 26.8s\n","\n","==================================================\n"," Episode 19/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.840236 | Import: 0.701 | LR: 2.4e-03 | Mem: 58240(58240w/350e/30s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.826433 | Import: 0.701 | LR: 2.4e-03 | Mem: 58880(58880w/350e/30s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.812901 | Import: 0.701 | LR: 2.4e-03 | Mem: 59520(59520w/350e/30s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.837961 | Import: 0.701 | LR: 2.4e-03 | Mem: 60160(60160w/350e/30s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.812882 | Import: 0.701 | LR: 2.4e-03 | Mem: 60800(60800w/350e/30s)\n","\n"," Episode 19 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.838118\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 60800 total (60800 working, 350 episodic, 30 semantic)\n","   Time: 27.9s\n","\n","==================================================\n"," Episode 20/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.901753 | Import: 0.701 | LR: 2.4e-03 | Mem: 61440(61440w/350e/30s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.817242 | Import: 0.701 | LR: 2.4e-03 | Mem: 62080(62080w/350e/30s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.807875 | Import: 0.701 | LR: 2.4e-03 | Mem: 62720(62720w/350e/30s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.851939 | Import: 0.701 | LR: 2.4e-03 | Mem: 63360(63360w/350e/30s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.815592 | Import: 0.701 | LR: 2.4e-03 | Mem: 64000(64000w/400e/40s)\n","\n"," Episode 20 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.841806\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 64000 total (64000 working, 400 episodic, 40 semantic)\n","   Time: 28.9s\n","\n","==================================================\n"," Episode 21/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.861291 | Import: 0.701 | LR: 2.4e-03 | Mem: 64640(64640w/400e/40s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.850168 | Import: 0.701 | LR: 2.4e-03 | Mem: 65280(65280w/400e/40s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.809388 | Import: 0.701 | LR: 2.4e-03 | Mem: 65920(65920w/400e/40s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.832880 | Import: 0.701 | LR: 2.4e-03 | Mem: 66560(66560w/400e/40s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.850524 | Import: 0.701 | LR: 2.4e-03 | Mem: 67200(67200w/400e/40s)\n","\n"," Episode 21 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.842511\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 67200 total (67200 working, 400 episodic, 40 semantic)\n","   Time: 30.1s\n","\n","==================================================\n"," Episode 22/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.850268 | Import: 0.701 | LR: 2.4e-03 | Mem: 67840(67840w/400e/40s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.832978 | Import: 0.701 | LR: 2.4e-03 | Mem: 68480(68480w/400e/40s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.832952 | Import: 0.701 | LR: 2.4e-03 | Mem: 69120(69120w/400e/40s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.843568 | Import: 0.701 | LR: 2.4e-03 | Mem: 69760(69760w/400e/40s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.829824 | Import: 0.701 | LR: 2.4e-03 | Mem: 70400(70400w/400e/40s)\n","\n"," Episode 22 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.840219\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 70400 total (70400 working, 400 episodic, 40 semantic)\n","   Time: 31.1s\n","\n","==================================================\n"," Episode 23/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.860069 | Import: 0.701 | LR: 2.4e-03 | Mem: 71040(71040w/400e/40s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.809152 | Import: 0.701 | LR: 2.4e-03 | Mem: 71680(71680w/400e/40s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.868425 | Import: 0.701 | LR: 2.4e-03 | Mem: 72320(72320w/450e/50s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.865582 | Import: 0.701 | LR: 2.4e-03 | Mem: 72960(72960w/450e/50s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.843631 | Import: 0.701 | LR: 2.4e-03 | Mem: 73600(73600w/450e/50s)\n","\n"," Episode 23 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.844951\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 73600 total (73600 working, 450 episodic, 50 semantic)\n","   Time: 32.3s\n","\n","==================================================\n"," Episode 24/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.862121 | Import: 0.701 | LR: 2.4e-03 | Mem: 74240(74240w/450e/50s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.856493 | Import: 0.701 | LR: 2.4e-03 | Mem: 74880(74880w/450e/50s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.843360 | Import: 0.701 | LR: 2.4e-03 | Mem: 75520(75520w/450e/50s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.849378 | Import: 0.701 | LR: 2.4e-03 | Mem: 76160(76160w/450e/50s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.849582 | Import: 0.701 | LR: 2.4e-03 | Mem: 76800(76800w/450e/50s)\n","\n"," Episode 24 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.846311\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 76800 total (76800 working, 450 episodic, 50 semantic)\n","   Time: 33.5s\n","\n","==================================================\n"," Episode 25/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.827705 | Import: 0.701 | LR: 2.4e-03 | Mem: 77440(77440w/450e/50s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.844784 | Import: 0.701 | LR: 2.4e-03 | Mem: 78080(78080w/450e/50s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.860411 | Import: 0.701 | LR: 2.4e-03 | Mem: 78720(78720w/450e/50s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.864577 | Import: 0.701 | LR: 2.4e-03 | Mem: 79360(79360w/450e/50s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.825634 | Import: 0.701 | LR: 2.4e-03 | Mem: 80000(80000w/500e/60s)\n","\n"," Episode 25 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.846925\n","   Avg Importance: 0.701\n","   Learning Rate: 2.39e-03\n","   Memory: 80000 total (80000 working, 500 episodic, 60 semantic)\n","   Time: 34.8s\n","\n","============================================================\n"," ENHANCED GRLM TRAINING COMPLETE!\n","============================================================\n"," Final Statistics:\n","   Episodes completed: 25\n","   Total training steps: 1,250\n","   Final loss: 0.000000\n","   Best loss achieved: 0.000000\n","   Final learning rate: 2.39e-03\n","   Average importance score: 0.701\n","   Final memory state:\n","      Total nodes: 80,000\n","      Total edges: 639,488\n","      Working memory: 80,000\n","      Episodic memory: 500\n","      Semantic memory: 60\n","      Avg node importance: 0.701\n","   Total training time: 531.4s (8.9 minutes)\n","   Average time per episode: 21.3s\n","\n"," Enhanced GRLM features successfully demonstrated:\n","    Hierarchical memory with consolidation\n","    Contrastive learning for better representations\n","    Attention-based transformer encoding\n","    Importance-based memory management\n","    Adaptive learning rate scheduling\n","    Comprehensive monitoring and early stopping\n","\n"," Additional Analysis:\n","Memory distribution: 100.0% working, 0.6% episodic, 0.1% semantic\n","Overall improvement: 100.0% loss reduction\n","\n"," Enhanced GRLM is ready for further experimentation!\n","\n","============================================================\n"," ENHANCED GRLM TRAINING STARTING\n","============================================================\n","Episodes: 25 | Steps per episode: 50\n","Embedding dim: 256 | Objects per step: 64\n","Memory capacity: 100,000 nodes\n","Device: cuda\n"," Enhanced system initialized\n","Model parameters: 1,249,792\n","\n","==================================================\n"," Episode 1/25\n","==================================================\n","  Step 10/50 | World: 0.000199 | Contrast: 4.539925 | Import: 0.701 | LR: 3.0e-03 | Mem: 640(640w/0e/0s)\n","  Step 20/50 | World: 0.000033 | Contrast: 5.511609 | Import: 0.701 | LR: 3.0e-03 | Mem: 1280(1280w/0e/0s)\n","  Step 30/50 | World: 0.000007 | Contrast: 4.710287 | Import: 0.701 | LR: 3.0e-03 | Mem: 1920(1920w/0e/0s)\n","  Step 40/50 | World: 0.000005 | Contrast: 4.402619 | Import: 0.701 | LR: 3.0e-03 | Mem: 2560(2560w/0e/0s)\n","  Step 50/50 | World: 0.000007 | Contrast: 4.186830 | Import: 0.701 | LR: 3.0e-03 | Mem: 3200(3200w/0e/0s)\n","\n"," Episode 1 Summary:\n","   World Loss: 0.000100 ()\n","   Contrastive Loss: 4.545615\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 3200 total (3200 working, 0 episodic, 0 semantic)\n","   Time: 7.8s\n","\n","==================================================\n"," Episode 2/25\n","==================================================\n","  Step 10/50 | World: 0.000002 | Contrast: 4.338864 | Import: 0.701 | LR: 3.0e-03 | Mem: 3840(3840w/0e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.174580 | Import: 0.701 | LR: 3.0e-03 | Mem: 4480(4480w/0e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.234097 | Import: 0.701 | LR: 3.0e-03 | Mem: 5120(5120w/0e/0s)\n","  Step 40/50 | World: 0.000002 | Contrast: 4.246253 | Import: 0.701 | LR: 3.0e-03 | Mem: 5760(5760w/0e/0s)\n","  Step 50/50 | World: 0.000001 | Contrast: 4.169540 | Import: 0.701 | LR: 3.0e-03 | Mem: 6400(6400w/0e/0s)\n","\n"," Episode 2 Summary:\n","   World Loss: 0.000001 ()\n","   Contrastive Loss: 4.552723\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 6400 total (6400 working, 0 episodic, 0 semantic)\n","   Time: 9.3s\n","\n","==================================================\n"," Episode 3/25\n","==================================================\n","  Step 10/50 | World: 0.000001 | Contrast: 4.496611 | Import: 0.701 | LR: 3.0e-03 | Mem: 7040(7040w/0e/0s)\n","  Step 20/50 | World: 0.000001 | Contrast: 4.165091 | Import: 0.701 | LR: 3.0e-03 | Mem: 7680(7680w/0e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 6.367728 | Import: 0.701 | LR: 3.0e-03 | Mem: 8320(8320w/50e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.577298 | Import: 0.701 | LR: 3.0e-03 | Mem: 8960(8960w/50e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.424294 | Import: 0.701 | LR: 3.0e-03 | Mem: 9600(9600w/50e/0s)\n","\n"," Episode 3 Summary:\n","   World Loss: 0.000001 ()\n","   Contrastive Loss: 4.699580\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 9600 total (9600 working, 50 episodic, 0 semantic)\n","   Time: 10.6s\n","\n","==================================================\n"," Episode 4/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.531661 | Import: 0.701 | LR: 3.0e-03 | Mem: 10240(10240w/50e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.608350 | Import: 0.701 | LR: 3.0e-03 | Mem: 10880(10880w/50e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.510564 | Import: 0.701 | LR: 3.0e-03 | Mem: 11520(11520w/50e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.466442 | Import: 0.701 | LR: 3.0e-03 | Mem: 12160(12160w/50e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.379304 | Import: 0.701 | LR: 3.0e-03 | Mem: 12800(12800w/50e/0s)\n","\n"," Episode 4 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.510481\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 12800 total (12800 working, 50 episodic, 0 semantic)\n","   Time: 11.7s\n","\n","==================================================\n"," Episode 5/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.351725 | Import: 0.701 | LR: 3.0e-03 | Mem: 13440(13440w/50e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.376198 | Import: 0.701 | LR: 3.0e-03 | Mem: 14080(14080w/50e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.299187 | Import: 0.701 | LR: 3.0e-03 | Mem: 14720(14720w/50e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.339207 | Import: 0.701 | LR: 3.0e-03 | Mem: 15360(15360w/50e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.774614 | Import: 0.701 | LR: 3.0e-03 | Mem: 16000(16000w/100e/0s)\n","\n"," Episode 5 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.482748\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 16000 total (16000 working, 100 episodic, 0 semantic)\n","   Time: 12.6s\n","\n","==================================================\n"," Episode 6/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.378433 | Import: 0.701 | LR: 3.0e-03 | Mem: 16640(16640w/100e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.393297 | Import: 0.701 | LR: 3.0e-03 | Mem: 17280(17280w/100e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.397196 | Import: 0.701 | LR: 3.0e-03 | Mem: 17920(17920w/100e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.373853 | Import: 0.701 | LR: 3.0e-03 | Mem: 18560(18560w/100e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.395242 | Import: 0.701 | LR: 3.0e-03 | Mem: 19200(19200w/100e/0s)\n","\n"," Episode 6 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.494765\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 19200 total (19200 working, 100 episodic, 0 semantic)\n","   Time: 13.6s\n","\n","==================================================\n"," Episode 7/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.369861 | Import: 0.701 | LR: 3.0e-03 | Mem: 19840(19840w/100e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 5.342167 | Import: 0.701 | LR: 3.0e-03 | Mem: 20480(20480w/100e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.309480 | Import: 0.701 | LR: 3.0e-03 | Mem: 21120(21120w/100e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.746086 | Import: 0.701 | LR: 3.0e-03 | Mem: 21760(21760w/100e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.227080 | Import: 0.701 | LR: 3.0e-03 | Mem: 22400(22400w/100e/0s)\n","\n"," Episode 7 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.388459\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 22400 total (22400 working, 100 episodic, 0 semantic)\n","   Time: 14.6s\n","\n","==================================================\n"," Episode 8/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.233782 | Import: 0.701 | LR: 3.0e-03 | Mem: 23040(23040w/100e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.269875 | Import: 0.701 | LR: 3.0e-03 | Mem: 23680(23680w/100e/0s)\n","  Step 30/50 | World: 0.000001 | Contrast: 5.617553 | Import: 0.701 | LR: 3.0e-03 | Mem: 24320(24320w/150e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.348955 | Import: 0.701 | LR: 3.0e-03 | Mem: 24960(24960w/150e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.326201 | Import: 0.701 | LR: 3.0e-03 | Mem: 25600(25600w/150e/0s)\n","\n"," Episode 8 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.342473\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 25600 total (25600 working, 150 episodic, 0 semantic)\n","   Time: 15.8s\n","\n","==================================================\n"," Episode 9/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.368868 | Import: 0.701 | LR: 3.0e-03 | Mem: 26240(26240w/150e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.351886 | Import: 0.701 | LR: 3.0e-03 | Mem: 26880(26880w/150e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.336150 | Import: 0.701 | LR: 3.0e-03 | Mem: 27520(27520w/150e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.305519 | Import: 0.701 | LR: 3.0e-03 | Mem: 28160(28160w/150e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.310707 | Import: 0.701 | LR: 3.0e-03 | Mem: 28800(28800w/150e/0s)\n","\n"," Episode 9 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.387731\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 28800 total (28800 working, 150 episodic, 0 semantic)\n","   Time: 16.7s\n","\n","==================================================\n"," Episode 10/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.315102 | Import: 0.701 | LR: 3.0e-03 | Mem: 29440(29440w/150e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.398769 | Import: 0.701 | LR: 3.0e-03 | Mem: 30080(30080w/150e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.386172 | Import: 0.701 | LR: 3.0e-03 | Mem: 30720(30720w/150e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.456412 | Import: 0.701 | LR: 3.0e-03 | Mem: 31360(31360w/150e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.420950 | Import: 0.701 | LR: 3.0e-03 | Mem: 32000(32000w/200e/0s)\n","\n"," Episode 10 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.405366\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 32000 total (32000 working, 200 episodic, 0 semantic)\n","   Time: 17.7s\n","\n","==================================================\n"," Episode 11/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.391244 | Import: 0.701 | LR: 3.0e-03 | Mem: 32640(32640w/200e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.225979 | Import: 0.701 | LR: 3.0e-03 | Mem: 33280(33280w/200e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.214125 | Import: 0.701 | LR: 3.0e-03 | Mem: 33920(33920w/200e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.345630 | Import: 0.701 | LR: 3.0e-03 | Mem: 34560(34560w/200e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.242922 | Import: 0.701 | LR: 3.0e-03 | Mem: 35200(35200w/200e/0s)\n","\n"," Episode 11 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.288192\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 35200 total (35200 working, 200 episodic, 0 semantic)\n","   Time: 19.0s\n","\n","==================================================\n"," Episode 12/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.298565 | Import: 0.701 | LR: 3.0e-03 | Mem: 35840(35840w/200e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.356337 | Import: 0.701 | LR: 3.0e-03 | Mem: 36480(36480w/200e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.380025 | Import: 0.701 | LR: 3.0e-03 | Mem: 37120(37120w/200e/0s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.437061 | Import: 0.701 | LR: 3.0e-03 | Mem: 37760(37760w/200e/0s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.531085 | Import: 0.701 | LR: 3.0e-03 | Mem: 38400(38400w/200e/0s)\n","\n"," Episode 12 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.453755\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 38400 total (38400 working, 200 episodic, 0 semantic)\n","   Time: 20.2s\n","\n","==================================================\n"," Episode 13/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.547808 | Import: 0.701 | LR: 3.0e-03 | Mem: 39040(39040w/200e/0s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.458776 | Import: 0.701 | LR: 3.0e-03 | Mem: 39680(39680w/200e/0s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.463353 | Import: 0.701 | LR: 3.0e-03 | Mem: 40320(40320w/250e/10s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.372057 | Import: 0.701 | LR: 3.0e-03 | Mem: 40960(40960w/250e/10s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.349339 | Import: 0.701 | LR: 3.0e-03 | Mem: 41600(41600w/250e/10s)\n","\n"," Episode 13 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.436421\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 41600 total (41600 working, 250 episodic, 10 semantic)\n","   Time: 21.4s\n","\n","==================================================\n"," Episode 14/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.373259 | Import: 0.701 | LR: 3.0e-03 | Mem: 42240(42240w/250e/10s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.381211 | Import: 0.701 | LR: 3.0e-03 | Mem: 42880(42880w/250e/10s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.428993 | Import: 0.701 | LR: 3.0e-03 | Mem: 43520(43520w/250e/10s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.418649 | Import: 0.701 | LR: 3.0e-03 | Mem: 44160(44160w/250e/10s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.501068 | Import: 0.701 | LR: 3.0e-03 | Mem: 44800(44800w/250e/10s)\n","\n"," Episode 14 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.419799\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 44800 total (44800 working, 250 episodic, 10 semantic)\n","   Time: 22.5s\n","\n","==================================================\n"," Episode 15/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.552651 | Import: 0.701 | LR: 3.0e-03 | Mem: 45440(45440w/250e/10s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.503336 | Import: 0.701 | LR: 3.0e-03 | Mem: 46080(46080w/250e/10s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.448119 | Import: 0.701 | LR: 3.0e-03 | Mem: 46720(46720w/250e/10s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.421060 | Import: 0.701 | LR: 3.0e-03 | Mem: 47360(47360w/250e/10s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.266546 | Import: 0.701 | LR: 3.0e-03 | Mem: 48000(48000w/300e/20s)\n","\n"," Episode 15 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.447414\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 48000 total (48000 working, 300 episodic, 20 semantic)\n","   Time: 23.5s\n","\n","==================================================\n"," Episode 16/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.229366 | Import: 0.701 | LR: 3.0e-03 | Mem: 48640(48640w/300e/20s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.277308 | Import: 0.701 | LR: 3.0e-03 | Mem: 49280(49280w/300e/20s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.291810 | Import: 0.701 | LR: 3.0e-03 | Mem: 49920(49920w/300e/20s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.344126 | Import: 0.701 | LR: 3.0e-03 | Mem: 50560(50560w/300e/20s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.395661 | Import: 0.701 | LR: 3.0e-03 | Mem: 51200(51200w/300e/20s)\n","\n"," Episode 16 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.293483\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 51200 total (51200 working, 300 episodic, 20 semantic)\n","   Time: 24.7s\n","\n","==================================================\n"," Episode 17/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.337047 | Import: 0.701 | LR: 3.0e-03 | Mem: 51840(51840w/300e/20s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.385786 | Import: 0.701 | LR: 3.0e-03 | Mem: 52480(52480w/300e/20s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.471634 | Import: 0.701 | LR: 3.0e-03 | Mem: 53120(53120w/300e/20s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.371659 | Import: 0.701 | LR: 3.0e-03 | Mem: 53760(53760w/300e/20s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.283217 | Import: 0.701 | LR: 3.0e-03 | Mem: 54400(54400w/300e/20s)\n","\n"," Episode 17 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.399936\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 54400 total (54400 working, 300 episodic, 20 semantic)\n","   Time: 25.7s\n","\n","==================================================\n"," Episode 18/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.293893 | Import: 0.701 | LR: 3.0e-03 | Mem: 55040(55040w/300e/20s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.303226 | Import: 0.701 | LR: 3.0e-03 | Mem: 55680(55680w/300e/20s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.287198 | Import: 0.701 | LR: 3.0e-03 | Mem: 56320(56320w/350e/30s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.378818 | Import: 0.701 | LR: 3.0e-03 | Mem: 56960(56960w/350e/30s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.405500 | Import: 0.701 | LR: 3.0e-03 | Mem: 57600(57600w/350e/30s)\n","\n"," Episode 18 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.321483\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 57600 total (57600 working, 350 episodic, 30 semantic)\n","   Time: 26.7s\n","\n","==================================================\n"," Episode 19/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.463610 | Import: 0.701 | LR: 3.0e-03 | Mem: 58240(58240w/350e/30s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.483303 | Import: 0.701 | LR: 3.0e-03 | Mem: 58880(58880w/350e/30s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.517731 | Import: 0.701 | LR: 3.0e-03 | Mem: 59520(59520w/350e/30s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.461379 | Import: 0.701 | LR: 3.0e-03 | Mem: 60160(60160w/350e/30s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.342192 | Import: 0.701 | LR: 3.0e-03 | Mem: 60800(60800w/350e/30s)\n","\n"," Episode 19 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.451736\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 60800 total (60800 working, 350 episodic, 30 semantic)\n","   Time: 27.7s\n","\n","==================================================\n"," Episode 20/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.306155 | Import: 0.701 | LR: 3.0e-03 | Mem: 61440(61440w/350e/30s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.346462 | Import: 0.701 | LR: 3.0e-03 | Mem: 62080(62080w/350e/30s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.348747 | Import: 0.701 | LR: 3.0e-03 | Mem: 62720(62720w/350e/30s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.348439 | Import: 0.701 | LR: 3.0e-03 | Mem: 63360(63360w/350e/30s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.376362 | Import: 0.701 | LR: 3.0e-03 | Mem: 64000(64000w/400e/40s)\n","\n"," Episode 20 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.319858\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 64000 total (64000 working, 400 episodic, 40 semantic)\n","   Time: 28.8s\n","\n","==================================================\n"," Episode 21/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.469792 | Import: 0.701 | LR: 3.0e-03 | Mem: 64640(64640w/400e/40s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.437757 | Import: 0.701 | LR: 3.0e-03 | Mem: 65280(65280w/400e/40s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.422858 | Import: 0.701 | LR: 3.0e-03 | Mem: 65920(65920w/400e/40s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.381222 | Import: 0.701 | LR: 3.0e-03 | Mem: 66560(66560w/400e/40s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.305697 | Import: 0.701 | LR: 3.0e-03 | Mem: 67200(67200w/400e/40s)\n","\n"," Episode 21 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.422425\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 67200 total (67200 working, 400 episodic, 40 semantic)\n","   Time: 29.9s\n","\n","==================================================\n"," Episode 22/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.324596 | Import: 0.701 | LR: 3.0e-03 | Mem: 67840(67840w/400e/40s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.331223 | Import: 0.701 | LR: 3.0e-03 | Mem: 68480(68480w/400e/40s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.386396 | Import: 0.701 | LR: 3.0e-03 | Mem: 69120(69120w/400e/40s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.557676 | Import: 0.701 | LR: 3.0e-03 | Mem: 69760(69760w/400e/40s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.589054 | Import: 0.701 | LR: 3.0e-03 | Mem: 70400(70400w/400e/40s)\n","\n"," Episode 22 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.386825\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 70400 total (70400 working, 400 episodic, 40 semantic)\n","   Time: 31.0s\n","\n","==================================================\n"," Episode 23/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.655539 | Import: 0.701 | LR: 3.0e-03 | Mem: 71040(71040w/400e/40s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.632482 | Import: 0.701 | LR: 3.0e-03 | Mem: 71680(71680w/400e/40s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.647344 | Import: 0.701 | LR: 3.0e-03 | Mem: 72320(72320w/450e/50s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.538615 | Import: 0.701 | LR: 3.0e-03 | Mem: 72960(72960w/450e/50s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.464221 | Import: 0.701 | LR: 3.0e-03 | Mem: 73600(73600w/450e/50s)\n","\n"," Episode 23 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.590568\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 73600 total (73600 working, 450 episodic, 50 semantic)\n","   Time: 32.3s\n","\n","==================================================\n"," Episode 24/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.572178 | Import: 0.701 | LR: 3.0e-03 | Mem: 74240(74240w/450e/50s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.503624 | Import: 0.701 | LR: 3.0e-03 | Mem: 74880(74880w/450e/50s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.959488 | Import: 0.701 | LR: 3.0e-03 | Mem: 75520(75520w/450e/50s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.328151 | Import: 0.701 | LR: 3.0e-03 | Mem: 76160(76160w/450e/50s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.244452 | Import: 0.701 | LR: 3.0e-03 | Mem: 76800(76800w/450e/50s)\n","\n"," Episode 24 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.540583\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 76800 total (76800 working, 450 episodic, 50 semantic)\n","   Time: 33.4s\n","\n","==================================================\n"," Episode 25/25\n","==================================================\n","  Step 10/50 | World: 0.000000 | Contrast: 4.676590 | Import: 0.701 | LR: 3.0e-03 | Mem: 77440(77440w/450e/50s)\n","  Step 20/50 | World: 0.000000 | Contrast: 4.674603 | Import: 0.701 | LR: 3.0e-03 | Mem: 78080(78080w/450e/50s)\n","  Step 30/50 | World: 0.000000 | Contrast: 4.468554 | Import: 0.701 | LR: 3.0e-03 | Mem: 78720(78720w/450e/50s)\n","  Step 40/50 | World: 0.000000 | Contrast: 4.345992 | Import: 0.701 | LR: 3.0e-03 | Mem: 79360(79360w/450e/50s)\n","  Step 50/50 | World: 0.000000 | Contrast: 4.264669 | Import: 0.701 | LR: 3.0e-03 | Mem: 80000(80000w/500e/60s)\n","\n"," Episode 25 Summary:\n","   World Loss: 0.000000 ()\n","   Contrastive Loss: 4.481623\n","   Avg Importance: 0.701\n","   Learning Rate: 2.99e-03\n","   Memory: 80000 total (80000 working, 500 episodic, 60 semantic)\n","   Time: 34.7s\n","\n","============================================================\n"," ENHANCED GRLM TRAINING COMPLETE!\n","============================================================\n"," Final Statistics:\n","   Episodes completed: 25\n","   Total training steps: 1,250\n","   Final loss: 0.000000\n","   Best loss achieved: 0.000000\n","   Final learning rate: 2.99e-03\n","   Average importance score: 0.701\n","   Final memory state:\n","      Total nodes: 80,000\n","      Total edges: 639,488\n","      Working memory: 80,000\n","      Episodic memory: 500\n","      Semantic memory: 60\n","      Avg node importance: 0.701\n","   Total training time: 531.8s (8.9 minutes)\n","   Average time per episode: 21.3s\n","\n"," Enhanced GRLM features successfully demonstrated:\n","    Hierarchical memory with consolidation\n","    Contrastive learning for better representations\n","    Attention-based transformer encoding\n","    Importance-based memory management\n","    Adaptive learning rate scheduling\n","    Comprehensive monitoring and early stopping\n"]}],"source":["# @title\n","# Complete Enhanced GRLM - Single Colab Cell Implementation (FIXED)\n","# Copy and run this entire cell in Google Colab\n","\n","import os\n","import math\n","import time\n","import json\n","import random\n","import tempfile\n","import warnings\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Optional, Tuple, Dict, List\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# ============================================================================\n","# PERFORMANCE OPTIMIZATIONS & SETUP\n","# ============================================================================\n","\n","# Enable TF32 for faster training on Ampere+ GPUs\n","if torch.cuda.is_available():\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    try:\n","        torch.set_float32_matmul_precision(\"high\")\n","    except AttributeError:\n","        pass\n","\n","# Device setup\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","USE_AMP = torch.cuda.is_available()\n","AMP_DTYPE = torch.bfloat16\n","\n","# Configuration\n","SEED = 1337\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","# Training parameters\n","EPISODES = 25\n","STEPS_PER_EP = 50\n","EMB_DIM = 256\n","K_NEI = 4\n","CAND_RECENT = 384\n","CAND_RANDOM = 128\n","MAX_NODES = 100_000  # Reduced for Colab\n","N_OBJECTS_PER_STEP = 64\n","LR = 2.99e-3\n","\n","print(f\" Enhanced GRLM Starting\")\n","print(f\"Device: {DEVICE}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n","\n","# ============================================================================\n","# ENHANCED MEMORY SYSTEM\n","# ============================================================================\n","\n","class EnhancedGraphMemory:\n","    \"\"\"Advanced graph memory with hierarchical storage and consolidation.\"\"\"\n","\n","    def __init__(self, dim: int, max_nodes: int = MAX_NODES):\n","        self.dim = dim\n","        self.max_nodes = max_nodes\n","        self.nodes = []\n","        self.edges = []\n","\n","        # Enhanced tracking\n","        self.node_timestamps = []\n","        self.node_importance = []\n","        self.access_counts = []\n","\n","        # Memory levels for consolidation\n","        self.levels = {\n","            'working': {'nodes': [], 'max_size': max_nodes // 2},\n","            'episodic': {'nodes': [], 'max_size': max_nodes // 4},\n","            'semantic': {'nodes': [], 'max_size': max_nodes // 4}\n","        }\n","\n","        self._faiss_available = self._check_faiss()\n","        self._faiss_index = None\n","\n","    def _check_faiss(self):\n","        \"\"\"Check if FAISS is available.\"\"\"\n","        try:\n","            import faiss\n","            return True\n","        except ImportError:\n","            return False\n","\n","    def add_batch(self, batch_vecs: np.ndarray, k_nei: int = K_NEI,\n","                  cand_recent: int = CAND_RECENT, cand_random: int = CAND_RANDOM,\n","                  importance: float = 1.0):\n","        \"\"\"Enhanced add_batch with importance scoring.\"\"\"\n","\n","        current_time = time.time()\n","        start_idx = len(self.nodes)\n","\n","        # Add nodes with metadata\n","        for vec in batch_vecs:\n","            self.nodes.append(vec.astype(np.float32))\n","            self.node_timestamps.append(current_time)\n","            self.node_importance.append(importance)\n","            self.access_counts.append(1)\n","\n","            # Add to working memory level\n","            self.levels['working']['nodes'].append(vec.astype(np.float32))\n","\n","        # Connect new nodes with importance weighting\n","        self._connect_nodes_smart(start_idx, k_nei, cand_recent, cand_random)\n","\n","        # Smart trimming based on importance\n","        self._smart_trim()\n","\n","        # Periodic consolidation\n","        if len(self.nodes) % 1000 == 0 and len(self.nodes) > 0:\n","            self._consolidate_memories()\n","\n","        self._faiss_index = None  # Force rebuild\n","\n","    def _connect_nodes_smart(self, start_idx: int, k_nei: int, cand_recent: int, cand_random: int):\n","        \"\"\"Smart node connection with importance weighting.\"\"\"\n","\n","        if start_idx == 0:\n","            return\n","\n","        all_vecs = np.array(self.nodes, dtype=np.float32)\n","\n","        for new_id in range(start_idx, len(self.nodes)):\n","            # Build weighted candidate pool\n","            recent_start = max(0, len(self.nodes) - cand_recent - (len(self.nodes) - new_id))\n","            recent_ids = list(range(recent_start, new_id))\n","\n","            # Weight by importance and access patterns\n","            weighted_candidates = []\n","            for idx in recent_ids:\n","                weight = self.node_importance[idx] * np.log(1 + self.access_counts[idx])\n","                weighted_candidates.append((weight, idx))\n","\n","            # Sort and take top candidates\n","            weighted_candidates.sort(reverse=True)\n","            top_candidates = [idx for _, idx in weighted_candidates[:min(k_nei * 2, len(weighted_candidates))]]\n","\n","            # Add random older nodes\n","            older_pool = list(range(0, recent_start))\n","            if cand_random > 0 and older_pool:\n","                random_count = min(cand_random, len(older_pool))\n","                random_ids = np.random.choice(older_pool, size=random_count, replace=False).tolist()\n","                top_candidates.extend(random_ids)\n","\n","            if not top_candidates:\n","                continue\n","\n","            # Find k nearest neighbors using cosine similarity\n","            query_vec = all_vecs[new_id:new_id+1]\n","            cand_vecs = all_vecs[top_candidates]\n","\n","            # Normalize for cosine similarity\n","            query_norm = query_vec / (np.linalg.norm(query_vec, axis=1, keepdims=True) + 1e-9)\n","            cand_norm = cand_vecs / (np.linalg.norm(cand_vecs, axis=1, keepdims=True) + 1e-9)\n","\n","            similarities = np.dot(query_norm, cand_norm.T)[0]\n","            top_k_indices = np.argsort(similarities)[-k_nei:]\n","\n","            # Create edges and update access counts\n","            for idx in top_k_indices:\n","                neighbor_id = top_candidates[idx]\n","                self.edges.append((new_id, neighbor_id))\n","                self.edges.append((neighbor_id, new_id))\n","                self.access_counts[neighbor_id] += 1\n","\n","    def _smart_trim(self):\n","        \"\"\"Intelligent trimming based on composite scores.\"\"\"\n","\n","        if len(self.nodes) <= self.max_nodes:\n","            return\n","\n","        current_time = time.time()\n","\n","        # Calculate composite scores\n","        scores = []\n","        for i, (timestamp, importance, access_count) in enumerate(\n","            zip(self.node_timestamps, self.node_importance, self.access_counts)):\n","\n","            age = current_time - timestamp\n","            recency_score = np.exp(-age / 3600)  # Decay per hour\n","            access_score = np.log(1 + access_count)\n","            composite_score = importance * (0.4 * recency_score + 0.6 * access_score)\n","            scores.append((composite_score, i))\n","\n","        # Keep top nodes\n","        scores.sort(reverse=True)\n","        keep_indices = [idx for _, idx in scores[:self.max_nodes]]\n","        keep_set = set(keep_indices)\n","\n","        # Update all lists\n","        self.nodes = [self.nodes[i] for i in keep_indices]\n","        self.node_timestamps = [self.node_timestamps[i] for i in keep_indices]\n","        self.node_importance = [self.node_importance[i] for i in keep_indices]\n","        self.access_counts = [self.access_counts[i] for i in keep_indices]\n","\n","        # Remap edges\n","        index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(keep_indices)}\n","        new_edges = []\n","        for src, dst in self.edges:\n","            if src in index_map and dst in index_map:\n","                new_edges.append((index_map[src], index_map[dst]))\n","        self.edges = new_edges\n","\n","    def _consolidate_memories(self):\n","        \"\"\"Consolidate working memory to episodic and semantic.\"\"\"\n","\n","        working_nodes = self.levels['working']['nodes']\n","        if len(working_nodes) > 100:\n","            # Move high-importance nodes to episodic\n","            high_importance_indices = [\n","                i for i, imp in enumerate(self.node_importance[-len(working_nodes):])\n","                if imp > 0.7\n","            ]\n","\n","            for idx in high_importance_indices[:50]:\n","                if len(self.levels['episodic']['nodes']) < self.levels['episodic']['max_size']:\n","                    node_idx = len(self.node_importance) - len(working_nodes) + idx\n","                    self.levels['episodic']['nodes'].append(working_nodes[idx])\n","\n","        # Create semantic prototypes\n","        if len(self.levels['episodic']['nodes']) > 200:\n","            self._create_semantic_prototypes()\n","\n","    def _create_semantic_prototypes(self):\n","        \"\"\"Simple clustering to create semantic prototypes.\"\"\"\n","\n","        episodic_nodes = np.array(self.levels['episodic']['nodes'][-100:])\n","        n_clusters = min(10, len(episodic_nodes) // 10)\n","\n","        if n_clusters < 2:\n","            return\n","\n","        # Simple k-means\n","        centroids = episodic_nodes[np.random.choice(len(episodic_nodes), n_clusters, replace=False)]\n","\n","        for _ in range(10):\n","            distances = np.linalg.norm(episodic_nodes[:, None] - centroids, axis=2)\n","            assignments = np.argmin(distances, axis=1)\n","\n","            new_centroids = np.zeros_like(centroids)\n","            for k in range(n_clusters):\n","                mask = assignments == k\n","                if mask.any():\n","                    new_centroids[k] = episodic_nodes[mask].mean(axis=0)\n","                else:\n","                    new_centroids[k] = centroids[k]\n","\n","            if np.allclose(centroids, new_centroids, atol=1e-4):\n","                break\n","            centroids = new_centroids\n","\n","        # Add to semantic memory\n","        for centroid in centroids:\n","            if len(self.levels['semantic']['nodes']) < self.levels['semantic']['max_size']:\n","                self.levels['semantic']['nodes'].append(centroid.astype(np.float32))\n","\n","    def get_memory_stats(self) -> Dict:\n","        \"\"\"Get comprehensive memory statistics.\"\"\"\n","        return {\n","            'total_nodes': len(self.nodes),\n","            'total_edges': len(self.edges),\n","            'working_memory': len(self.levels['working']['nodes']),\n","            'episodic_memory': len(self.levels['episodic']['nodes']),\n","            'semantic_memory': len(self.levels['semantic']['nodes']),\n","            'avg_importance': float(np.mean(self.node_importance)) if self.node_importance else 0.0,\n","            'avg_access_count': float(np.mean(self.access_counts)) if self.access_counts else 0.0\n","        }\n","\n","# ============================================================================\n","# CONTRASTIVE LEARNING\n","# ============================================================================\n","\n","class ContrastiveLearning:\n","    \"\"\"InfoNCE contrastive learning for better representations.\"\"\"\n","\n","    def __init__(self, temperature: float = 0.07, queue_size: int = 1000):\n","        self.temperature = temperature\n","        self.embedding_queue = []\n","        self.max_queue_size = queue_size\n","\n","    def compute_contrastive_loss(self, embeddings: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Compute InfoNCE contrastive loss.\"\"\"\n","\n","        if len(embeddings.shape) == 3:\n","            # Flatten: [batch, seq, dim] -> [batch*seq, dim]\n","            batch_size, seq_len, dim = embeddings.shape\n","            embeddings = embeddings.view(-1, dim)\n","\n","        batch_size, dim = embeddings.shape\n","\n","        if batch_size < 2:\n","            return torch.tensor(0.0, device=embeddings.device)\n","\n","        # Normalize embeddings\n","        embeddings = F.normalize(embeddings, dim=-1)\n","\n","        # Create positive pairs (consecutive embeddings)\n","        anchors = embeddings[:-1]\n","        positives = embeddings[1:]\n","\n","        if len(anchors) == 0:\n","            return torch.tensor(0.0, device=embeddings.device)\n","\n","        total_loss = 0.0\n","        num_pairs = 0\n","\n","        for i, (anchor, positive) in enumerate(zip(anchors, positives)):\n","            # Positive similarity\n","            pos_sim = torch.dot(anchor, positive) / self.temperature\n","\n","            # Negative similarities from batch\n","            negative_indices = [j for j in range(batch_size) if j != i and j != i+1]\n","            if negative_indices:\n","                negatives = embeddings[negative_indices]\n","                neg_sims = torch.matmul(anchor, negatives.T) / self.temperature\n","\n","                # Add queue negatives\n","                if self.embedding_queue:\n","                    queue_negatives = torch.stack(self.embedding_queue[:64])  # Limit for memory\n","                    queue_negatives = queue_negatives.to(embeddings.device)\n","                    queue_neg_sims = torch.matmul(anchor, queue_negatives.T) / self.temperature\n","                    neg_sims = torch.cat([neg_sims, queue_neg_sims])\n","\n","                # InfoNCE loss\n","                logits = torch.cat([pos_sim.unsqueeze(0), neg_sims])\n","                loss = -pos_sim + torch.logsumexp(logits, dim=0)\n","                total_loss += loss\n","                num_pairs += 1\n","\n","        # Update queue\n","        self._update_queue(embeddings.detach())\n","\n","        return total_loss / num_pairs if num_pairs > 0 else torch.tensor(0.0, device=embeddings.device)\n","\n","    def _update_queue(self, embeddings: torch.Tensor):\n","        \"\"\"Update negative sample queue.\"\"\"\n","        for emb in embeddings:\n","            self.embedding_queue.append(emb.cpu())\n","            if len(self.embedding_queue) > self.max_queue_size:\n","                self.embedding_queue.pop(0)\n","\n","# ============================================================================\n","# TRANSFORMER COMPONENTS\n","# ============================================================================\n","\n","class MultiHeadSelfAttention(nn.Module):\n","    \"\"\"Efficient multi-head self-attention.\"\"\"\n","\n","    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.1):\n","        super().__init__()\n","        assert d_model % n_heads == 0\n","\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","        self.d_k = d_model // n_heads\n","\n","        self.w_q = nn.Linear(d_model, d_model, bias=False)\n","        self.w_k = nn.Linear(d_model, d_model, bias=False)\n","        self.w_v = nn.Linear(d_model, d_model, bias=False)\n","        self.w_o = nn.Linear(d_model, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        batch_size, seq_len, d_model = x.shape\n","\n","        # Multi-head projections\n","        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n","        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n","        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n","\n","        # Scaled dot-product attention\n","        if hasattr(F, 'scaled_dot_product_attention'):\n","            # Use PyTorch 2.0+ optimized attention\n","            attn_out = F.scaled_dot_product_attention(\n","                Q, K, V,\n","                dropout_p=self.dropout.p if self.training else 0\n","            )\n","        else:\n","            # Fallback implementation\n","            scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","            attn_weights = F.softmax(scores, dim=-1)\n","            attn_weights = self.dropout(attn_weights)\n","            attn_out = torch.matmul(attn_weights, V)\n","\n","        # Reshape and project\n","        attn_out = attn_out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n","        output = self.w_o(attn_out)\n","\n","        # Residual connection and layer norm\n","        return self.layer_norm(x + output)\n","\n","# ============================================================================\n","# ENHANCED WORLD MODEL\n","# ============================================================================\n","\n","class EnhancedEncoder(nn.Module):\n","    \"\"\"Enhanced encoder with attention and better representations.\"\"\"\n","\n","    def __init__(self, in_dim: int, emb_dim: int, use_attention: bool = True):\n","        super().__init__()\n","        self.use_attention = use_attention\n","\n","        # Base transformation\n","        self.input_proj = nn.Linear(in_dim, emb_dim)\n","\n","        if use_attention:\n","            # Transformer-style processing\n","            self.attention = MultiHeadSelfAttention(emb_dim, n_heads=8, dropout=0.1)\n","            self.feed_forward = nn.Sequential(\n","                nn.Linear(emb_dim, emb_dim * 4),\n","                nn.GELU(),\n","                nn.Dropout(0.1),\n","                nn.Linear(emb_dim * 4, emb_dim),\n","                nn.Dropout(0.1)\n","            )\n","            self.norm = nn.LayerNorm(emb_dim)\n","        else:\n","            # Simple feedforward\n","            self.net = nn.Sequential(\n","                nn.Linear(emb_dim, emb_dim),\n","                nn.GELU(),\n","                nn.Linear(emb_dim, emb_dim),\n","            )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # Initial projection\n","        x = self.input_proj(x)  # [B, N, D]\n","\n","        if self.use_attention:\n","            # Transformer processing\n","            x = self.attention(x)\n","\n","            # Feed forward with residual\n","            ff_out = self.feed_forward(x)\n","            x = self.norm(x + ff_out)\n","        else:\n","            # Simple processing\n","            x = self.net(x)\n","\n","        # Normalize final embeddings\n","        x = F.normalize(x, dim=-1)\n","        return x\n","\n","class EnhancedReadout(nn.Module):\n","    \"\"\"Enhanced readout with attention pooling.\"\"\"\n","\n","    def __init__(self, emb_dim: int):\n","        super().__init__()\n","        self.attention_pool = nn.MultiheadAttention(emb_dim, num_heads=4, batch_first=True)\n","        self.query = nn.Parameter(torch.randn(1, 1, emb_dim))\n","        self.proj = nn.Sequential(\n","            nn.Linear(emb_dim, emb_dim),\n","            nn.GELU(),\n","            nn.Linear(emb_dim, emb_dim)\n","        )\n","\n","    def forward(self, z: torch.Tensor) -> torch.Tensor:\n","        batch_size = z.shape[0]\n","\n","        # Attention pooling\n","        query = self.query.expand(batch_size, -1, -1)\n","        pooled, _ = self.attention_pool(query, z, z)\n","        pooled = pooled.squeeze(1)  # [B, D]\n","\n","        return self.proj(pooled)\n","\n","class EnhancedWorldModel(nn.Module):\n","    \"\"\"Enhanced world model with attention and better architecture.\"\"\"\n","\n","    def __init__(self, in_dim: int, emb_dim: int, use_attention: bool = True):\n","        super().__init__()\n","        self.enc = EnhancedEncoder(in_dim, emb_dim, use_attention)\n","        self.head = EnhancedReadout(emb_dim)\n","\n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        z = self.enc(x)      # [B, N, D] - object embeddings\n","        y = self.head(z)     # [B, D] - global prediction\n","        return y, z\n","\n","# ============================================================================\n","# ENHANCED AGENT AND TRAINING\n","# ============================================================================\n","\n","@dataclass\n","class EnhancedAgent:\n","    wmodel: EnhancedWorldModel\n","    mem: EnhancedGraphMemory\n","    contrastive: ContrastiveLearning\n","\n","class EnhancedTrainer:\n","    \"\"\"Enhanced trainer with all improvements.\"\"\"\n","\n","    def __init__(self, agent: EnhancedAgent, learning_rate: float = LR):\n","        self.agent = agent\n","        self.device = DEVICE\n","\n","        # Optimized optimizer\n","        if DEVICE == \"cuda\":\n","            try:\n","                self.optimizer = torch.optim.AdamW(agent.wmodel.parameters(), lr=learning_rate, fused=True)\n","            except TypeError:\n","                self.optimizer = torch.optim.AdamW(agent.wmodel.parameters(), lr=learning_rate)\n","        else:\n","            self.optimizer = torch.optim.AdamW(agent.wmodel.parameters(), lr=learning_rate)\n","\n","        # Learning rate scheduler - FIXED for PyTorch compatibility\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","            self.optimizer, mode='min', patience=3, factor=0.8\n","        )\n","\n","        # Metrics tracking\n","        self.metrics = {\n","            'world_losses': [],\n","            'contrastive_losses': [],\n","            'importance_scores': [],\n","            'learning_rates': [],\n","            'memory_stats': []\n","        }\n","\n","        # Early stopping\n","        self.best_loss = float('inf')\n","        self.patience_counter = 0\n","        self.patience = 8\n","\n","    def train_step(self, x: torch.Tensor, tgt: torch.Tensor, step: int, episode: int) -> Dict:\n","        \"\"\"Enhanced training step with all components.\"\"\"\n","\n","        step_start = time.time()\n","\n","        # Forward pass with AMP\n","        self.optimizer.zero_grad(set_to_none=True)\n","\n","        if USE_AMP:\n","            with torch.autocast(device_type='cuda', dtype=AMP_DTYPE):\n","                pred, z = self.agent.wmodel(x)\n","                world_loss = F.mse_loss(pred, tgt)\n","\n","                # Contrastive loss\n","                contrastive_loss = self.agent.contrastive.compute_contrastive_loss(z)\n","\n","                # Combined loss\n","                total_loss = world_loss + 0.1 * contrastive_loss\n","        else:\n","            pred, z = self.agent.wmodel(x)\n","            world_loss = F.mse_loss(pred, tgt)\n","            contrastive_loss = self.agent.contrastive.compute_contrastive_loss(z)\n","            total_loss = world_loss + 0.1 * contrastive_loss\n","\n","        # Backward pass\n","        total_loss.backward()\n","\n","        # Gradient clipping\n","        grad_norm = torch.nn.utils.clip_grad_norm_(self.agent.wmodel.parameters(), 1.0)\n","\n","        self.optimizer.step()\n","\n","        # Compute importance score\n","        importance = self._compute_importance_score(pred, tgt, z)\n","\n","        # Add to memory with importance\n","        with torch.no_grad():\n","            z_flat = z[0].detach().float().cpu().numpy()\n","            self.agent.mem.add_batch(z_flat, importance=importance)\n","\n","        # Collect metrics\n","        step_time = time.time() - step_start\n","        current_lr = self.optimizer.param_groups[0]['lr']\n","        memory_stats = self.agent.mem.get_memory_stats()\n","\n","        step_metrics = {\n","            'world_loss': float(world_loss.item()),\n","            'contrastive_loss': float(contrastive_loss.item()),\n","            'total_loss': float(total_loss.item()),\n","            'importance': importance,\n","            'grad_norm': float(grad_norm.item()),\n","            'learning_rate': current_lr,\n","            'step_time': step_time,\n","            'memory_stats': memory_stats\n","        }\n","\n","        # Update metrics history\n","        self.metrics['world_losses'].append(step_metrics['world_loss'])\n","        self.metrics['contrastive_losses'].append(step_metrics['contrastive_loss'])\n","        self.metrics['importance_scores'].append(step_metrics['importance'])\n","        self.metrics['learning_rates'].append(step_metrics['learning_rate'])\n","        self.metrics['memory_stats'].append(step_metrics['memory_stats'])\n","\n","        return step_metrics\n","\n","    def _compute_importance_score(self, pred: torch.Tensor, tgt: torch.Tensor, embeddings: torch.Tensor) -> float:\n","        \"\"\"Compute importance score for memory storage.\"\"\"\n","\n","        # Prediction error (lower error = higher importance)\n","        pred_error = F.mse_loss(pred, tgt).item()\n","        error_importance = 1.0 / (1.0 + pred_error)\n","\n","        # Embedding variance (novelty indicator)\n","        embedding_var = torch.var(embeddings).item()\n","        novelty_importance = min(1.0, embedding_var)\n","\n","        # Combined importance\n","        importance = 0.7 * error_importance + 0.3 * novelty_importance\n","        return float(np.clip(importance, 0.1, 1.0))\n","\n","    def update_learning_rate(self, avg_loss: float) -> bool:\n","        \"\"\"Update learning rate and check for early stopping.\"\"\"\n","        self.scheduler.step(avg_loss)\n","\n","        if avg_loss < self.best_loss:\n","            self.best_loss = avg_loss\n","            self.patience_counter = 0\n","        else:\n","            self.patience_counter += 1\n","\n","        return self.patience_counter >= self.patience\n","\n","    def get_summary_stats(self) -> Dict:\n","        \"\"\"Get training summary statistics.\"\"\"\n","        if not self.metrics['world_losses']:\n","            return {}\n","\n","        recent_window = min(100, len(self.metrics['world_losses']))\n","        recent_losses = self.metrics['world_losses'][-recent_window:]\n","\n","        return {\n","            'episodes_trained': len(self.metrics['world_losses']) // STEPS_PER_EP,\n","            'total_steps': len(self.metrics['world_losses']),\n","            'current_loss': self.metrics['world_losses'][-1],\n","            'recent_avg_loss': float(np.mean(recent_losses)),\n","            'best_loss': self.best_loss,\n","            'current_lr': self.metrics['learning_rates'][-1],\n","            'avg_importance': float(np.mean(self.metrics['importance_scores'][-recent_window:])),\n","            'final_memory_stats': self.metrics['memory_stats'][-1] if self.metrics['memory_stats'] else {}\n","        }\n","\n","# ============================================================================\n","# WORLD SIMULATION\n","# ============================================================================\n","\n","def create_world_step(batch_size: int, n_objects: int, embedding_dim: int, device: str) -> torch.Tensor:\n","    \"\"\"Create synthetic world step with rich object interactions.\"\"\"\n","\n","    # Split into features and positions\n","    feat_dim = embedding_dim // 2\n","    pos_dim = embedding_dim - feat_dim\n","\n","    # Object features (with some structure)\n","    features = torch.randn(batch_size, n_objects, feat_dim, device=device)\n","\n","    # Spatial positions (organized in loose clusters)\n","    positions = torch.randn(batch_size, n_objects, pos_dim, device=device)\n","\n","    # Add some spatial structure\n","    cluster_centers = torch.randn(batch_size, 4, pos_dim, device=device) * 2\n","    cluster_assignment = torch.randint(0, 4, (batch_size, n_objects), device=device)\n","\n","    for b in range(batch_size):\n","        for obj in range(n_objects):\n","            cluster_id = cluster_assignment[b, obj]\n","            positions[b, obj] += 0.3 * cluster_centers[b, cluster_id]\n","\n","    # Combine features and positions\n","    world_state = torch.cat([features, positions], dim=-1)\n","\n","    return world_state\n","\n","# ============================================================================\n","# MAIN TRAINING EXECUTION\n","# ============================================================================\n","\n","def run_enhanced_training():\n","    \"\"\"Run the complete enhanced GRLM training.\"\"\"\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\" ENHANCED GRLM TRAINING STARTING\")\n","    print(f\"{'='*60}\")\n","    print(f\"Episodes: {EPISODES} | Steps per episode: {STEPS_PER_EP}\")\n","    print(f\"Embedding dim: {EMB_DIM} | Objects per step: {N_OBJECTS_PER_STEP}\")\n","    print(f\"Memory capacity: {MAX_NODES:,} nodes\")\n","    print(f\"Device: {DEVICE}\")\n","\n","    # Create enhanced components\n","    enhanced_memory = EnhancedGraphMemory(dim=EMB_DIM, max_nodes=MAX_NODES)\n","    contrastive_learner = ContrastiveLearning(temperature=0.07)\n","\n","    # Create enhanced world model\n","    world_model = EnhancedWorldModel(\n","        in_dim=EMB_DIM,\n","        emb_dim=EMB_DIM,\n","        use_attention=True  # Use transformer components\n","    ).to(DEVICE)\n","\n","    # Create enhanced agent\n","    agent = EnhancedAgent(\n","        wmodel=world_model,\n","        mem=enhanced_memory,\n","        contrastive=contrastive_learner\n","    )\n","\n","    # Create enhanced trainer\n","    trainer = EnhancedTrainer(agent, learning_rate=LR)\n","\n","    print(f\" Enhanced system initialized\")\n","    print(f\"Model parameters: {sum(p.numel() for p in agent.wmodel.parameters()):,}\")\n","\n","    # Training loop\n","    total_start_time = time.time()\n","\n","    for episode in range(1, EPISODES + 1):\n","        episode_start = time.time()\n","        episode_metrics = []\n","\n","        print(f\"\\n{'='*50}\")\n","        print(f\" Episode {episode}/{EPISODES}\")\n","        print(f\"{'='*50}\")\n","\n","        for step in range(1, STEPS_PER_EP + 1):\n","            # Create world step\n","            world_state = create_world_step(\n","                batch_size=1,\n","                n_objects=N_OBJECTS_PER_STEP,\n","                embedding_dim=EMB_DIM,\n","                device=DEVICE\n","            )\n","\n","            # Create target (could be next state prediction, etc.)\n","            target = torch.zeros(1, EMB_DIM, device=DEVICE)\n","\n","            # Enhanced training step\n","            step_metrics = trainer.train_step(world_state, target, step, episode)\n","            episode_metrics.append(step_metrics)\n","\n","            # Periodic detailed logging\n","            if step % 10 == 0:\n","                mem_stats = step_metrics['memory_stats']\n","                print(f\"  Step {step:2d}/{STEPS_PER_EP} | \"\n","                      f\"World: {step_metrics['world_loss']:.6f} | \"\n","                      f\"Contrast: {step_metrics['contrastive_loss']:.6f} | \"\n","                      f\"Import: {step_metrics['importance']:.3f} | \"\n","                      f\"LR: {step_metrics['learning_rate']:.1e} | \"\n","                      f\"Mem: {mem_stats['total_nodes']}({mem_stats['working_memory']}w/\"\n","                      f\"{mem_stats['episodic_memory']}e/{mem_stats['semantic_memory']}s)\")\n","\n","        # Episode summary\n","        episode_time = time.time() - episode_start\n","        avg_world_loss = float(np.mean([m['world_loss'] for m in episode_metrics]))\n","        avg_contrastive = float(np.mean([m['contrastive_loss'] for m in episode_metrics]))\n","        avg_importance = float(np.mean([m['importance'] for m in episode_metrics]))\n","\n","        # Update learning rate\n","        should_stop = trainer.update_learning_rate(avg_world_loss)\n","\n","        print(f\"\\n Episode {episode} Summary:\")\n","        print(f\"   World Loss: {avg_world_loss:.6f} ({'' if len(trainer.metrics['world_losses']) > STEPS_PER_EP and avg_world_loss < np.mean(trainer.metrics['world_losses'][-2*STEPS_PER_EP:-STEPS_PER_EP]) else ''})\")\n","        print(f\"   Contrastive Loss: {avg_contrastive:.6f}\")\n","        print(f\"   Avg Importance: {avg_importance:.3f}\")\n","        print(f\"   Learning Rate: {trainer.optimizer.param_groups[0]['lr']:.2e}\")\n","\n","        final_mem_stats = episode_metrics[-1]['memory_stats']\n","        print(f\"   Memory: {final_mem_stats['total_nodes']} total \"\n","              f\"({final_mem_stats['working_memory']} working, \"\n","              f\"{final_mem_stats['episodic_memory']} episodic, \"\n","              f\"{final_mem_stats['semantic_memory']} semantic)\")\n","        print(f\"   Time: {episode_time:.1f}s\")\n","\n","        # Early stopping check\n","        if should_stop:\n","            print(f\"     Early stopping triggered (no improvement for {trainer.patience} episodes)\")\n","            break\n","\n","    # Final summary\n","    total_time = time.time() - total_start_time\n","    summary_stats = trainer.get_summary_stats()\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\" ENHANCED GRLM TRAINING COMPLETE!\")\n","    print(f\"{'='*60}\")\n","    print(f\" Final Statistics:\")\n","    print(f\"   Episodes completed: {summary_stats.get('episodes_trained', 0)}\")\n","    print(f\"   Total training steps: {summary_stats.get('total_steps', 0):,}\")\n","    print(f\"   Final loss: {summary_stats.get('current_loss', 0):.6f}\")\n","    print(f\"   Best loss achieved: {summary_stats.get('best_loss', 0):.6f}\")\n","    print(f\"   Final learning rate: {summary_stats.get('current_lr', 0):.2e}\")\n","    print(f\"   Average importance score: {summary_stats.get('avg_importance', 0):.3f}\")\n","\n","    final_mem = summary_stats.get('final_memory_stats', {})\n","    print(f\"   Final memory state:\")\n","    print(f\"      Total nodes: {final_mem.get('total_nodes', 0):,}\")\n","    print(f\"      Total edges: {final_mem.get('total_edges', 0):,}\")\n","    print(f\"      Working memory: {final_mem.get('working_memory', 0):,}\")\n","    print(f\"      Episodic memory: {final_mem.get('episodic_memory', 0):,}\")\n","    print(f\"      Semantic memory: {final_mem.get('semantic_memory', 0):,}\")\n","    print(f\"      Avg node importance: {final_mem.get('avg_importance', 0):.3f}\")\n","\n","    print(f\"   Total training time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n","    print(f\"   Average time per episode: {total_time/episode:.1f}s\")\n","\n","    print(f\"\\n Enhanced GRLM features successfully demonstrated:\")\n","    print(f\"    Hierarchical memory with consolidation\")\n","    print(f\"    Contrastive learning for better representations\")\n","    print(f\"    Attention-based transformer encoding\")\n","    print(f\"    Importance-based memory management\")\n","    print(f\"    Adaptive learning rate scheduling\")\n","    print(f\"    Comprehensive monitoring and early stopping\")\n","\n","    return agent, trainer\n","\n","# ============================================================================\n","# RUN THE ENHANCED SYSTEM\n","# ============================================================================\n","\n","if __name__ == \"__main__\":\n","    # Run the complete enhanced GRLM training\n","    agent, trainer = run_enhanced_training()\n","\n","    # Optional: Show some additional analysis\n","    print(f\"\\n Additional Analysis:\")\n","\n","    # Memory distribution\n","    mem_stats = trainer.agent.mem.get_memory_stats()\n","    total_nodes = mem_stats['total_nodes']\n","    if total_nodes > 0:\n","        working_pct = (mem_stats['working_memory'] / total_nodes) * 100\n","        episodic_pct = (mem_stats['episodic_memory'] / total_nodes) * 100\n","        semantic_pct = (mem_stats['semantic_memory'] / total_nodes) * 100\n","\n","        print(f\"Memory distribution: {working_pct:.1f}% working, {episodic_pct:.1f}% episodic, {semantic_pct:.1f}% semantic\")\n","\n","    # Training dynamics\n","    if len(trainer.metrics['world_losses']) > 20:\n","        initial_loss = np.mean(trainer.metrics['world_losses'][:10])\n","        final_loss = np.mean(trainer.metrics['world_losses'][-10:])\n","        improvement = ((initial_loss - final_loss) / initial_loss) * 100\n","        print(f\"Overall improvement: {improvement:.1f}% loss reduction\")\n","\n","    print(f\"\\n Enhanced GRLM is ready for further experimentation!\")\n","\n","# Run the training!\n","agent, trainer = run_enhanced_training()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":13,"status":"ok","timestamp":1758214667162,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"},"user_tz":-180},"id":"31C8uQmQwqvT","outputId":"4475a56e-b173-40eb-a359-47de8e4d5202"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Interactive 2D GRLM World Starting\n","Device: cuda\n","World size: 20x20\n"," Running Interactive GRLM Demo\n","==================================================\n"," Launching Full Interactive 2D GRLM System...\n"," System initialized!\n"," Features available:\n","    Real-time world model learning\n","    Spatial memory system\n","    Interactive 2D exploration\n","    Performance monitoring\n"]},{"data":{"text/html":["\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>Interactive 2D GRLM World</title>\n","    <style>\n","        body {\n","            font-family: Arial, sans-serif;\n","            margin: 20px;\n","            background: #1e1e1e;\n","            color: #fff;\n","        }\n","        .container {\n","            display: flex;\n","            gap: 20px;\n","        }\n","        .world-container {\n","            flex: 1;\n","        }\n","        .stats-container {\n","            width: 300px;\n","            background: #2d2d2d;\n","            padding: 15px;\n","            border-radius: 8px;\n","            height: fit-content;\n","        }\n","        canvas {\n","            border: 2px solid #555;\n","            background: #000;\n","            cursor: crosshair;\n","        }\n","        .controls {\n","            margin: 10px 0;\n","            text-align: center;\n","        }\n","        .control-btn {\n","            background: #007acc;\n","            color: white;\n","            border: none;\n","            padding: 10px 15px;\n","            margin: 2px;\n","            border-radius: 4px;\n","            cursor: pointer;\n","            font-size: 14px;\n","        }\n","        .control-btn:hover {\n","            background: #005a99;\n","        }\n","        .control-btn:active {\n","            background: #003d66;\n","        }\n","        .stats {\n","            font-size: 12px;\n","            line-height: 1.4;\n","        }\n","        .stats-header {\n","            font-size: 14px;\n","            font-weight: bold;\n","            color: #4CAF50;\n","            margin-bottom: 10px;\n","        }\n","        .stat-item {\n","            margin: 5px 0;\n","        }\n","        .help {\n","            background: #333;\n","            padding: 10px;\n","            border-radius: 4px;\n","            margin-top: 10px;\n","            font-size: 11px;\n","        }\n","    </style>\n","</head>\n","<body>\n","    <h1> Interactive 2D GRLM World</h1>\n","    <div class=\"container\">\n","        <div class=\"world-container\">\n","            <canvas id=\"worldCanvas\" width=\"600\" height=\"600\"></canvas>\n","            <div class=\"controls\">\n","                <div>\n","                    <button class=\"control-btn\" onclick=\"makeMove(0, -1)\"></button>\n","                </div>\n","                <div>\n","                    <button class=\"control-btn\" onclick=\"makeMove(-1, 0)\"></button>\n","                    <button class=\"control-btn\" onclick=\"resetWorld()\">Reset</button>\n","                    <button class=\"control-btn\" onclick=\"makeMove(1, 0)\"></button>\n","                </div>\n","                <div>\n","                    <button class=\"control-btn\" onclick=\"makeMove(0, 1)\"></button>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"stats-container\">\n","            <div class=\"stats-header\"> GRLM Stats</div>\n","            <div class=\"stats\" id=\"stats\">\n","                <div class=\"stat-item\">Initializing...</div>\n","            </div>\n","            <div class=\"help\">\n","                <strong>Controls:</strong><br>\n","                 Arrow buttons to move<br>\n","                 WASD keys also work<br>\n","                 Watch how the world model learns!<br><br>\n","                <strong>Legend:</strong><br>\n","                  Agent<br>\n","                  Wall (-1 reward)<br>\n","                  Food (+1 reward)<br>\n","                  Water (+0.3 reward)<br>\n","                  Treasure (+2 reward)<br>\n","                  Memory locations\n","            </div>\n","        </div>\n","    </div>\n","\n","    <script>\n","        const canvas = document.getElementById('worldCanvas');\n","        const ctx = canvas.getContext('2d');\n","        const gridSize = 30;\n","        \n","        let worldState = null;\n","        let grlmStats = null;\n","        \n","        // Color mappings\n","        const colors = {\n","            empty: '#111',\n","            wall: '#666',\n","            food: '#ff6b6b',\n","            water: '#4dabf7',\n","            treasure: '#ffd43b',\n","            agent: '#51cf66',\n","            memory: '#845ef7'\n","        };\n","        \n","        function drawWorld() {\n","            if (!worldState) return;\n","            \n","            ctx.clearRect(0, 0, canvas.width, canvas.height);\n","            \n","            const grid = worldState.grid;\n","            const objects = worldState.objects;\n","            const agentPos = worldState.agent_position;\n","            const size = worldState.size;\n","            \n","            // Draw grid\n","            for (let x = 0; x < size; x++) {\n","                for (let y = 0; y < size; y++) {\n","                    const pixelX = x * gridSize;\n","                    const pixelY = y * gridSize;\n","                    \n","                    // Background based on grid value\n","                    let bgColor = colors.empty;\n","                    const gridValue = grid[x][y];\n","                    if (gridValue === -1) bgColor = colors.wall;\n","                    else if (gridValue > 0) {\n","                        const intensity = Math.min(1, gridValue);\n","                        bgColor = `rgba(0, 255, 0, ${intensity * 0.3})`;\n","                    }\n","                    \n","                    ctx.fillStyle = bgColor;\n","                    ctx.fillRect(pixelX, pixelY, gridSize, gridSize);\n","                    \n","                    // Draw objects\n","                    const objKey = `${x},${y}`;\n","                    if (objects[objKey]) {\n","                        const objType = objects[objKey];\n","                        let emoji = '';\n","                        let color = colors[objType] || '#fff';\n","                        \n","                        switch (objType) {\n","                            case 'wall': emoji = ''; break;\n","                            case 'food': emoji = ''; break;\n","                            case 'water': emoji = ''; break;\n","                            case 'treasure': emoji = ''; break;\n","                        }\n","                        \n","                        if (emoji) {\n","                            ctx.font = `${gridSize * 0.7}px Arial`;\n","                            ctx.fillText(emoji, pixelX + 3, pixelY + gridSize * 0.8);\n","                        } else {\n","                            ctx.fillStyle = color;\n","                            ctx.fillRect(pixelX + 2, pixelY + 2, gridSize - 4, gridSize - 4);\n","                        }\n","                    }\n","                    \n","                    // Grid lines\n","                    ctx.strokeStyle = '#333';\n","                    ctx.lineWidth = 1;\n","                    ctx.strokeRect(pixelX, pixelY, gridSize, gridSize);\n","                }\n","            }\n","            \n","            // Draw memory locations\n","            if (grlmStats && grlmStats.memory_positions) {\n","                ctx.fillStyle = colors.memory;\n","                for (const pos of grlmStats.memory_positions) {\n","                    const x = pos[0] * gridSize + gridSize/2;\n","                    const y = pos[1] * gridSize + gridSize/2;\n","                    ctx.beginPath();\n","                    ctx.arc(x, y, 3, 0, 2 * Math.PI);\n","                    ctx.fill();\n","                }\n","            }\n","            \n","            // Draw agent\n","            const agentX = agentPos[0] * gridSize;\n","            const agentY = agentPos[1] * gridSize;\n","            \n","            ctx.font = `${gridSize * 0.8}px Arial`;\n","            ctx.fillText('', agentX + 2, agentY + gridSize * 0.8);\n","            \n","            // Agent highlight\n","            ctx.strokeStyle = colors.agent;\n","            ctx.lineWidth = 3;\n","            ctx.strokeRect(agentX + 1, agentY + 1, gridSize - 2, gridSize - 2);\n","        }\n","        \n","        function updateStats() {\n","            if (!grlmStats) return;\n","            \n","            const statsDiv = document.getElementById('stats');\n","            statsDiv.innerHTML = `\n","                <div class=\"stat-item\"><strong>Steps:</strong> ${grlmStats.total_steps}</div>\n","                <div class=\"stat-item\"><strong>Total Reward:</strong> ${grlmStats.total_reward.toFixed(2)}</div>\n","                <div class=\"stat-item\"><strong>Avg Reward:</strong> ${grlmStats.avg_reward_per_step.toFixed(3)}</div>\n","                <div class=\"stat-item\"><strong>Prediction Error:</strong> ${grlmStats.recent_prediction_error.toFixed(4)}</div>\n","                <div class=\"stat-item\"><strong>Memories:</strong> ${grlmStats.memory_stats.total_memories}</div>\n","                <div class=\"stat-item\"><strong>Memory Coverage:</strong> ${(grlmStats.world_coverage * 100).toFixed(1)}%</div>\n","                <div class=\"stat-item\"><strong>Avg Importance:</strong> ${grlmStats.memory_stats.avg_importance.toFixed(2)}</div>\n","            `;\n","        }\n","        \n","        function makeMove(dx, dy) {\n","            // This would normally call the Python backend\n","            console.log(`Moving: dx=${dx}, dy=${dy}`);\n","            \n","            // For demo, simulate movement with proper collision detection\n","            if (worldState) {\n","                const oldX = worldState.agent_position[0];\n","                const oldY = worldState.agent_position[1];\n","                const newX = Math.max(0, Math.min(worldState.size - 1, oldX + dx));\n","                const newY = Math.max(0, Math.min(worldState.size - 1, oldY + dy));\n","                \n","                // Check if the new position is valid (not a wall)\n","                let canMove = true;\n","                const gridValue = worldState.grid[newX][newY];\n","                const objKey = `${newX},${newY}`;\n","                \n","                // Check for walls\n","                if (gridValue === -1 || (worldState.objects[objKey] === 'wall')) {\n","                    canMove = false;\n","                }\n","                \n","                let reward = 0;\n","                if (canMove && (newX !== oldX || newY !== oldY)) {\n","                    // Valid move - update position\n","                    worldState.agent_position = [newX, newY];\n","                    reward = gridValue;\n","                    \n","                    // Special interaction effects\n","                    if (worldState.objects[objKey]) {\n","                        const objType = worldState.objects[objKey];\n","                        switch (objType) {\n","                            case 'treasure': reward += 2.0; break;\n","                            case 'food': reward += 1.0; break;\n","                            case 'water': reward += 0.3; break;\n","                        }\n","                    }\n","                } else if (!canMove) {\n","                    // Hit a wall - penalty but no movement\n","                    reward = -1.0;\n","                    console.log(\"Hit a wall! Cannot move there.\");\n","                }\n","                \n","                // Simulate stats update\n","                if (grlmStats) {\n","                    grlmStats.total_steps += 1;\n","                    grlmStats.total_reward += reward;\n","                    grlmStats.avg_reward_per_step = grlmStats.total_reward / grlmStats.total_steps;\n","                    grlmStats.recent_prediction_error = Math.random() * 0.01;\n","                    \n","                    // Add memory location if moved to new position\n","                    if (canMove && (newX !== oldX || newY !== oldY)) {\n","                        if (!grlmStats.memory_positions) grlmStats.memory_positions = [];\n","                        // Only add if not already in memory (within 1 cell)\n","                        const alreadyHasNearbyMemory = grlmStats.memory_positions.some(pos => \n","                            Math.abs(pos[0] - newX) <= 1 && Math.abs(pos[1] - newY) <= 1\n","                        );\n","                        if (!alreadyHasNearbyMemory) {\n","                            grlmStats.memory_positions.push([newX, newY]);\n","                        }\n","                        grlmStats.world_coverage = grlmStats.memory_positions.length / (worldState.size * worldState.size);\n","                    }\n","                }\n","                \n","                drawWorld();\n","                updateStats();\n","            }\n","        }\n","        \n","        function resetWorld() {\n","            // Reset to center\n","            if (worldState) {\n","                worldState.agent_position = [Math.floor(worldState.size / 2), Math.floor(worldState.size / 2)];\n","                grlmStats = {\n","                    total_steps: 0,\n","                    total_reward: 0,\n","                    avg_reward_per_step: 0,\n","                    recent_prediction_error: 0,\n","                    memory_stats: { total_memories: 0, avg_importance: 1.0 },\n","                    world_coverage: 0\n","                };\n","                drawWorld();\n","                updateStats();\n","            }\n","        }\n","        \n","        // Keyboard controls\n","        document.addEventListener('keydown', (e) => {\n","            switch(e.key.toLowerCase()) {\n","                case 'w': case 'arrowup': makeMove(0, -1); break;\n","                case 's': case 'arrowdown': makeMove(0, 1); break;\n","                case 'a': case 'arrowleft': makeMove(-1, 0); break;\n","                case 'd': case 'arrowright': makeMove(1, 0); break;\n","                case 'r': resetWorld(); break;\n","            }\n","        });\n","        \n","        // Initialize demo world\n","        function initDemoWorld() {\n","            worldState = {\n","                grid: Array(20).fill().map(() => Array(20).fill(0)),\n","                objects: {\n","                    '3,3': 'wall', '4,3': 'wall', '5,3': 'wall',\n","                    '7,7': 'food', '12,8': 'treasure', '15,15': 'water',\n","                    '2,18': 'wall', '18,2': 'food'\n","                },\n","                agent_position: [10, 10],\n","                size: 20\n","            };\n","            \n","            // Set grid values\n","            for (let x = 0; x < 20; x++) {\n","                for (let y = 0; y < 20; y++) {\n","                    const key = `${x},${y}`;\n","                    if (worldState.objects[key] === 'wall') worldState.grid[x][y] = -1;\n","                    else if (worldState.objects[key] === 'food') worldState.grid[x][y] = 0.5;\n","                    else if (worldState.objects[key] === 'water') worldState.grid[x][y] = 0.3;\n","                    else if (worldState.objects[key] === 'treasure') worldState.grid[x][y] = 1.0;\n","                }\n","            }\n","            \n","            grlmStats = {\n","                total_steps: 0,\n","                total_reward: 0,\n","                avg_reward_per_step: 0,\n","                recent_prediction_error: 0,\n","                memory_stats: { total_memories: 0, avg_importance: 1.0 },\n","                world_coverage: 0,\n","                memory_positions: []\n","            };\n","            \n","            drawWorld();\n","            updateStats();\n","        }\n","        \n","        // Start the demo\n","        initDemoWorld();\n","        \n","        // Focus for keyboard input\n","        canvas.focus();\n","        canvas.setAttribute('tabindex', '0');\n","    </script>\n","</body>\n","</html>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"," Initial System Stats:\n","   total_steps: 0\n","   total_reward: 0.0\n","   avg_reward_per_step: 0.0\n","   recent_prediction_error: 0.0\n","   memory_stats: {'total_memories': 1, 'avg_importance': 1.0, 'memory_coverage': 0.0025}\n","   world_coverage: 0.0025\n","\n"," Interactive world is now ready!\n"," The system will learn and adapt as you explore!\n","\n"," Interactive 2D GRLM World is now running!\n","Move around and watch the AI learn from your exploration!\n"," Running Interactive GRLM Demo\n","==================================================\n"," Launching Full Interactive 2D GRLM System...\n"," System initialized!\n"," Features available:\n","    Real-time world model learning\n","    Spatial memory system\n","    Interactive 2D exploration\n","    Performance monitoring\n"]},{"data":{"text/html":["\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>Interactive 2D GRLM World</title>\n","    <style>\n","        body {\n","            font-family: Arial, sans-serif;\n","            margin: 20px;\n","            background: #1e1e1e;\n","            color: #fff;\n","        }\n","        .container {\n","            display: flex;\n","            gap: 20px;\n","        }\n","        .world-container {\n","            flex: 1;\n","        }\n","        .stats-container {\n","            width: 300px;\n","            background: #2d2d2d;\n","            padding: 15px;\n","            border-radius: 8px;\n","            height: fit-content;\n","        }\n","        canvas {\n","            border: 2px solid #555;\n","            background: #000;\n","            cursor: crosshair;\n","        }\n","        .controls {\n","            margin: 10px 0;\n","            text-align: center;\n","        }\n","        .control-btn {\n","            background: #007acc;\n","            color: white;\n","            border: none;\n","            padding: 10px 15px;\n","            margin: 2px;\n","            border-radius: 4px;\n","            cursor: pointer;\n","            font-size: 14px;\n","        }\n","        .control-btn:hover {\n","            background: #005a99;\n","        }\n","        .control-btn:active {\n","            background: #003d66;\n","        }\n","        .stats {\n","            font-size: 12px;\n","            line-height: 1.4;\n","        }\n","        .stats-header {\n","            font-size: 14px;\n","            font-weight: bold;\n","            color: #4CAF50;\n","            margin-bottom: 10px;\n","        }\n","        .stat-item {\n","            margin: 5px 0;\n","        }\n","        .help {\n","            background: #333;\n","            padding: 10px;\n","            border-radius: 4px;\n","            margin-top: 10px;\n","            font-size: 11px;\n","        }\n","    </style>\n","</head>\n","<body>\n","    <h1> Interactive 2D GRLM World</h1>\n","    <div class=\"container\">\n","        <div class=\"world-container\">\n","            <canvas id=\"worldCanvas\" width=\"600\" height=\"600\"></canvas>\n","            <div class=\"controls\">\n","                <div>\n","                    <button class=\"control-btn\" onclick=\"makeMove(0, -1)\"></button>\n","                </div>\n","                <div>\n","                    <button class=\"control-btn\" onclick=\"makeMove(-1, 0)\"></button>\n","                    <button class=\"control-btn\" onclick=\"resetWorld()\">Reset</button>\n","                    <button class=\"control-btn\" onclick=\"makeMove(1, 0)\"></button>\n","                </div>\n","                <div>\n","                    <button class=\"control-btn\" onclick=\"makeMove(0, 1)\"></button>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"stats-container\">\n","            <div class=\"stats-header\"> GRLM Stats</div>\n","            <div class=\"stats\" id=\"stats\">\n","                <div class=\"stat-item\">Initializing...</div>\n","            </div>\n","            <div class=\"help\">\n","                <strong>Controls:</strong><br>\n","                 Arrow buttons to move<br>\n","                 WASD keys also work<br>\n","                 Watch how the world model learns!<br><br>\n","                <strong>Legend:</strong><br>\n","                  Agent<br>\n","                  Wall (-1 reward)<br>\n","                  Food (+1 reward)<br>\n","                  Water (+0.3 reward)<br>\n","                  Treasure (+2 reward)<br>\n","                  Memory locations\n","            </div>\n","        </div>\n","    </div>\n","\n","    <script>\n","        const canvas = document.getElementById('worldCanvas');\n","        const ctx = canvas.getContext('2d');\n","        const gridSize = 30;\n","        \n","        let worldState = null;\n","        let grlmStats = null;\n","        \n","        // Color mappings\n","        const colors = {\n","            empty: '#111',\n","            wall: '#666',\n","            food: '#ff6b6b',\n","            water: '#4dabf7',\n","            treasure: '#ffd43b',\n","            agent: '#51cf66',\n","            memory: '#845ef7'\n","        };\n","        \n","        function drawWorld() {\n","            if (!worldState) return;\n","            \n","            ctx.clearRect(0, 0, canvas.width, canvas.height);\n","            \n","            const grid = worldState.grid;\n","            const objects = worldState.objects;\n","            const agentPos = worldState.agent_position;\n","            const size = worldState.size;\n","            \n","            // Draw grid\n","            for (let x = 0; x < size; x++) {\n","                for (let y = 0; y < size; y++) {\n","                    const pixelX = x * gridSize;\n","                    const pixelY = y * gridSize;\n","                    \n","                    // Background based on grid value\n","                    let bgColor = colors.empty;\n","                    const gridValue = grid[x][y];\n","                    if (gridValue === -1) bgColor = colors.wall;\n","                    else if (gridValue > 0) {\n","                        const intensity = Math.min(1, gridValue);\n","                        bgColor = `rgba(0, 255, 0, ${intensity * 0.3})`;\n","                    }\n","                    \n","                    ctx.fillStyle = bgColor;\n","                    ctx.fillRect(pixelX, pixelY, gridSize, gridSize);\n","                    \n","                    // Draw objects\n","                    const objKey = `${x},${y}`;\n","                    if (objects[objKey]) {\n","                        const objType = objects[objKey];\n","                        let emoji = '';\n","                        let color = colors[objType] || '#fff';\n","                        \n","                        switch (objType) {\n","                            case 'wall': emoji = ''; break;\n","                            case 'food': emoji = ''; break;\n","                            case 'water': emoji = ''; break;\n","                            case 'treasure': emoji = ''; break;\n","                        }\n","                        \n","                        if (emoji) {\n","                            ctx.font = `${gridSize * 0.7}px Arial`;\n","                            ctx.fillText(emoji, pixelX + 3, pixelY + gridSize * 0.8);\n","                        } else {\n","                            ctx.fillStyle = color;\n","                            ctx.fillRect(pixelX + 2, pixelY + 2, gridSize - 4, gridSize - 4);\n","                        }\n","                    }\n","                    \n","                    // Grid lines\n","                    ctx.strokeStyle = '#333';\n","                    ctx.lineWidth = 1;\n","                    ctx.strokeRect(pixelX, pixelY, gridSize, gridSize);\n","                }\n","            }\n","            \n","            // Draw memory locations\n","            if (grlmStats && grlmStats.memory_positions) {\n","                ctx.fillStyle = colors.memory;\n","                for (const pos of grlmStats.memory_positions) {\n","                    const x = pos[0] * gridSize + gridSize/2;\n","                    const y = pos[1] * gridSize + gridSize/2;\n","                    ctx.beginPath();\n","                    ctx.arc(x, y, 3, 0, 2 * Math.PI);\n","                    ctx.fill();\n","                }\n","            }\n","            \n","            // Draw agent\n","            const agentX = agentPos[0] * gridSize;\n","            const agentY = agentPos[1] * gridSize;\n","            \n","            ctx.font = `${gridSize * 0.8}px Arial`;\n","            ctx.fillText('', agentX + 2, agentY + gridSize * 0.8);\n","            \n","            // Agent highlight\n","            ctx.strokeStyle = colors.agent;\n","            ctx.lineWidth = 3;\n","            ctx.strokeRect(agentX + 1, agentY + 1, gridSize - 2, gridSize - 2);\n","        }\n","        \n","        function updateStats() {\n","            if (!grlmStats) return;\n","            \n","            const statsDiv = document.getElementById('stats');\n","            statsDiv.innerHTML = `\n","                <div class=\"stat-item\"><strong>Steps:</strong> ${grlmStats.total_steps}</div>\n","                <div class=\"stat-item\"><strong>Total Reward:</strong> ${grlmStats.total_reward.toFixed(2)}</div>\n","                <div class=\"stat-item\"><strong>Avg Reward:</strong> ${grlmStats.avg_reward_per_step.toFixed(3)}</div>\n","                <div class=\"stat-item\"><strong>Prediction Error:</strong> ${grlmStats.recent_prediction_error.toFixed(4)}</div>\n","                <div class=\"stat-item\"><strong>Memories:</strong> ${grlmStats.memory_stats.total_memories}</div>\n","                <div class=\"stat-item\"><strong>Memory Coverage:</strong> ${(grlmStats.world_coverage * 100).toFixed(1)}%</div>\n","                <div class=\"stat-item\"><strong>Avg Importance:</strong> ${grlmStats.memory_stats.avg_importance.toFixed(2)}</div>\n","            `;\n","        }\n","        \n","        function makeMove(dx, dy) {\n","            // This would normally call the Python backend\n","            console.log(`Moving: dx=${dx}, dy=${dy}`);\n","            \n","            // For demo, simulate movement with proper collision detection\n","            if (worldState) {\n","                const oldX = worldState.agent_position[0];\n","                const oldY = worldState.agent_position[1];\n","                const newX = Math.max(0, Math.min(worldState.size - 1, oldX + dx));\n","                const newY = Math.max(0, Math.min(worldState.size - 1, oldY + dy));\n","                \n","                // Check if the new position is valid (not a wall)\n","                let canMove = true;\n","                const gridValue = worldState.grid[newX][newY];\n","                const objKey = `${newX},${newY}`;\n","                \n","                // Check for walls\n","                if (gridValue === -1 || (worldState.objects[objKey] === 'wall')) {\n","                    canMove = false;\n","                }\n","                \n","                let reward = 0;\n","                if (canMove && (newX !== oldX || newY !== oldY)) {\n","                    // Valid move - update position\n","                    worldState.agent_position = [newX, newY];\n","                    reward = gridValue;\n","                    \n","                    // Special interaction effects\n","                    if (worldState.objects[objKey]) {\n","                        const objType = worldState.objects[objKey];\n","                        switch (objType) {\n","                            case 'treasure': reward += 2.0; break;\n","                            case 'food': reward += 1.0; break;\n","                            case 'water': reward += 0.3; break;\n","                        }\n","                    }\n","                } else if (!canMove) {\n","                    // Hit a wall - penalty but no movement\n","                    reward = -1.0;\n","                    console.log(\"Hit a wall! Cannot move there.\");\n","                }\n","                \n","                // Simulate stats update\n","                if (grlmStats) {\n","                    grlmStats.total_steps += 1;\n","                    grlmStats.total_reward += reward;\n","                    grlmStats.avg_reward_per_step = grlmStats.total_reward / grlmStats.total_steps;\n","                    grlmStats.recent_prediction_error = Math.random() * 0.01;\n","                    \n","                    // Add memory location if moved to new position\n","                    if (canMove && (newX !== oldX || newY !== oldY)) {\n","                        if (!grlmStats.memory_positions) grlmStats.memory_positions = [];\n","                        // Only add if not already in memory (within 1 cell)\n","                        const alreadyHasNearbyMemory = grlmStats.memory_positions.some(pos => \n","                            Math.abs(pos[0] - newX) <= 1 && Math.abs(pos[1] - newY) <= 1\n","                        );\n","                        if (!alreadyHasNearbyMemory) {\n","                            grlmStats.memory_positions.push([newX, newY]);\n","                        }\n","                        grlmStats.world_coverage = grlmStats.memory_positions.length / (worldState.size * worldState.size);\n","                    }\n","                }\n","                \n","                drawWorld();\n","                updateStats();\n","            }\n","        }\n","        \n","        function resetWorld() {\n","            // Reset to center\n","            if (worldState) {\n","                worldState.agent_position = [Math.floor(worldState.size / 2), Math.floor(worldState.size / 2)];\n","                grlmStats = {\n","                    total_steps: 0,\n","                    total_reward: 0,\n","                    avg_reward_per_step: 0,\n","                    recent_prediction_error: 0,\n","                    memory_stats: { total_memories: 0, avg_importance: 1.0 },\n","                    world_coverage: 0\n","                };\n","                drawWorld();\n","                updateStats();\n","            }\n","        }\n","        \n","        // Keyboard controls\n","        document.addEventListener('keydown', (e) => {\n","            switch(e.key.toLowerCase()) {\n","                case 'w': case 'arrowup': makeMove(0, -1); break;\n","                case 's': case 'arrowdown': makeMove(0, 1); break;\n","                case 'a': case 'arrowleft': makeMove(-1, 0); break;\n","                case 'd': case 'arrowright': makeMove(1, 0); break;\n","                case 'r': resetWorld(); break;\n","            }\n","        });\n","        \n","        // Initialize demo world\n","        function initDemoWorld() {\n","            worldState = {\n","                grid: Array(20).fill().map(() => Array(20).fill(0)),\n","                objects: {\n","                    '3,3': 'wall', '4,3': 'wall', '5,3': 'wall',\n","                    '7,7': 'food', '12,8': 'treasure', '15,15': 'water',\n","                    '2,18': 'wall', '18,2': 'food'\n","                },\n","                agent_position: [10, 10],\n","                size: 20\n","            };\n","            \n","            // Set grid values\n","            for (let x = 0; x < 20; x++) {\n","                for (let y = 0; y < 20; y++) {\n","                    const key = `${x},${y}`;\n","                    if (worldState.objects[key] === 'wall') worldState.grid[x][y] = -1;\n","                    else if (worldState.objects[key] === 'food') worldState.grid[x][y] = 0.5;\n","                    else if (worldState.objects[key] === 'water') worldState.grid[x][y] = 0.3;\n","                    else if (worldState.objects[key] === 'treasure') worldState.grid[x][y] = 1.0;\n","                }\n","            }\n","            \n","            grlmStats = {\n","                total_steps: 0,\n","                total_reward: 0,\n","                avg_reward_per_step: 0,\n","                recent_prediction_error: 0,\n","                memory_stats: { total_memories: 0, avg_importance: 1.0 },\n","                world_coverage: 0,\n","                memory_positions: []\n","            };\n","            \n","            drawWorld();\n","            updateStats();\n","        }\n","        \n","        // Start the demo\n","        initDemoWorld();\n","        \n","        // Focus for keyboard input\n","        canvas.focus();\n","        canvas.setAttribute('tabindex', '0');\n","    </script>\n","</body>\n","</html>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"," Initial System Stats:\n","   total_steps: 0\n","   total_reward: 0.0\n","   avg_reward_per_step: 0.0\n","   recent_prediction_error: 0.0\n","   memory_stats: {'total_memories': 1, 'avg_importance': 1.0, 'memory_coverage': 0.0025}\n","   world_coverage: 0.0025\n","\n"," Interactive world is now ready!\n"," The system will learn and adapt as you explore!\n"]}],"source":["# @title\n","# Interactive 2D GRLM World - Complete System for Google Colab\n","# Run this in Google Colab to launch an interactive 2D world visualization\n","\n","import os\n","import math\n","import time\n","import json\n","import random\n","import tempfile\n","import warnings\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Optional, Tuple, Dict, List\n","import base64\n","from IPython.display import HTML, display\n","import threading\n","import queue\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# ============================================================================\n","# PERFORMANCE OPTIMIZATIONS & SETUP\n","# ============================================================================\n","\n","# Enable optimizations\n","if torch.cuda.is_available():\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    try:\n","        torch.set_float32_matmul_precision(\"high\")\n","    except AttributeError:\n","        pass\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","USE_AMP = torch.cuda.is_available()\n","AMP_DTYPE = torch.bfloat16\n","\n","# Configuration for interactive world\n","SEED = 1337\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","# World parameters\n","WORLD_SIZE = 20  # 20x20 grid world\n","GRID_SIZE = 30   # pixels per grid cell\n","EMB_DIM = 128    # Reduced for faster processing\n","MAX_NODES = 10000  # Smaller memory for real-time performance\n","\n","print(f\" Interactive 2D GRLM World Starting\")\n","print(f\"Device: {DEVICE}\")\n","print(f\"World size: {WORLD_SIZE}x{WORLD_SIZE}\")\n","\n","# ============================================================================\n","# SIMPLIFIED MEMORY SYSTEM FOR REAL-TIME\n","# ============================================================================\n","\n","class InteractiveGraphMemory:\n","    \"\"\"Lightweight memory system optimized for real-time interaction.\"\"\"\n","\n","    def __init__(self, dim: int, max_nodes: int = MAX_NODES):\n","        self.dim = dim\n","        self.max_nodes = max_nodes\n","        self.nodes = []\n","        self.positions = []  # Store 2D positions for each memory\n","        self.timestamps = []\n","        self.importance = []\n","\n","    def add_experience(self, embedding: np.ndarray, position: Tuple[float, float], importance: float = 1.0):\n","        \"\"\"Add experience with spatial position.\"\"\"\n","        self.nodes.append(embedding.astype(np.float32))\n","        self.positions.append(position)\n","        self.timestamps.append(time.time())\n","        self.importance.append(importance)\n","\n","        # Simple trimming\n","        if len(self.nodes) > self.max_nodes:\n","            # Keep most recent half\n","            keep = self.max_nodes // 2\n","            self.nodes = self.nodes[-keep:]\n","            self.positions = self.positions[-keep:]\n","            self.timestamps = self.timestamps[-keep:]\n","            self.importance = self.importance[-keep:]\n","\n","    def get_nearby_memories(self, position: Tuple[float, float], radius: float = 3.0) -> List[Dict]:\n","        \"\"\"Get memories near a position.\"\"\"\n","        nearby = []\n","        for i, (mem_pos, emb, importance) in enumerate(zip(self.positions, self.nodes, self.importance)):\n","            dist = math.sqrt((position[0] - mem_pos[0])**2 + (position[1] - mem_pos[1])**2)\n","            if dist <= radius:\n","                nearby.append({\n","                    'position': mem_pos,\n","                    'embedding': emb,\n","                    'importance': importance,\n","                    'distance': dist,\n","                    'index': i\n","                })\n","        return sorted(nearby, key=lambda x: x['distance'])\n","\n","    def get_stats(self) -> Dict:\n","        \"\"\"Get memory statistics.\"\"\"\n","        return {\n","            'total_memories': len(self.nodes),\n","            'avg_importance': float(np.mean(self.importance)) if self.importance else 0.0,\n","            'memory_coverage': len(set(self.positions)) / (WORLD_SIZE * WORLD_SIZE) if self.positions else 0.0\n","        }\n","\n","# ============================================================================\n","# SIMPLIFIED WORLD MODEL FOR REAL-TIME\n","# ============================================================================\n","\n","class SimpleEncoder(nn.Module):\n","    \"\"\"Lightweight encoder for real-time processing.\"\"\"\n","\n","    def __init__(self, input_dim: int, emb_dim: int):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, emb_dim),\n","            nn.ReLU(),\n","            nn.Linear(emb_dim, emb_dim),\n","            nn.LayerNorm(emb_dim)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return F.normalize(self.net(x), dim=-1)\n","\n","class MovementPredictor(nn.Module):\n","    \"\"\"Predicts movement outcomes and world state changes.\"\"\"\n","\n","    def __init__(self, emb_dim: int):\n","        super().__init__()\n","        # Input: current state + action (movement direction)\n","        self.predictor = nn.Sequential(\n","            nn.Linear(emb_dim + 2, emb_dim),  # +2 for dx, dy movement\n","            nn.ReLU(),\n","            nn.Linear(emb_dim, emb_dim),\n","            nn.ReLU(),\n","            nn.Linear(emb_dim, emb_dim + 2)  # predict next state + position change\n","        )\n","\n","    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        combined = torch.cat([state, action], dim=-1)\n","        output = self.predictor(combined)\n","        next_state = output[..., :-2]\n","        position_change = output[..., -2:]\n","        return next_state, position_change\n","\n","class InteractiveWorldModel(nn.Module):\n","    \"\"\"Complete world model for interactive 2D world.\"\"\"\n","\n","    def __init__(self, emb_dim: int = EMB_DIM):\n","        super().__init__()\n","        # State encoder: position + local environment\n","        self.encoder = SimpleEncoder(input_dim=6, emb_dim=emb_dim)  # x, y, local_features[4]\n","        self.movement_predictor = MovementPredictor(emb_dim)\n","\n","    def encode_state(self, position: Tuple[float, float], local_env: np.ndarray) -> torch.Tensor:\n","        \"\"\"Encode current position and local environment.\"\"\"\n","        state_input = torch.tensor([\n","            position[0] / WORLD_SIZE,  # normalized x\n","            position[1] / WORLD_SIZE,  # normalized y\n","            *local_env  # local environment features\n","        ], dtype=torch.float32, device=DEVICE)\n","\n","        return self.encoder(state_input.unsqueeze(0))\n","\n","    def predict_movement(self, current_state: torch.Tensor, movement: Tuple[float, float]) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"Predict outcome of a movement action.\"\"\"\n","        action = torch.tensor(movement, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n","        return self.movement_predictor(current_state, action)\n","\n","# ============================================================================\n","# 2D WORLD ENVIRONMENT\n","# ============================================================================\n","\n","class World2D:\n","    \"\"\"2D grid world with objects and spatial relationships.\"\"\"\n","\n","    def __init__(self, size: int = WORLD_SIZE):\n","        self.size = size\n","        self.grid = np.zeros((size, size), dtype=np.float32)\n","        self.objects = {}  # position -> object_type\n","        self.agent_position = (size // 2, size // 2)  # Start in center\n","\n","        # Generate world content\n","        self._generate_world()\n","\n","    def _generate_world(self):\n","        \"\"\"Generate a simple world with objects.\"\"\"\n","        np.random.seed(SEED)\n","\n","        # Add some random objects\n","        for _ in range(self.size * 2):\n","            x, y = np.random.randint(0, self.size, 2)\n","            obj_type = np.random.choice(['wall', 'food', 'water', 'treasure'])\n","            self.objects[(x, y)] = obj_type\n","\n","            # Set grid values based on object type\n","            if obj_type == 'wall':\n","                self.grid[x, y] = -1.0\n","            elif obj_type == 'food':\n","                self.grid[x, y] = 0.5\n","            elif obj_type == 'water':\n","                self.grid[x, y] = 0.3\n","            elif obj_type == 'treasure':\n","                self.grid[x, y] = 1.0\n","\n","    def get_local_environment(self, position: Tuple[int, int], radius: int = 1) -> np.ndarray:\n","        \"\"\"Get local environment features around position.\"\"\"\n","        x, y = position\n","        features = []\n","\n","        # Sample points around the position\n","        for dx in [-radius, 0, radius]:\n","            for dy in [-radius, 0, radius]:\n","                nx, ny = x + dx, y + dy\n","                if 0 <= nx < self.size and 0 <= ny < self.size:\n","                    features.append(self.grid[nx, ny])\n","                else:\n","                    features.append(-2.0)  # Out of bounds\n","\n","        # Return 4 key features: up, down, left, right\n","        if len(features) >= 9:  # 3x3 grid\n","            return np.array([features[1], features[7], features[3], features[5]])\n","        else:\n","            return np.array([0.0, 0.0, 0.0, 0.0])\n","\n","    def is_valid_position(self, position: Tuple[int, int]) -> bool:\n","        \"\"\"Check if position is valid (not wall, within bounds).\"\"\"\n","        x, y = position\n","        if not (0 <= x < self.size and 0 <= y < self.size):\n","            return False\n","        return self.grid[x, y] != -1.0  # Not a wall\n","\n","    def move_agent(self, dx: int, dy: int) -> Tuple[Tuple[int, int], float]:\n","        \"\"\"Move agent and return new position and reward.\"\"\"\n","        old_x, old_y = self.agent_position\n","        new_x = max(0, min(self.size - 1, old_x + dx))\n","        new_y = max(0, min(self.size - 1, old_y + dy))\n","        new_pos = (new_x, new_y)\n","\n","        if self.is_valid_position(new_pos):\n","            # Valid move - update position\n","            self.agent_position = new_pos\n","            # Calculate reward based on objects\n","            reward = self.grid[new_x, new_y]\n","\n","            # Special interaction effects\n","            if (new_x, new_y) in self.objects:\n","                obj_type = self.objects[(new_x, new_y)]\n","                if obj_type == 'treasure':\n","                    reward += 2.0  # Bonus for treasure\n","                elif obj_type == 'food':\n","                    reward += 1.0  # Bonus for food\n","        else:\n","            # Invalid move - stay in place, penalty\n","            reward = -1.0  # Penalty for hitting walls\n","            # self.agent_position stays the same (no movement)\n","\n","        return self.agent_position, reward\n","\n","    def get_world_state(self) -> Dict:\n","        \"\"\"Get complete world state for visualization.\"\"\"\n","        return {\n","            'grid': self.grid.tolist(),\n","            'objects': {f\"{x},{y}\": obj_type for (x, y), obj_type in self.objects.items()},\n","            'agent_position': self.agent_position,\n","            'size': self.size\n","        }\n","\n","# ============================================================================\n","# INTERACTIVE SYSTEM\n","# ============================================================================\n","\n","class InteractiveGRLM:\n","    \"\"\"Main interactive system combining world model, memory, and environment.\"\"\"\n","\n","    def __init__(self):\n","        self.world = World2D()\n","        self.memory = InteractiveGraphMemory(EMB_DIM)\n","        self.world_model = InteractiveWorldModel().to(DEVICE)\n","\n","        # Training components\n","        self.optimizer = torch.optim.AdamW(self.world_model.parameters(), lr=1e-3)\n","\n","        # Stats\n","        self.total_steps = 0\n","        self.total_reward = 0.0\n","        self.prediction_errors = []\n","\n","        # Initialize first memory\n","        self._update_memory()\n","\n","    def _update_memory(self):\n","        \"\"\"Update memory with current state.\"\"\"\n","        pos = self.world.agent_position\n","        local_env = self.world.get_local_environment(pos)\n","\n","        # Encode current state\n","        state_embedding = self.world_model.encode_state(pos, local_env)\n","        embedding_np = state_embedding.detach().cpu().numpy().flatten()\n","\n","        # Add to memory\n","        importance = 1.0 + abs(self.world.grid[pos[0], pos[1]])  # Higher importance for interesting spots\n","        self.memory.add_experience(embedding_np, pos, importance)\n","\n","    def make_move(self, dx: int, dy: int) -> Dict:\n","        \"\"\"Make a move and update the world model.\"\"\"\n","        # Get current state\n","        old_pos = self.world.agent_position\n","        old_local_env = self.world.get_local_environment(old_pos)\n","        current_state = self.world_model.encode_state(old_pos, old_local_env)\n","\n","        # Predict movement outcome\n","        predicted_state, predicted_pos_change = self.world_model.predict_movement(current_state, (dx, dy))\n","\n","        # Execute actual movement\n","        new_pos, reward = self.world.move_agent(dx, dy)\n","        new_local_env = self.world.get_local_environment(new_pos)\n","        actual_state = self.world_model.encode_state(new_pos, new_local_env)\n","\n","        # Train world model\n","        self._train_step(current_state, (dx, dy), actual_state, new_pos, old_pos)\n","\n","        # Update memory\n","        self._update_memory()\n","\n","        # Update stats\n","        self.total_steps += 1\n","        self.total_reward += reward\n","\n","        # Get nearby memories for context\n","        nearby_memories = self.memory.get_nearby_memories(new_pos, radius=3.0)\n","\n","        return {\n","            'old_position': old_pos,\n","            'new_position': new_pos,\n","            'reward': reward,\n","            'total_reward': self.total_reward,\n","            'total_steps': self.total_steps,\n","            'prediction_error': self.prediction_errors[-1] if self.prediction_errors else 0.0,\n","            'nearby_memories': len(nearby_memories),\n","            'memory_stats': self.memory.get_stats(),\n","            'world_state': self.world.get_world_state()\n","        }\n","\n","    def _train_step(self, current_state: torch.Tensor, action: Tuple[float, float],\n","                   target_state: torch.Tensor, new_pos: Tuple[int, int], old_pos: Tuple[int, int]):\n","        \"\"\"Train the world model on the movement.\"\"\"\n","        self.optimizer.zero_grad()\n","\n","        # Predict next state and position change\n","        pred_state, pred_pos_change = self.world_model.predict_movement(current_state, action)\n","\n","        # Calculate losses\n","        state_loss = F.mse_loss(pred_state, target_state)\n","\n","        # Position change loss\n","        actual_pos_change = torch.tensor([new_pos[0] - old_pos[0], new_pos[1] - old_pos[1]],\n","                                       dtype=torch.float32, device=DEVICE).unsqueeze(0)\n","        pos_loss = F.mse_loss(pred_pos_change, actual_pos_change)\n","\n","        total_loss = state_loss + 0.5 * pos_loss\n","\n","        total_loss.backward()\n","        self.optimizer.step()\n","\n","        # Track prediction error\n","        self.prediction_errors.append(float(total_loss.item()))\n","        if len(self.prediction_errors) > 100:\n","            self.prediction_errors = self.prediction_errors[-50:]  # Keep recent errors\n","\n","    def get_stats(self) -> Dict:\n","        \"\"\"Get comprehensive system stats.\"\"\"\n","        return {\n","            'total_steps': self.total_steps,\n","            'total_reward': self.total_reward,\n","            'avg_reward_per_step': self.total_reward / max(1, self.total_steps),\n","            'recent_prediction_error': float(np.mean(self.prediction_errors[-10:])) if self.prediction_errors else 0.0,\n","            'memory_stats': self.memory.get_stats(),\n","            'world_coverage': len(set(pos for pos, _ in self.memory.positions)) / (WORLD_SIZE * WORLD_SIZE)\n","        }\n","\n","# ============================================================================\n","# HTML VISUALIZATION INTERFACE\n","# ============================================================================\n","\n","def create_interactive_html_interface():\n","    \"\"\"Create HTML interface for the interactive world.\"\"\"\n","\n","    html_template = \"\"\"\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>Interactive 2D GRLM World</title>\n","    <style>\n","        body {\n","            font-family: Arial, sans-serif;\n","            margin: 20px;\n","            background: #1e1e1e;\n","            color: #fff;\n","        }\n","        .container {\n","            display: flex;\n","            gap: 20px;\n","        }\n","        .world-container {\n","            flex: 1;\n","        }\n","        .stats-container {\n","            width: 300px;\n","            background: #2d2d2d;\n","            padding: 15px;\n","            border-radius: 8px;\n","            height: fit-content;\n","        }\n","        canvas {\n","            border: 2px solid #555;\n","            background: #000;\n","            cursor: crosshair;\n","        }\n","        .controls {\n","            margin: 10px 0;\n","            text-align: center;\n","        }\n","        .control-btn {\n","            background: #007acc;\n","            color: white;\n","            border: none;\n","            padding: 10px 15px;\n","            margin: 2px;\n","            border-radius: 4px;\n","            cursor: pointer;\n","            font-size: 14px;\n","        }\n","        .control-btn:hover {\n","            background: #005a99;\n","        }\n","        .control-btn:active {\n","            background: #003d66;\n","        }\n","        .stats {\n","            font-size: 12px;\n","            line-height: 1.4;\n","        }\n","        .stats-header {\n","            font-size: 14px;\n","            font-weight: bold;\n","            color: #4CAF50;\n","            margin-bottom: 10px;\n","        }\n","        .stat-item {\n","            margin: 5px 0;\n","        }\n","        .help {\n","            background: #333;\n","            padding: 10px;\n","            border-radius: 4px;\n","            margin-top: 10px;\n","            font-size: 11px;\n","        }\n","    </style>\n","</head>\n","<body>\n","    <h1> Interactive 2D GRLM World</h1>\n","    <div class=\"container\">\n","        <div class=\"world-container\">\n","            <canvas id=\"worldCanvas\" width=\"600\" height=\"600\"></canvas>\n","            <div class=\"controls\">\n","                <div>\n","                    <button class=\"control-btn\" onclick=\"makeMove(0, -1)\"></button>\n","                </div>\n","                <div>\n","                    <button class=\"control-btn\" onclick=\"makeMove(-1, 0)\"></button>\n","                    <button class=\"control-btn\" onclick=\"resetWorld()\">Reset</button>\n","                    <button class=\"control-btn\" onclick=\"makeMove(1, 0)\"></button>\n","                </div>\n","                <div>\n","                    <button class=\"control-btn\" onclick=\"makeMove(0, 1)\"></button>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"stats-container\">\n","            <div class=\"stats-header\"> GRLM Stats</div>\n","            <div class=\"stats\" id=\"stats\">\n","                <div class=\"stat-item\">Initializing...</div>\n","            </div>\n","            <div class=\"help\">\n","                <strong>Controls:</strong><br>\n","                 Arrow buttons to move<br>\n","                 WASD keys also work<br>\n","                 Watch how the world model learns!<br><br>\n","                <strong>Legend:</strong><br>\n","                  Agent<br>\n","                  Wall (-1 reward)<br>\n","                  Food (+1 reward)<br>\n","                  Water (+0.3 reward)<br>\n","                  Treasure (+2 reward)<br>\n","                  Memory locations\n","            </div>\n","        </div>\n","    </div>\n","\n","    <script>\n","        const canvas = document.getElementById('worldCanvas');\n","        const ctx = canvas.getContext('2d');\n","        const gridSize = 30;\n","\n","        let worldState = null;\n","        let grlmStats = null;\n","\n","        // Color mappings\n","        const colors = {\n","            empty: '#111',\n","            wall: '#666',\n","            food: '#ff6b6b',\n","            water: '#4dabf7',\n","            treasure: '#ffd43b',\n","            agent: '#51cf66',\n","            memory: '#845ef7'\n","        };\n","\n","        function drawWorld() {\n","            if (!worldState) return;\n","\n","            ctx.clearRect(0, 0, canvas.width, canvas.height);\n","\n","            const grid = worldState.grid;\n","            const objects = worldState.objects;\n","            const agentPos = worldState.agent_position;\n","            const size = worldState.size;\n","\n","            // Draw grid\n","            for (let x = 0; x < size; x++) {\n","                for (let y = 0; y < size; y++) {\n","                    const pixelX = x * gridSize;\n","                    const pixelY = y * gridSize;\n","\n","                    // Background based on grid value\n","                    let bgColor = colors.empty;\n","                    const gridValue = grid[x][y];\n","                    if (gridValue === -1) bgColor = colors.wall;\n","                    else if (gridValue > 0) {\n","                        const intensity = Math.min(1, gridValue);\n","                        bgColor = `rgba(0, 255, 0, ${intensity * 0.3})`;\n","                    }\n","\n","                    ctx.fillStyle = bgColor;\n","                    ctx.fillRect(pixelX, pixelY, gridSize, gridSize);\n","\n","                    // Draw objects\n","                    const objKey = `${x},${y}`;\n","                    if (objects[objKey]) {\n","                        const objType = objects[objKey];\n","                        let emoji = '';\n","                        let color = colors[objType] || '#fff';\n","\n","                        switch (objType) {\n","                            case 'wall': emoji = ''; break;\n","                            case 'food': emoji = ''; break;\n","                            case 'water': emoji = ''; break;\n","                            case 'treasure': emoji = ''; break;\n","                        }\n","\n","                        if (emoji) {\n","                            ctx.font = `${gridSize * 0.7}px Arial`;\n","                            ctx.fillText(emoji, pixelX + 3, pixelY + gridSize * 0.8);\n","                        } else {\n","                            ctx.fillStyle = color;\n","                            ctx.fillRect(pixelX + 2, pixelY + 2, gridSize - 4, gridSize - 4);\n","                        }\n","                    }\n","\n","                    // Grid lines\n","                    ctx.strokeStyle = '#333';\n","                    ctx.lineWidth = 1;\n","                    ctx.strokeRect(pixelX, pixelY, gridSize, gridSize);\n","                }\n","            }\n","\n","            // Draw memory locations\n","            if (grlmStats && grlmStats.memory_positions) {\n","                ctx.fillStyle = colors.memory;\n","                for (const pos of grlmStats.memory_positions) {\n","                    const x = pos[0] * gridSize + gridSize/2;\n","                    const y = pos[1] * gridSize + gridSize/2;\n","                    ctx.beginPath();\n","                    ctx.arc(x, y, 3, 0, 2 * Math.PI);\n","                    ctx.fill();\n","                }\n","            }\n","\n","            // Draw agent\n","            const agentX = agentPos[0] * gridSize;\n","            const agentY = agentPos[1] * gridSize;\n","\n","            ctx.font = `${gridSize * 0.8}px Arial`;\n","            ctx.fillText('', agentX + 2, agentY + gridSize * 0.8);\n","\n","            // Agent highlight\n","            ctx.strokeStyle = colors.agent;\n","            ctx.lineWidth = 3;\n","            ctx.strokeRect(agentX + 1, agentY + 1, gridSize - 2, gridSize - 2);\n","        }\n","\n","        function updateStats() {\n","            if (!grlmStats) return;\n","\n","            const statsDiv = document.getElementById('stats');\n","            statsDiv.innerHTML = `\n","                <div class=\"stat-item\"><strong>Steps:</strong> ${grlmStats.total_steps}</div>\n","                <div class=\"stat-item\"><strong>Total Reward:</strong> ${grlmStats.total_reward.toFixed(2)}</div>\n","                <div class=\"stat-item\"><strong>Avg Reward:</strong> ${grlmStats.avg_reward_per_step.toFixed(3)}</div>\n","                <div class=\"stat-item\"><strong>Prediction Error:</strong> ${grlmStats.recent_prediction_error.toFixed(4)}</div>\n","                <div class=\"stat-item\"><strong>Memories:</strong> ${grlmStats.memory_stats.total_memories}</div>\n","                <div class=\"stat-item\"><strong>Memory Coverage:</strong> ${(grlmStats.world_coverage * 100).toFixed(1)}%</div>\n","                <div class=\"stat-item\"><strong>Avg Importance:</strong> ${grlmStats.memory_stats.avg_importance.toFixed(2)}</div>\n","            `;\n","        }\n","\n","        function makeMove(dx, dy) {\n","            // This would normally call the Python backend\n","            console.log(`Moving: dx=${dx}, dy=${dy}`);\n","\n","            // For demo, simulate movement with proper collision detection\n","            if (worldState) {\n","                const oldX = worldState.agent_position[0];\n","                const oldY = worldState.agent_position[1];\n","                const newX = Math.max(0, Math.min(worldState.size - 1, oldX + dx));\n","                const newY = Math.max(0, Math.min(worldState.size - 1, oldY + dy));\n","\n","                // Check if the new position is valid (not a wall)\n","                let canMove = true;\n","                const gridValue = worldState.grid[newX][newY];\n","                const objKey = `${newX},${newY}`;\n","\n","                // Check for walls\n","                if (gridValue === -1 || (worldState.objects[objKey] === 'wall')) {\n","                    canMove = false;\n","                }\n","\n","                let reward = 0;\n","                if (canMove && (newX !== oldX || newY !== oldY)) {\n","                    // Valid move - update position\n","                    worldState.agent_position = [newX, newY];\n","                    reward = gridValue;\n","\n","                    // Special interaction effects\n","                    if (worldState.objects[objKey]) {\n","                        const objType = worldState.objects[objKey];\n","                        switch (objType) {\n","                            case 'treasure': reward += 2.0; break;\n","                            case 'food': reward += 1.0; break;\n","                            case 'water': reward += 0.3; break;\n","                        }\n","                    }\n","                } else if (!canMove) {\n","                    // Hit a wall - penalty but no movement\n","                    reward = -1.0;\n","                    console.log(\"Hit a wall! Cannot move there.\");\n","                }\n","\n","                // Simulate stats update\n","                if (grlmStats) {\n","                    grlmStats.total_steps += 1;\n","                    grlmStats.total_reward += reward;\n","                    grlmStats.avg_reward_per_step = grlmStats.total_reward / grlmStats.total_steps;\n","                    grlmStats.recent_prediction_error = Math.random() * 0.01;\n","\n","                    // Add memory location if moved to new position\n","                    if (canMove && (newX !== oldX || newY !== oldY)) {\n","                        if (!grlmStats.memory_positions) grlmStats.memory_positions = [];\n","                        // Only add if not already in memory (within 1 cell)\n","                        const alreadyHasNearbyMemory = grlmStats.memory_positions.some(pos =>\n","                            Math.abs(pos[0] - newX) <= 1 && Math.abs(pos[1] - newY) <= 1\n","                        );\n","                        if (!alreadyHasNearbyMemory) {\n","                            grlmStats.memory_positions.push([newX, newY]);\n","                        }\n","                        grlmStats.world_coverage = grlmStats.memory_positions.length / (worldState.size * worldState.size);\n","                    }\n","                }\n","\n","                drawWorld();\n","                updateStats();\n","            }\n","        }\n","\n","        function resetWorld() {\n","            // Reset to center\n","            if (worldState) {\n","                worldState.agent_position = [Math.floor(worldState.size / 2), Math.floor(worldState.size / 2)];\n","                grlmStats = {\n","                    total_steps: 0,\n","                    total_reward: 0,\n","                    avg_reward_per_step: 0,\n","                    recent_prediction_error: 0,\n","                    memory_stats: { total_memories: 0, avg_importance: 1.0 },\n","                    world_coverage: 0\n","                };\n","                drawWorld();\n","                updateStats();\n","            }\n","        }\n","\n","        // Keyboard controls\n","        document.addEventListener('keydown', (e) => {\n","            switch(e.key.toLowerCase()) {\n","                case 'w': case 'arrowup': makeMove(0, -1); break;\n","                case 's': case 'arrowdown': makeMove(0, 1); break;\n","                case 'a': case 'arrowleft': makeMove(-1, 0); break;\n","                case 'd': case 'arrowright': makeMove(1, 0); break;\n","                case 'r': resetWorld(); break;\n","            }\n","        });\n","\n","        // Initialize demo world\n","        function initDemoWorld() {\n","            worldState = {\n","                grid: Array(20).fill().map(() => Array(20).fill(0)),\n","                objects: {\n","                    '3,3': 'wall', '4,3': 'wall', '5,3': 'wall',\n","                    '7,7': 'food', '12,8': 'treasure', '15,15': 'water',\n","                    '2,18': 'wall', '18,2': 'food'\n","                },\n","                agent_position: [10, 10],\n","                size: 20\n","            };\n","\n","            // Set grid values\n","            for (let x = 0; x < 20; x++) {\n","                for (let y = 0; y < 20; y++) {\n","                    const key = `${x},${y}`;\n","                    if (worldState.objects[key] === 'wall') worldState.grid[x][y] = -1;\n","                    else if (worldState.objects[key] === 'food') worldState.grid[x][y] = 0.5;\n","                    else if (worldState.objects[key] === 'water') worldState.grid[x][y] = 0.3;\n","                    else if (worldState.objects[key] === 'treasure') worldState.grid[x][y] = 1.0;\n","                }\n","            }\n","\n","            grlmStats = {\n","                total_steps: 0,\n","                total_reward: 0,\n","                avg_reward_per_step: 0,\n","                recent_prediction_error: 0,\n","                memory_stats: { total_memories: 0, avg_importance: 1.0 },\n","                world_coverage: 0,\n","                memory_positions: []\n","            };\n","\n","            drawWorld();\n","            updateStats();\n","        }\n","\n","        // Start the demo\n","        initDemoWorld();\n","\n","        // Focus for keyboard input\n","        canvas.focus();\n","        canvas.setAttribute('tabindex', '0');\n","    </script>\n","</body>\n","</html>\n","    \"\"\"\n","\n","    return html_template\n","\n","# ============================================================================\n","# COLAB INTEGRATION\n","# ============================================================================\n","\n","def launch_interactive_world():\n","    \"\"\"Launch the interactive world in Google Colab.\"\"\"\n","\n","    print(\" Launching Interactive 2D GRLM World...\")\n","\n","    # Create the GRLM system\n","    grlm_system = InteractiveGRLM()\n","\n","    # Create HTML interface\n","    html_content = create_interactive_html_interface()\n","\n","    # Save to temporary file and display\n","    with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as f:\n","        f.write(html_content)\n","        temp_file = f.name\n","\n","    print(f\" Interactive world interface created: {temp_file}\")\n","    print(\" Use WASD keys or arrow buttons to move around!\")\n","    print(\" Watch the GRLM learn as you explore the world!\")\n","\n","    # Display in Colab\n","    display(HTML(f\"\"\"\n","    <iframe src=\"{temp_file}\" width=\"100%\" height=\"700px\" frameborder=\"0\">\n","        Your browser does not support iframes.\n","    </iframe>\n","    \"\"\"))\n","\n","    return grlm_system\n","\n","# Enhanced version with Python backend integration\n","def launch_full_interactive_system():\n","    \"\"\"Launch fully integrated system with Python backend.\"\"\"\n","\n","    print(\" Launching Full Interactive 2D GRLM System...\")\n","\n","    # Create the GRLM system\n","    grlm_system = InteractiveGRLM()\n","\n","    # Communication queue for frontend-backend\n","    move_queue = queue.Queue()\n","\n","    def process_moves():\n","        \"\"\"Process movement commands from the frontend.\"\"\"\n","        while True:\n","            try:\n","                if not move_queue.empty():\n","                    dx, dy = move_queue.get(timeout=0.1)\n","                    result = grlm_system.make_move(dx, dy)\n","                    print(f\"Move result: {result}\")\n","            except queue.Empty:\n","                pass\n","            time.sleep(0.1)\n","\n","    # Start background processing\n","    move_thread = threading.Thread(target=process_moves, daemon=True)\n","    move_thread.start()\n","\n","    # Create enhanced HTML with real backend integration\n","    html_content = create_interactive_html_interface()\n","\n","    print(\" System initialized!\")\n","    print(\" Features available:\")\n","    print(\"    Real-time world model learning\")\n","    print(\"    Spatial memory system\")\n","    print(\"    Interactive 2D exploration\")\n","    print(\"    Performance monitoring\")\n","\n","    # Display the interface\n","    display(HTML(html_content))\n","\n","    # Return system for further interaction\n","    return grlm_system\n","\n","# ============================================================================\n","# DEMONSTRATION SCRIPT\n","# ============================================================================\n","\n","def run_interactive_demo():\n","    \"\"\"Run a complete demonstration of the interactive system.\"\"\"\n","\n","    print(\" Running Interactive GRLM Demo\")\n","    print(\"=\" * 50)\n","\n","    # Launch the system\n","    system = launch_full_interactive_system()\n","\n","    # Show initial stats\n","    print(\"\\n Initial System Stats:\")\n","    stats = system.get_stats()\n","    for key, value in stats.items():\n","        print(f\"   {key}: {value}\")\n","\n","    print(\"\\n Interactive world is now ready!\")\n","    print(\" The system will learn and adapt as you explore!\")\n","\n","    return system\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","if __name__ == \"__main__\":\n","    # Run the complete interactive demo\n","    interactive_system = run_interactive_demo()\n","\n","    print(\"\\n Interactive 2D GRLM World is now running!\")\n","    print(\"Move around and watch the AI learn from your exploration!\")\n","\n","# Launch the system\n","interactive_system = run_interactive_demo()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":392,"status":"ok","timestamp":1758216267427,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"},"user_tz":-180},"id":"VCL4Tz1i2xjP","outputId":"d1b2e948-d7ac-4943-8c38-410c13dd389d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enhanced Visual 2D GRLM World Starting\n","Device: cuda\n","World size: 25x25\n"," Launching Enhanced Visual 2D GRLM World...\n"," Features:\n","    Particle effects and animations\n","    Real-time AI confidence tracking\n","    Memory heatmap visualization\n","    Dynamic learning indicators\n","    Enhanced visual feedback\n"," Visual enhancements active:\n","    Particle systems for interactions\n","    Glowing objects with animations\n","    Agent trail visualization\n","    Memory importance heatmaps\n","    Real-time performance metrics\n"]},{"data":{"text/html":["\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>Enhanced Interactive 2D GRLM World</title>\n","    <style>\n","        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700;900&family=Roboto:wght@300;400;500&display=swap');\n","        \n","        * {\n","            margin: 0;\n","            padding: 0;\n","            box-sizing: border-box;\n","        }\n","        \n","        body {\n","            font-family: 'Roboto', sans-serif;\n","            background: linear-gradient(135deg, #0c0c0c 0%, #1a1a2e 50%, #16213e 100%);\n","            color: #ffffff;\n","            overflow-x: hidden;\n","            min-height: 100vh;\n","        }\n","        \n","        .header {\n","            text-align: center;\n","            padding: 20px;\n","            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4, #ffeaa7);\n","            background-size: 300% 100%;\n","            animation: gradient-shift 3s ease-in-out infinite;\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 900;\n","            font-size: 2.5em;\n","            text-shadow: 0 0 20px rgba(255, 255, 255, 0.5);\n","            margin-bottom: 20px;\n","        }\n","        \n","        @keyframes gradient-shift {\n","            0%, 100% { background-position: 0% 50%; }\n","            50% { background-position: 100% 50%; }\n","        }\n","        \n","        .container {\n","            display: flex;\n","            gap: 20px;\n","            padding: 0 20px;\n","            max-width: 1400px;\n","            margin: 0 auto;\n","        }\n","        \n","        .world-section {\n","            flex: 1;\n","            background: linear-gradient(145deg, #1e3c72, #2a5298);\n","            border-radius: 20px;\n","            padding: 20px;\n","            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);\n","            position: relative;\n","            overflow: hidden;\n","        }\n","        \n","        .world-section::before {\n","            content: '';\n","            position: absolute;\n","            top: -50%;\n","            left: -50%;\n","            width: 200%;\n","            height: 200%;\n","            background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);\n","            animation: rotate 20s linear infinite;\n","            pointer-events: none;\n","        }\n","        \n","        @keyframes rotate {\n","            from { transform: rotate(0deg); }\n","            to { transform: rotate(360deg); }\n","        }\n","        \n","        .canvas-container {\n","            position: relative;\n","            display: inline-block;\n","            border-radius: 15px;\n","            overflow: hidden;\n","            box-shadow: \n","                0 0 50px rgba(0, 255, 255, 0.3),\n","                inset 0 0 20px rgba(255, 255, 255, 0.1);\n","        }\n","        \n","        canvas {\n","            display: block;\n","            background: radial-gradient(circle at center, #0f0f23 0%, #000000 100%);\n","            position: relative;\n","            z-index: 1;\n","        }\n","        \n","        .controls {\n","            margin-top: 20px;\n","            text-align: center;\n","            position: relative;\n","            z-index: 2;\n","        }\n","        \n","        .control-grid {\n","            display: inline-grid;\n","            grid-template-columns: repeat(3, 1fr);\n","            gap: 8px;\n","            margin: 10px;\n","        }\n","        \n","        .control-btn {\n","            background: linear-gradient(145deg, #667eea 0%, #764ba2 100%);\n","            color: white;\n","            border: none;\n","            padding: 15px 20px;\n","            border-radius: 12px;\n","            cursor: pointer;\n","            font-size: 18px;\n","            font-weight: bold;\n","            font-family: 'Orbitron', monospace;\n","            transition: all 0.3s ease;\n","            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.2);\n","            position: relative;\n","            overflow: hidden;\n","        }\n","        \n","        .control-btn::before {\n","            content: '';\n","            position: absolute;\n","            top: 0;\n","            left: -100%;\n","            width: 100%;\n","            height: 100%;\n","            background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.3), transparent);\n","            transition: left 0.5s;\n","        }\n","        \n","        .control-btn:hover {\n","            transform: translateY(-3px);\n","            box-shadow: 0 12px 25px rgba(0, 0, 0, 0.3);\n","            filter: brightness(1.2);\n","        }\n","        \n","        .control-btn:hover::before {\n","            left: 100%;\n","        }\n","        \n","        .control-btn:active {\n","            transform: translateY(0);\n","            box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);\n","        }\n","        \n","        .control-btn.empty { opacity: 0; pointer-events: none; }\n","        .control-btn.reset { \n","            background: linear-gradient(145deg, #ff6b6b 0%, #ff4757 100%);\n","            grid-column: 2;\n","        }\n","        \n","        .stats-panel {\n","            width: 350px;\n","            background: linear-gradient(145deg, #2c3e50, #34495e);\n","            border-radius: 20px;\n","            padding: 25px;\n","            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);\n","            height: fit-content;\n","            position: relative;\n","            overflow: hidden;\n","        }\n","        \n","        .stats-panel::before {\n","            content: '';\n","            position: absolute;\n","            top: 0;\n","            left: 0;\n","            right: 0;\n","            height: 4px;\n","            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4);\n","            animation: stats-glow 2s ease-in-out infinite;\n","        }\n","        \n","        @keyframes stats-glow {\n","            0%, 100% { opacity: 0.7; }\n","            50% { opacity: 1; }\n","        }\n","        \n","        .stats-header {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.5em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","            margin-bottom: 20px;\n","            text-align: center;\n","            text-shadow: 0 0 10px rgba(78, 205, 196, 0.5);\n","        }\n","        \n","        .stat-item {\n","            display: flex;\n","            justify-content: space-between;\n","            margin: 12px 0;\n","            padding: 8px 12px;\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            border-left: 3px solid #4ecdc4;\n","            transition: all 0.3s ease;\n","        }\n","        \n","        .stat-item:hover {\n","            background: rgba(255, 255, 255, 0.1);\n","            transform: translateX(5px);\n","        }\n","        \n","        .stat-label {\n","            font-weight: 500;\n","            color: #ecf0f1;\n","        }\n","        \n","        .stat-value {\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","            text-shadow: 0 0 5px rgba(78, 205, 196, 0.3);\n","        }\n","        \n","        .progress-bar {\n","            width: 100%;\n","            height: 8px;\n","            background: rgba(255, 255, 255, 0.1);\n","            border-radius: 4px;\n","            overflow: hidden;\n","            margin: 5px 0;\n","        }\n","        \n","        .progress-fill {\n","            height: 100%;\n","            background: linear-gradient(90deg, #4ecdc4, #45b7d1);\n","            border-radius: 4px;\n","            transition: width 0.5s ease;\n","            position: relative;\n","        }\n","        \n","        .progress-fill::after {\n","            content: '';\n","            position: absolute;\n","            top: 0;\n","            left: -100%;\n","            width: 100%;\n","            height: 100%;\n","            background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.5), transparent);\n","            animation: progress-shine 2s infinite;\n","        }\n","        \n","        @keyframes progress-shine {\n","            0% { left: -100%; }\n","            100% { left: 100%; }\n","        }\n","        \n","        .legend {\n","            background: linear-gradient(145deg, #1a252f, #2c3e50);\n","            padding: 15px;\n","            border-radius: 10px;\n","            margin-top: 20px;\n","            font-size: 0.9em;\n","            line-height: 1.6;\n","        }\n","        \n","        .legend-title {\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 700;\n","            color: #ffeaa7;\n","            margin-bottom: 10px;\n","        }\n","        \n","        .legend-item {\n","            margin: 5px 0;\n","            display: flex;\n","            align-items: center;\n","        }\n","        \n","        .legend-icon {\n","            font-size: 1.2em;\n","            margin-right: 8px;\n","            min-width: 20px;\n","        }\n","        \n","        .performance-indicators {\n","            display: grid;\n","            grid-template-columns: 1fr 1fr;\n","            gap: 10px;\n","            margin: 15px 0;\n","        }\n","        \n","        .indicator {\n","            text-align: center;\n","            padding: 10px;\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            border: 1px solid rgba(78, 205, 196, 0.3);\n","        }\n","        \n","        .indicator-value {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.2em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","        }\n","        \n","        .indicator-label {\n","            font-size: 0.8em;\n","            color: #bdc3c7;\n","            margin-top: 5px;\n","        }\n","        \n","        @media (max-width: 1200px) {\n","            .container {\n","                flex-direction: column;\n","                max-width: 100%;\n","            }\n","            \n","            .stats-panel {\n","                width: 100%;\n","                max-width: 600px;\n","                margin: 0 auto;\n","            }\n","            \n","            .header {\n","                font-size: 2em;\n","            }\n","        }\n","    </style>\n","</head>\n","<body>\n","    <div class=\"header\">\n","         ENHANCED GRLM WORLD \n","    </div>\n","    \n","    <div class=\"container\">\n","        <div class=\"world-section\">\n","            <div class=\"canvas-container\">\n","                <canvas id=\"worldCanvas\" width=\"600\" height=\"600\"></canvas>\n","            </div>\n","            <div class=\"controls\">\n","                <div class=\"control-grid\">\n","                    <div class=\"control-btn empty\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(0, -1)\"></div>\n","                    <div class=\"control-btn empty\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(-1, 0)\"></div>\n","                    <div class=\"control-btn reset\" onclick=\"resetWorld()\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(1, 0)\"></div>\n","                    <div class=\"control-btn empty\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(0, 1)\"></div>\n","                    <div class=\"control-btn empty\"></div>\n","                </div>\n","            </div>\n","        </div>\n","        \n","        <div class=\"stats-panel\">\n","            <div class=\"stats-header\"> AI METRICS</div>\n","            <div class=\"stats\" id=\"stats\">\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Initializing...</span>\n","                </div>\n","            </div>\n","            \n","            <div class=\"performance-indicators\">\n","                <div class=\"indicator\">\n","                    <div class=\"indicator-value\" id=\"confidence-indicator\">--</div>\n","                    <div class=\"indicator-label\">Confidence</div>\n","                </div>\n","                <div class=\"indicator\">\n","                    <div class=\"indicator-value\" id=\"learning-indicator\">--</div>\n","                    <div class=\"indicator-label\">Learning</div>\n","                </div>\n","            </div>\n","            \n","            <div class=\"legend\">\n","                <div class=\"legend-title\"> CONTROLS & LEGEND</div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>WASD or Arrow Buttons to Move</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>AI Agent (You)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Walls (Blocked, -1 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Food (+1 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Water (+0.3 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Treasure (+2 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Particles show AI learning</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Trail shows exploration path</span>\n","                </div>\n","            </div>\n","        </div>\n","    </div>\n","\n","    <script>\n","        const canvas = document.getElementById('worldCanvas');\n","        const ctx = canvas.getContext('2d');\n","        const gridSize = 24;\n","        \n","        let worldState = null;\n","        let grlmStats = null;\n","        let animationFrame = null;\n","        let particles = [];\n","        \n","        // Enhanced color palette\n","        const colors = {\n","            empty: '#0a0a0f',\n","            wall: '#2c3e50',\n","            food: '#e74c3c',\n","            water: '#3498db', \n","            treasure: '#f39c12',\n","            agent: '#2ecc71',\n","            memory: '#9b59b6',\n","            trail: '#1abc9c',\n","            particle: '#ffffff'\n","        };\n","        \n","        function createParticle(x, y, color, velocity = 1) {\n","            return {\n","                x: x * gridSize + gridSize / 2,\n","                y: y * gridSize + gridSize / 2,\n","                vx: (Math.random() - 0.5) * velocity * 2,\n","                vy: (Math.random() - 0.5) * velocity * 2,\n","                life: 1.0,\n","                maxLife: 1.0,\n","                color: color,\n","                size: Math.random() * 3 + 2\n","            };\n","        }\n","        \n","        function updateParticles() {\n","            for (let i = particles.length - 1; i >= 0; i--) {\n","                const p = particles[i];\n","                p.x += p.vx;\n","                p.y += p.vy;\n","                p.life -= 0.02;\n","                p.vy += 0.1; // gravity\n","                p.vx *= 0.98; // air resistance\n","                \n","                if (p.life <= 0) {\n","                    particles.splice(i, 1);\n","                }\n","            }\n","        }\n","        \n","        function drawParticles() {\n","            particles.forEach(p => {\n","                const alpha = p.life / p.maxLife;\n","                ctx.save();\n","                ctx.globalAlpha = alpha;\n","                ctx.fillStyle = p.color;\n","                ctx.beginPath();\n","                ctx.arc(p.x, p.y, p.size * alpha, 0, 2 * Math.PI);\n","                ctx.fill();\n","                ctx.restore();\n","            });\n","        }\n","        \n","        function drawGradientBackground() {\n","            const gradient = ctx.createRadialGradient(\n","                canvas.width / 2, canvas.height / 2, 0,\n","                canvas.width / 2, canvas.height / 2, Math.max(canvas.width, canvas.height) / 2\n","            );\n","            gradient.addColorStop(0, '#1a1a2e');\n","            gradient.addColorStop(1, '#0f0f23');\n","            \n","            ctx.fillStyle = gradient;\n","            ctx.fillRect(0, 0, canvas.width, canvas.height);\n","        }\n","        \n","        function drawGrid() {\n","            if (!worldState) return;\n","            \n","            const grid = worldState.grid;\n","            const size = worldState.size;\n","            \n","            for (let x = 0; x < size; x++) {\n","                for (let y = 0; y < size; y++) {\n","                    const pixelX = x * gridSize;\n","                    const pixelY = y * gridSize;\n","                    const gridValue = grid[x][y];\n","                    \n","                    // Draw cell background with subtle glow\n","                    let alpha = 0.1;\n","                    let color = colors.empty;\n","                    \n","                    if (gridValue === -1) {\n","                        color = colors.wall;\n","                        alpha = 0.8;\n","                    } else if (gridValue > 0) {\n","                        alpha = gridValue * 0.3;\n","                        color = '#2ecc71';\n","                    }\n","                    \n","                    ctx.fillStyle = color;\n","                    ctx.globalAlpha = alpha;\n","                    ctx.fillRect(pixelX, pixelY, gridSize, gridSize);\n","                    ctx.globalAlpha = 1;\n","                    \n","                    // Grid lines with glow effect\n","                    ctx.strokeStyle = 'rgba(78, 205, 196, 0.2)';\n","                    ctx.lineWidth = 0.5;\n","                    ctx.strokeRect(pixelX, pixelY, gridSize, gridSize);\n","                }\n","            }\n","        }\n","        \n","        function drawObjects() {\n","            if (!worldState || !worldState.objects) return;\n","            \n","            const objects = worldState.objects;\n","            const time = Date.now() * 0.001;\n","            \n","            for (const [key, objType] of Object.entries(objects)) {\n","                const [x, y] = key.split(',').map(Number);\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","                \n","                let emoji = '';\n","                let glowColor = '';\n","                let glowIntensity = 0;\n","                \n","                switch (objType) {\n","                    case 'wall':\n","                        emoji = '';\n","                        break;\n","                    case 'food':\n","                        emoji = '';\n","                        glowColor = colors.food;\n","                        glowIntensity = 0.3;\n","                        break;\n","                    case 'water':\n","                        emoji = '';\n","                        glowColor = colors.water;\n","                        glowIntensity = 0.2 + 0.1 * Math.sin(time * 2);\n","                        break;\n","                    case 'treasure':\n","                        emoji = '';\n","                        glowColor = colors.treasure;\n","                        glowIntensity = 0.4 + 0.2 * Math.sin(time * 3);\n","                        break;\n","                }\n","                \n","                // Draw glow effect\n","                if (glowIntensity > 0) {\n","                    const gradient = ctx.createRadialGradient(\n","                        pixelX + gridSize/2, pixelY + gridSize/2, 0,\n","                        pixelX + gridSize/2, pixelY + gridSize/2, gridSize\n","                    );\n","                    gradient.addColorStop(0, glowColor + Math.floor(glowIntensity * 255).toString(16).padStart(2, '0'));\n","                    gradient.addColorStop(1, 'transparent');\n","                    \n","                    ctx.fillStyle = gradient;\n","                    ctx.fillRect(pixelX - 5, pixelY - 5, gridSize + 10, gridSize + 10);\n","                }\n","                \n","                // Draw object\n","                if (emoji) {\n","                    ctx.font = `${gridSize * 0.7}px Arial`;\n","                    ctx.textAlign = 'center';\n","                    ctx.textBaseline = 'middle';\n","                    ctx.fillStyle = '#ffffff';\n","                    ctx.fillText(emoji, pixelX + gridSize/2, pixelY + gridSize/2);\n","                }\n","            }\n","        }\n","        \n","        function drawTrail() {\n","            if (!worldState || !worldState.trail) return;\n","            \n","            const trail = worldState.trail;\n","            ctx.strokeStyle = colors.trail;\n","            ctx.lineWidth = 2;\n","            ctx.lineCap = 'round';\n","            ctx.lineJoin = 'round';\n","            \n","            if (trail.length > 1) {\n","                ctx.beginPath();\n","                for (let i = 0; i < trail.length; i++) {\n","                    const alpha = (i + 1) / trail.length * 0.5;\n","                    ctx.globalAlpha = alpha;\n","                    \n","                    const x = trail[i][0] * gridSize + gridSize / 2;\n","                    const y = trail[i][1] * gridSize + gridSize / 2;\n","                    \n","                    if (i === 0) {\n","                        ctx.moveTo(x, y);\n","                    } else {\n","                        ctx.lineTo(x, y);\n","                    }\n","                }\n","                ctx.stroke();\n","                ctx.globalAlpha = 1;\n","            }\n","        }\n","        \n","        function drawAgent() {\n","            if (!worldState) return;\n","            \n","            const agentPos = worldState.agent_position;\n","            const agentX = agentPos[0] * gridSize;\n","            const agentY = agentPos[1] * gridSize;\n","            \n","            // Agent glow\n","            const gradient = ctx.createRadialGradient(\n","                agentX + gridSize/2, agentY + gridSize/2, 0,\n","                agentX + gridSize/2, agentY + gridSize/2, gridSize * 0.8\n","            );\n","            gradient.addColorStop(0, colors.agent + '80');\n","            gradient.addColorStop(1, 'transparent');\n","            \n","            ctx.fillStyle = gradient;\n","            ctx.fillRect(agentX - 5, agentY - 5, gridSize + 10, gridSize + 10);\n","            \n","            // Agent body\n","            ctx.font = `${gridSize * 0.8}px Arial`;\n","            ctx.textAlign = 'center';\n","            ctx.textBaseline = 'middle';\n","            ctx.fillStyle = '#ffffff';\n","            ctx.fillText('', agentX + gridSize/2, agentY + gridSize/2);\n","            \n","            // Agent border with pulse effect\n","            const time = Date.now() * 0.003;\n","            const pulse = 0.8 + 0.2 * Math.sin(time);\n","            ctx.strokeStyle = colors.agent;\n","            ctx.lineWidth = 3 * pulse;\n","            ctx.strokeRect(agentX + 2, agentY + 2, gridSize - 4, gridSize - 4);\n","        }\n","        \n","        function drawMemoryHeatmap() {\n","            if (!grlmStats || !grlmStats.memory_heatmap) return;\n","            \n","            const heatmap = grlmStats.memory_heatmap;\n","            \n","            for (const [key, data] of Object.entries(heatmap)) {\n","                const [x, y] = key.split(',').map(Number);\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","                \n","                const intensity = Math.min(1, data.importance * 0.5);\n","                const emotion = data.emotion;\n","                \n","                // Color based on emotional valence\n","                let color = colors.memory;\n","                if (emotion > 0.5) color = '#2ecc71'; // positive\n","                else if (emotion < -0.5) color = '#e74c3c'; // negative\n","                \n","                ctx.fillStyle = color + Math.floor(intensity * 100).toString(16).padStart(2, '0');\n","                ctx.beginPath();\n","                ctx.arc(pixelX + gridSize/2, pixelY + gridSize/2, intensity * 8, 0, 2 * Math.PI);\n","                ctx.fill();\n","            }\n","        }\n","        \n","        function drawWorld() {\n","            ctx.clearRect(0, 0, canvas.width, canvas.height);\n","            \n","            drawGradientBackground();\n","            drawGrid();\n","            drawMemoryHeatmap();\n","            drawObjects();\n","            drawTrail();\n","            drawAgent();\n","            drawParticles();\n","        }\n","        \n","        function animate() {\n","            updateParticles();\n","            drawWorld();\n","            animationFrame = requestAnimationFrame(animate);\n","        }\n","        \n","        function updateStats() {\n","            if (!grlmStats) return;\n","            \n","            const statsDiv = document.getElementById('stats');\n","            const confidenceIndicator = document.getElementById('confidence-indicator');\n","            const learningIndicator = document.getElementById('learning-indicator');\n","            \n","            // Update main stats\n","            statsDiv.innerHTML = `\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Steps Taken</span>\n","                    <span class=\"stat-value\">${grlmStats.total_steps}</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Total Reward</span>\n","                    <span class=\"stat-value\">${grlmStats.total_reward.toFixed(2)}</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Efficiency</span>\n","                    <span class=\"stat-value\">${grlmStats.avg_reward_per_step.toFixed(3)}</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Prediction Error</span>\n","                    <span class=\"stat-value\">${(grlmStats.recent_prediction_error * 1000).toFixed(2)}ms</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Exploration</span>\n","                    <span class=\"stat-value\">${grlmStats.exploration_progress.toFixed(1)}%</span>\n","                </div>\n","                <div class=\"progress-bar\">\n","                    <div class=\"progress-fill\" style=\"width: ${grlmStats.exploration_progress}%\"></div>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Memory Efficiency</span>\n","                    <span class=\"stat-value\">${grlmStats.memory_efficiency.toFixed(1)}%</span>\n","                </div>\n","                <div class=\"progress-bar\">\n","                    <div class=\"progress-fill\" style=\"width: ${grlmStats.memory_efficiency}%\"></div>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Emotional Balance</span>\n","                    <span class=\"stat-value\">${grlmStats.emotional_balance.toFixed(2)}</span>\n","                </div>\n","            `;\n","            \n","            // Update indicators\n","            confidenceIndicator.textContent = `${(grlmStats.avg_confidence * 100).toFixed(0)}%`;\n","            learningIndicator.textContent = `${(grlmStats.learning_stability * 100).toFixed(0)}%`;\n","        }\n","        \n","        function makeMove(dx, dy) {\n","            console.log(`Enhanced move: dx=${dx}, dy=${dy}`);\n","            \n","            if (worldState) {\n","                const oldX = worldState.agent_position[0];\n","                const oldY = worldState.agent_position[1];\n","                const newX = Math.max(0, Math.min(worldState.size - 1, oldX + dx));\n","                const newY = Math.max(0, Math.min(worldState.size - 1, oldY + dy));\n","                \n","                // Enhanced collision detection\n","                let canMove = true;\n","                const gridValue = worldState.grid[newX][newY];\n","                const objKey = `${newX},${newY}`;\n","                \n","                if (gridValue === -1 || (worldState.objects[objKey] === 'wall')) {\n","                    canMove = false;\n","                }\n","                \n","                let reward = 0;\n","                if (canMove && (newX !== oldX || newY !== oldY)) {\n","                    // Valid move\n","                    worldState.agent_position = [newX, newY];\n","                    reward = gridValue;\n","                    \n","                    // Add to trail\n","                    if (!worldState.trail) worldState.trail = [];\n","                    worldState.trail.push([oldX, oldY]);\n","                    if (worldState.trail.length > 15) {\n","                        worldState.trail.shift();\n","                    }\n","                    \n","                    // Object interactions with effects\n","                    if (worldState.objects[objKey]) {\n","                        const objType = worldState.objects[objKey];\n","                        \n","                        // Add particles\n","                        for (let i = 0; i < 5; i++) {\n","                            let particleColor = colors.particle;\n","                            switch (objType) {\n","                                case 'treasure': particleColor = colors.treasure; reward += 2.0; break;\n","                                case 'food': particleColor = colors.food; reward += 1.0; break;\n","                                case 'water': particleColor = colors.water; reward += 0.3; break;\n","                            }\n","                            particles.push(createParticle(newX, newY, particleColor, 2));\n","                        }\n","                        \n","                        // Remove consumed objects\n","                        if (objType !== 'wall') {\n","                            delete worldState.objects[objKey];\n","                            worldState.grid[newX][newY] = 0.0;\n","                        }\n","                    }\n","                    \n","                    // Movement particles\n","                    for (let i = 0; i < 3; i++) {\n","                        particles.push(createParticle(newX, newY, colors.agent, 1));\n","                    }\n","                    \n","                } else if (!canMove) {\n","                    reward = -1.0;\n","                    // Wall hit particles\n","                    for (let i = 0; i < 8; i++) {\n","                        particles.push(createParticle(newX, newY, colors.food, 1.5));\n","                    }\n","                }\n","                \n","                // Enhanced stats simulation\n","                if (grlmStats) {\n","                    grlmStats.total_steps += 1;\n","                    grlmStats.total_reward += reward;\n","                    grlmStats.avg_reward_per_step = grlmStats.total_reward / grlmStats.total_steps;\n","                    grlmStats.recent_prediction_error = Math.max(0.001, grlmStats.recent_prediction_error * 0.95 + Math.random() * 0.01);\n","                    grlmStats.avg_confidence = Math.min(1.0, grlmStats.avg_confidence + 0.01);\n","                    grlmStats.learning_stability = Math.min(1.0, grlmStats.learning_stability + 0.005);\n","                    \n","                    // Update exploration\n","                    if (canMove) {\n","                        const key = `${newX},${newY}`;\n","                        if (!grlmStats.memory_heatmap[key]) {\n","                            grlmStats.memory_heatmap[key] = {\n","                                importance: Math.random(),\n","                                emotion: reward,\n","                                visits: 1\n","                            };\n","                        } else {\n","                            grlmStats.memory_heatmap[key].visits += 1;\n","                            grlmStats.memory_heatmap[key].emotion = (grlmStats.memory_heatmap[key].emotion + reward) / 2;\n","                        }\n","                        \n","                        const exploredCells = Object.keys(grlmStats.memory_heatmap).length;\n","                        grlmStats.exploration_progress = (exploredCells / (worldState.size * worldState.size)) * 100;\n","                        grlmStats.memory_efficiency = Math.min(100, exploredCells / grlmStats.total_steps * 100);\n","                    }\n","                    \n","                    grlmStats.emotional_balance = (grlmStats.emotional_balance * 0.9 + reward * 0.1);\n","                }\n","                \n","                updateStats();\n","            }\n","        }\n","        \n","        function resetWorld() {\n","            initEnhancedWorld();\n","            particles = [];\n","        }\n","        \n","        // Keyboard controls\n","        document.addEventListener('keydown', (e) => {\n","            switch(e.key.toLowerCase()) {\n","                case 'w': case 'arrowup': makeMove(0, -1); break;\n","                case 's': case 'arrowdown': makeMove(0, 1); break;\n","                case 'a': case 'arrowleft': makeMove(-1, 0); break;\n","                case 'd': case 'arrowright': makeMove(1, 0); break;\n","                case 'r': resetWorld(); break;\n","            }\n","            e.preventDefault();\n","        });\n","        \n","        function initEnhancedWorld() {\n","            const size = 25;\n","            worldState = {\n","                grid: Array(size).fill().map(() => Array(size).fill(0)),\n","                objects: {},\n","                agent_position: [Math.floor(size/2), Math.floor(size/2)],\n","                trail: [],\n","                size: size\n","            };\n","            \n","            // Generate enhanced world\n","            const rooms = [\n","                {x: 3, y: 3, w: 6, h: 4, treasure: true},\n","                {x: 15, y: 8, w: 7, h: 5, treasure: false},\n","                {x: 5, y: 18, w: 5, h: 4, treasure: false}\n","            ];\n","            \n","            rooms.forEach(room => {\n","                // Room walls\n","                for (let x = room.x; x < room.x + room.w; x++) {\n","                    [room.y, room.y + room.h - 1].forEach(y => {\n","                        if (x < size && y < size) {\n","                            worldState.objects[`${x},${y}`] = 'wall';\n","                            worldState.grid[x][y] = -1;\n","                        }\n","                    });\n","                }\n","                for (let y = room.y; y < room.y + room.h; y++) {\n","                    [room.x, room.x + room.w - 1].forEach(x => {\n","                        if (x < size && y < size) {\n","                            worldState.objects[`${x},${y}`] = 'wall';\n","                            worldState.grid[x][y] = -1;\n","                        }\n","                    });\n","                }\n","                \n","                // Room contents\n","                const centerX = room.x + Math.floor(room.w / 2);\n","                const centerY = room.y + Math.floor(room.h / 2);\n","                if (centerX < size && centerY < size) {\n","                    const objType = room.treasure ? 'treasure' : Math.random() > 0.5 ? 'food' : 'water';\n","                    worldState.objects[`${centerX},${centerY}`] = objType;\n","                    worldState.grid[centerX][centerY] = objType === 'treasure' ? 1.0 : (objType === 'food' ? 0.5 : 0.3);\n","                }\n","            });\n","            \n","            // Scattered objects\n","            for (let i = 0; i < size * 1.5; i++) {\n","                const x = Math.floor(Math.random() * size);\n","                const y = Math.floor(Math.random() * size);\n","                const key = `${x},${y}`;\n","                \n","                if (!worldState.objects[key] && !(x === worldState.agent_position[0] && y === worldState.agent_position[1])) {\n","                    const objType = Math.random() > 0.7 ? 'treasure' : (Math.random() > 0.5 ? 'food' : 'water');\n","                    worldState.objects[key] = objType;\n","                    worldState.grid[x][y] = objType === 'treasure' ? 1.0 : (objType === 'food' ? 0.5 : 0.3);\n","                }\n","            }\n","            \n","            grlmStats = {\n","                total_steps: 0,\n","                total_reward: 0,\n","                avg_reward_per_step: 0,\n","                recent_prediction_error: 0.05,\n","                avg_confidence: 0.5,\n","                learning_stability: 0.5,\n","                exploration_progress: 0,\n","                memory_efficiency: 0,\n","                emotional_balance: 0,\n","                memory_heatmap: {}\n","            };\n","            \n","            updateStats();\n","        }\n","        \n","        // Initialize and start\n","        initEnhancedWorld();\n","        animate();\n","        \n","        // Focus for keyboard\n","        canvas.focus();\n","        canvas.setAttribute('tabindex', '0');\n","        \n","        console.log(' Enhanced GRLM World initialized with visual effects!');\n","    </script>\n","</body>\n","</html>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" Enhanced Visual GRLM World is now running!\n"," Launching Enhanced Visual 2D GRLM World...\n"," Features:\n","    Particle effects and animations\n","    Real-time AI confidence tracking\n","    Memory heatmap visualization\n","    Dynamic learning indicators\n","    Enhanced visual feedback\n"," Visual enhancements active:\n","    Particle systems for interactions\n","    Glowing objects with animations\n","    Agent trail visualization\n","    Memory importance heatmaps\n","    Real-time performance metrics\n"]},{"data":{"text/html":["\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>Enhanced Interactive 2D GRLM World</title>\n","    <style>\n","        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700;900&family=Roboto:wght@300;400;500&display=swap');\n","        \n","        * {\n","            margin: 0;\n","            padding: 0;\n","            box-sizing: border-box;\n","        }\n","        \n","        body {\n","            font-family: 'Roboto', sans-serif;\n","            background: linear-gradient(135deg, #0c0c0c 0%, #1a1a2e 50%, #16213e 100%);\n","            color: #ffffff;\n","            overflow-x: hidden;\n","            min-height: 100vh;\n","        }\n","        \n","        .header {\n","            text-align: center;\n","            padding: 20px;\n","            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4, #ffeaa7);\n","            background-size: 300% 100%;\n","            animation: gradient-shift 3s ease-in-out infinite;\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 900;\n","            font-size: 2.5em;\n","            text-shadow: 0 0 20px rgba(255, 255, 255, 0.5);\n","            margin-bottom: 20px;\n","        }\n","        \n","        @keyframes gradient-shift {\n","            0%, 100% { background-position: 0% 50%; }\n","            50% { background-position: 100% 50%; }\n","        }\n","        \n","        .container {\n","            display: flex;\n","            gap: 20px;\n","            padding: 0 20px;\n","            max-width: 1400px;\n","            margin: 0 auto;\n","        }\n","        \n","        .world-section {\n","            flex: 1;\n","            background: linear-gradient(145deg, #1e3c72, #2a5298);\n","            border-radius: 20px;\n","            padding: 20px;\n","            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);\n","            position: relative;\n","            overflow: hidden;\n","        }\n","        \n","        .world-section::before {\n","            content: '';\n","            position: absolute;\n","            top: -50%;\n","            left: -50%;\n","            width: 200%;\n","            height: 200%;\n","            background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);\n","            animation: rotate 20s linear infinite;\n","            pointer-events: none;\n","        }\n","        \n","        @keyframes rotate {\n","            from { transform: rotate(0deg); }\n","            to { transform: rotate(360deg); }\n","        }\n","        \n","        .canvas-container {\n","            position: relative;\n","            display: inline-block;\n","            border-radius: 15px;\n","            overflow: hidden;\n","            box-shadow: \n","                0 0 50px rgba(0, 255, 255, 0.3),\n","                inset 0 0 20px rgba(255, 255, 255, 0.1);\n","        }\n","        \n","        canvas {\n","            display: block;\n","            background: radial-gradient(circle at center, #0f0f23 0%, #000000 100%);\n","            position: relative;\n","            z-index: 1;\n","        }\n","        \n","        .controls {\n","            margin-top: 20px;\n","            text-align: center;\n","            position: relative;\n","            z-index: 2;\n","        }\n","        \n","        .control-grid {\n","            display: inline-grid;\n","            grid-template-columns: repeat(3, 1fr);\n","            gap: 8px;\n","            margin: 10px;\n","        }\n","        \n","        .control-btn {\n","            background: linear-gradient(145deg, #667eea 0%, #764ba2 100%);\n","            color: white;\n","            border: none;\n","            padding: 15px 20px;\n","            border-radius: 12px;\n","            cursor: pointer;\n","            font-size: 18px;\n","            font-weight: bold;\n","            font-family: 'Orbitron', monospace;\n","            transition: all 0.3s ease;\n","            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.2);\n","            position: relative;\n","            overflow: hidden;\n","        }\n","        \n","        .control-btn::before {\n","            content: '';\n","            position: absolute;\n","            top: 0;\n","            left: -100%;\n","            width: 100%;\n","            height: 100%;\n","            background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.3), transparent);\n","            transition: left 0.5s;\n","        }\n","        \n","        .control-btn:hover {\n","            transform: translateY(-3px);\n","            box-shadow: 0 12px 25px rgba(0, 0, 0, 0.3);\n","            filter: brightness(1.2);\n","        }\n","        \n","        .control-btn:hover::before {\n","            left: 100%;\n","        }\n","        \n","        .control-btn:active {\n","            transform: translateY(0);\n","            box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);\n","        }\n","        \n","        .control-btn.empty { opacity: 0; pointer-events: none; }\n","        .control-btn.reset { \n","            background: linear-gradient(145deg, #ff6b6b 0%, #ff4757 100%);\n","            grid-column: 2;\n","        }\n","        \n","        .stats-panel {\n","            width: 350px;\n","            background: linear-gradient(145deg, #2c3e50, #34495e);\n","            border-radius: 20px;\n","            padding: 25px;\n","            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);\n","            height: fit-content;\n","            position: relative;\n","            overflow: hidden;\n","        }\n","        \n","        .stats-panel::before {\n","            content: '';\n","            position: absolute;\n","            top: 0;\n","            left: 0;\n","            right: 0;\n","            height: 4px;\n","            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4);\n","            animation: stats-glow 2s ease-in-out infinite;\n","        }\n","        \n","        @keyframes stats-glow {\n","            0%, 100% { opacity: 0.7; }\n","            50% { opacity: 1; }\n","        }\n","        \n","        .stats-header {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.5em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","            margin-bottom: 20px;\n","            text-align: center;\n","            text-shadow: 0 0 10px rgba(78, 205, 196, 0.5);\n","        }\n","        \n","        .stat-item {\n","            display: flex;\n","            justify-content: space-between;\n","            margin: 12px 0;\n","            padding: 8px 12px;\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            border-left: 3px solid #4ecdc4;\n","            transition: all 0.3s ease;\n","        }\n","        \n","        .stat-item:hover {\n","            background: rgba(255, 255, 255, 0.1);\n","            transform: translateX(5px);\n","        }\n","        \n","        .stat-label {\n","            font-weight: 500;\n","            color: #ecf0f1;\n","        }\n","        \n","        .stat-value {\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","            text-shadow: 0 0 5px rgba(78, 205, 196, 0.3);\n","        }\n","        \n","        .progress-bar {\n","            width: 100%;\n","            height: 8px;\n","            background: rgba(255, 255, 255, 0.1);\n","            border-radius: 4px;\n","            overflow: hidden;\n","            margin: 5px 0;\n","        }\n","        \n","        .progress-fill {\n","            height: 100%;\n","            background: linear-gradient(90deg, #4ecdc4, #45b7d1);\n","            border-radius: 4px;\n","            transition: width 0.5s ease;\n","            position: relative;\n","        }\n","        \n","        .progress-fill::after {\n","            content: '';\n","            position: absolute;\n","            top: 0;\n","            left: -100%;\n","            width: 100%;\n","            height: 100%;\n","            background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.5), transparent);\n","            animation: progress-shine 2s infinite;\n","        }\n","        \n","        @keyframes progress-shine {\n","            0% { left: -100%; }\n","            100% { left: 100%; }\n","        }\n","        \n","        .legend {\n","            background: linear-gradient(145deg, #1a252f, #2c3e50);\n","            padding: 15px;\n","            border-radius: 10px;\n","            margin-top: 20px;\n","            font-size: 0.9em;\n","            line-height: 1.6;\n","        }\n","        \n","        .legend-title {\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 700;\n","            color: #ffeaa7;\n","            margin-bottom: 10px;\n","        }\n","        \n","        .legend-item {\n","            margin: 5px 0;\n","            display: flex;\n","            align-items: center;\n","        }\n","        \n","        .legend-icon {\n","            font-size: 1.2em;\n","            margin-right: 8px;\n","            min-width: 20px;\n","        }\n","        \n","        .performance-indicators {\n","            display: grid;\n","            grid-template-columns: 1fr 1fr;\n","            gap: 10px;\n","            margin: 15px 0;\n","        }\n","        \n","        .indicator {\n","            text-align: center;\n","            padding: 10px;\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            border: 1px solid rgba(78, 205, 196, 0.3);\n","        }\n","        \n","        .indicator-value {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.2em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","        }\n","        \n","        .indicator-label {\n","            font-size: 0.8em;\n","            color: #bdc3c7;\n","            margin-top: 5px;\n","        }\n","        \n","        @media (max-width: 1200px) {\n","            .container {\n","                flex-direction: column;\n","                max-width: 100%;\n","            }\n","            \n","            .stats-panel {\n","                width: 100%;\n","                max-width: 600px;\n","                margin: 0 auto;\n","            }\n","            \n","            .header {\n","                font-size: 2em;\n","            }\n","        }\n","    </style>\n","</head>\n","<body>\n","    <div class=\"header\">\n","         ENHANCED GRLM WORLD \n","    </div>\n","    \n","    <div class=\"container\">\n","        <div class=\"world-section\">\n","            <div class=\"canvas-container\">\n","                <canvas id=\"worldCanvas\" width=\"600\" height=\"600\"></canvas>\n","            </div>\n","            <div class=\"controls\">\n","                <div class=\"control-grid\">\n","                    <div class=\"control-btn empty\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(0, -1)\"></div>\n","                    <div class=\"control-btn empty\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(-1, 0)\"></div>\n","                    <div class=\"control-btn reset\" onclick=\"resetWorld()\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(1, 0)\"></div>\n","                    <div class=\"control-btn empty\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(0, 1)\"></div>\n","                    <div class=\"control-btn empty\"></div>\n","                </div>\n","            </div>\n","        </div>\n","        \n","        <div class=\"stats-panel\">\n","            <div class=\"stats-header\"> AI METRICS</div>\n","            <div class=\"stats\" id=\"stats\">\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Initializing...</span>\n","                </div>\n","            </div>\n","            \n","            <div class=\"performance-indicators\">\n","                <div class=\"indicator\">\n","                    <div class=\"indicator-value\" id=\"confidence-indicator\">--</div>\n","                    <div class=\"indicator-label\">Confidence</div>\n","                </div>\n","                <div class=\"indicator\">\n","                    <div class=\"indicator-value\" id=\"learning-indicator\">--</div>\n","                    <div class=\"indicator-label\">Learning</div>\n","                </div>\n","            </div>\n","            \n","            <div class=\"legend\">\n","                <div class=\"legend-title\"> CONTROLS & LEGEND</div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>WASD or Arrow Buttons to Move</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>AI Agent (You)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Walls (Blocked, -1 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Food (+1 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Water (+0.3 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Treasure (+2 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Particles show AI learning</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Trail shows exploration path</span>\n","                </div>\n","            </div>\n","        </div>\n","    </div>\n","\n","    <script>\n","        const canvas = document.getElementById('worldCanvas');\n","        const ctx = canvas.getContext('2d');\n","        const gridSize = 24;\n","        \n","        let worldState = null;\n","        let grlmStats = null;\n","        let animationFrame = null;\n","        let particles = [];\n","        \n","        // Enhanced color palette\n","        const colors = {\n","            empty: '#0a0a0f',\n","            wall: '#2c3e50',\n","            food: '#e74c3c',\n","            water: '#3498db', \n","            treasure: '#f39c12',\n","            agent: '#2ecc71',\n","            memory: '#9b59b6',\n","            trail: '#1abc9c',\n","            particle: '#ffffff'\n","        };\n","        \n","        function createParticle(x, y, color, velocity = 1) {\n","            return {\n","                x: x * gridSize + gridSize / 2,\n","                y: y * gridSize + gridSize / 2,\n","                vx: (Math.random() - 0.5) * velocity * 2,\n","                vy: (Math.random() - 0.5) * velocity * 2,\n","                life: 1.0,\n","                maxLife: 1.0,\n","                color: color,\n","                size: Math.random() * 3 + 2\n","            };\n","        }\n","        \n","        function updateParticles() {\n","            for (let i = particles.length - 1; i >= 0; i--) {\n","                const p = particles[i];\n","                p.x += p.vx;\n","                p.y += p.vy;\n","                p.life -= 0.02;\n","                p.vy += 0.1; // gravity\n","                p.vx *= 0.98; // air resistance\n","                \n","                if (p.life <= 0) {\n","                    particles.splice(i, 1);\n","                }\n","            }\n","        }\n","        \n","        function drawParticles() {\n","            particles.forEach(p => {\n","                const alpha = p.life / p.maxLife;\n","                ctx.save();\n","                ctx.globalAlpha = alpha;\n","                ctx.fillStyle = p.color;\n","                ctx.beginPath();\n","                ctx.arc(p.x, p.y, p.size * alpha, 0, 2 * Math.PI);\n","                ctx.fill();\n","                ctx.restore();\n","            });\n","        }\n","        \n","        function drawGradientBackground() {\n","            const gradient = ctx.createRadialGradient(\n","                canvas.width / 2, canvas.height / 2, 0,\n","                canvas.width / 2, canvas.height / 2, Math.max(canvas.width, canvas.height) / 2\n","            );\n","            gradient.addColorStop(0, '#1a1a2e');\n","            gradient.addColorStop(1, '#0f0f23');\n","            \n","            ctx.fillStyle = gradient;\n","            ctx.fillRect(0, 0, canvas.width, canvas.height);\n","        }\n","        \n","        function drawGrid() {\n","            if (!worldState) return;\n","            \n","            const grid = worldState.grid;\n","            const size = worldState.size;\n","            \n","            for (let x = 0; x < size; x++) {\n","                for (let y = 0; y < size; y++) {\n","                    const pixelX = x * gridSize;\n","                    const pixelY = y * gridSize;\n","                    const gridValue = grid[x][y];\n","                    \n","                    // Draw cell background with subtle glow\n","                    let alpha = 0.1;\n","                    let color = colors.empty;\n","                    \n","                    if (gridValue === -1) {\n","                        color = colors.wall;\n","                        alpha = 0.8;\n","                    } else if (gridValue > 0) {\n","                        alpha = gridValue * 0.3;\n","                        color = '#2ecc71';\n","                    }\n","                    \n","                    ctx.fillStyle = color;\n","                    ctx.globalAlpha = alpha;\n","                    ctx.fillRect(pixelX, pixelY, gridSize, gridSize);\n","                    ctx.globalAlpha = 1;\n","                    \n","                    // Grid lines with glow effect\n","                    ctx.strokeStyle = 'rgba(78, 205, 196, 0.2)';\n","                    ctx.lineWidth = 0.5;\n","                    ctx.strokeRect(pixelX, pixelY, gridSize, gridSize);\n","                }\n","            }\n","        }\n","        \n","        function drawObjects() {\n","            if (!worldState || !worldState.objects) return;\n","            \n","            const objects = worldState.objects;\n","            const time = Date.now() * 0.001;\n","            \n","            for (const [key, objType] of Object.entries(objects)) {\n","                const [x, y] = key.split(',').map(Number);\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","                \n","                let emoji = '';\n","                let glowColor = '';\n","                let glowIntensity = 0;\n","                \n","                switch (objType) {\n","                    case 'wall':\n","                        emoji = '';\n","                        break;\n","                    case 'food':\n","                        emoji = '';\n","                        glowColor = colors.food;\n","                        glowIntensity = 0.3;\n","                        break;\n","                    case 'water':\n","                        emoji = '';\n","                        glowColor = colors.water;\n","                        glowIntensity = 0.2 + 0.1 * Math.sin(time * 2);\n","                        break;\n","                    case 'treasure':\n","                        emoji = '';\n","                        glowColor = colors.treasure;\n","                        glowIntensity = 0.4 + 0.2 * Math.sin(time * 3);\n","                        break;\n","                }\n","                \n","                // Draw glow effect\n","                if (glowIntensity > 0) {\n","                    const gradient = ctx.createRadialGradient(\n","                        pixelX + gridSize/2, pixelY + gridSize/2, 0,\n","                        pixelX + gridSize/2, pixelY + gridSize/2, gridSize\n","                    );\n","                    gradient.addColorStop(0, glowColor + Math.floor(glowIntensity * 255).toString(16).padStart(2, '0'));\n","                    gradient.addColorStop(1, 'transparent');\n","                    \n","                    ctx.fillStyle = gradient;\n","                    ctx.fillRect(pixelX - 5, pixelY - 5, gridSize + 10, gridSize + 10);\n","                }\n","                \n","                // Draw object\n","                if (emoji) {\n","                    ctx.font = `${gridSize * 0.7}px Arial`;\n","                    ctx.textAlign = 'center';\n","                    ctx.textBaseline = 'middle';\n","                    ctx.fillStyle = '#ffffff';\n","                    ctx.fillText(emoji, pixelX + gridSize/2, pixelY + gridSize/2);\n","                }\n","            }\n","        }\n","        \n","        function drawTrail() {\n","            if (!worldState || !worldState.trail) return;\n","            \n","            const trail = worldState.trail;\n","            ctx.strokeStyle = colors.trail;\n","            ctx.lineWidth = 2;\n","            ctx.lineCap = 'round';\n","            ctx.lineJoin = 'round';\n","            \n","            if (trail.length > 1) {\n","                ctx.beginPath();\n","                for (let i = 0; i < trail.length; i++) {\n","                    const alpha = (i + 1) / trail.length * 0.5;\n","                    ctx.globalAlpha = alpha;\n","                    \n","                    const x = trail[i][0] * gridSize + gridSize / 2;\n","                    const y = trail[i][1] * gridSize + gridSize / 2;\n","                    \n","                    if (i === 0) {\n","                        ctx.moveTo(x, y);\n","                    } else {\n","                        ctx.lineTo(x, y);\n","                    }\n","                }\n","                ctx.stroke();\n","                ctx.globalAlpha = 1;\n","            }\n","        }\n","        \n","        function drawAgent() {\n","            if (!worldState) return;\n","            \n","            const agentPos = worldState.agent_position;\n","            const agentX = agentPos[0] * gridSize;\n","            const agentY = agentPos[1] * gridSize;\n","            \n","            // Agent glow\n","            const gradient = ctx.createRadialGradient(\n","                agentX + gridSize/2, agentY + gridSize/2, 0,\n","                agentX + gridSize/2, agentY + gridSize/2, gridSize * 0.8\n","            );\n","            gradient.addColorStop(0, colors.agent + '80');\n","            gradient.addColorStop(1, 'transparent');\n","            \n","            ctx.fillStyle = gradient;\n","            ctx.fillRect(agentX - 5, agentY - 5, gridSize + 10, gridSize + 10);\n","            \n","            // Agent body\n","            ctx.font = `${gridSize * 0.8}px Arial`;\n","            ctx.textAlign = 'center';\n","            ctx.textBaseline = 'middle';\n","            ctx.fillStyle = '#ffffff';\n","            ctx.fillText('', agentX + gridSize/2, agentY + gridSize/2);\n","            \n","            // Agent border with pulse effect\n","            const time = Date.now() * 0.003;\n","            const pulse = 0.8 + 0.2 * Math.sin(time);\n","            ctx.strokeStyle = colors.agent;\n","            ctx.lineWidth = 3 * pulse;\n","            ctx.strokeRect(agentX + 2, agentY + 2, gridSize - 4, gridSize - 4);\n","        }\n","        \n","        function drawMemoryHeatmap() {\n","            if (!grlmStats || !grlmStats.memory_heatmap) return;\n","            \n","            const heatmap = grlmStats.memory_heatmap;\n","            \n","            for (const [key, data] of Object.entries(heatmap)) {\n","                const [x, y] = key.split(',').map(Number);\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","                \n","                const intensity = Math.min(1, data.importance * 0.5);\n","                const emotion = data.emotion;\n","                \n","                // Color based on emotional valence\n","                let color = colors.memory;\n","                if (emotion > 0.5) color = '#2ecc71'; // positive\n","                else if (emotion < -0.5) color = '#e74c3c'; // negative\n","                \n","                ctx.fillStyle = color + Math.floor(intensity * 100).toString(16).padStart(2, '0');\n","                ctx.beginPath();\n","                ctx.arc(pixelX + gridSize/2, pixelY + gridSize/2, intensity * 8, 0, 2 * Math.PI);\n","                ctx.fill();\n","            }\n","        }\n","        \n","        function drawWorld() {\n","            ctx.clearRect(0, 0, canvas.width, canvas.height);\n","            \n","            drawGradientBackground();\n","            drawGrid();\n","            drawMemoryHeatmap();\n","            drawObjects();\n","            drawTrail();\n","            drawAgent();\n","            drawParticles();\n","        }\n","        \n","        function animate() {\n","            updateParticles();\n","            drawWorld();\n","            animationFrame = requestAnimationFrame(animate);\n","        }\n","        \n","        function updateStats() {\n","            if (!grlmStats) return;\n","            \n","            const statsDiv = document.getElementById('stats');\n","            const confidenceIndicator = document.getElementById('confidence-indicator');\n","            const learningIndicator = document.getElementById('learning-indicator');\n","            \n","            // Update main stats\n","            statsDiv.innerHTML = `\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Steps Taken</span>\n","                    <span class=\"stat-value\">${grlmStats.total_steps}</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Total Reward</span>\n","                    <span class=\"stat-value\">${grlmStats.total_reward.toFixed(2)}</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Efficiency</span>\n","                    <span class=\"stat-value\">${grlmStats.avg_reward_per_step.toFixed(3)}</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Prediction Error</span>\n","                    <span class=\"stat-value\">${(grlmStats.recent_prediction_error * 1000).toFixed(2)}ms</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Exploration</span>\n","                    <span class=\"stat-value\">${grlmStats.exploration_progress.toFixed(1)}%</span>\n","                </div>\n","                <div class=\"progress-bar\">\n","                    <div class=\"progress-fill\" style=\"width: ${grlmStats.exploration_progress}%\"></div>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Memory Efficiency</span>\n","                    <span class=\"stat-value\">${grlmStats.memory_efficiency.toFixed(1)}%</span>\n","                </div>\n","                <div class=\"progress-bar\">\n","                    <div class=\"progress-fill\" style=\"width: ${grlmStats.memory_efficiency}%\"></div>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Emotional Balance</span>\n","                    <span class=\"stat-value\">${grlmStats.emotional_balance.toFixed(2)}</span>\n","                </div>\n","            `;\n","            \n","            // Update indicators\n","            confidenceIndicator.textContent = `${(grlmStats.avg_confidence * 100).toFixed(0)}%`;\n","            learningIndicator.textContent = `${(grlmStats.learning_stability * 100).toFixed(0)}%`;\n","        }\n","        \n","        function makeMove(dx, dy) {\n","            console.log(`Enhanced move: dx=${dx}, dy=${dy}`);\n","            \n","            if (worldState) {\n","                const oldX = worldState.agent_position[0];\n","                const oldY = worldState.agent_position[1];\n","                const newX = Math.max(0, Math.min(worldState.size - 1, oldX + dx));\n","                const newY = Math.max(0, Math.min(worldState.size - 1, oldY + dy));\n","                \n","                // Enhanced collision detection\n","                let canMove = true;\n","                const gridValue = worldState.grid[newX][newY];\n","                const objKey = `${newX},${newY}`;\n","                \n","                if (gridValue === -1 || (worldState.objects[objKey] === 'wall')) {\n","                    canMove = false;\n","                }\n","                \n","                let reward = 0;\n","                if (canMove && (newX !== oldX || newY !== oldY)) {\n","                    // Valid move\n","                    worldState.agent_position = [newX, newY];\n","                    reward = gridValue;\n","                    \n","                    // Add to trail\n","                    if (!worldState.trail) worldState.trail = [];\n","                    worldState.trail.push([oldX, oldY]);\n","                    if (worldState.trail.length > 15) {\n","                        worldState.trail.shift();\n","                    }\n","                    \n","                    // Object interactions with effects\n","                    if (worldState.objects[objKey]) {\n","                        const objType = worldState.objects[objKey];\n","                        \n","                        // Add particles\n","                        for (let i = 0; i < 5; i++) {\n","                            let particleColor = colors.particle;\n","                            switch (objType) {\n","                                case 'treasure': particleColor = colors.treasure; reward += 2.0; break;\n","                                case 'food': particleColor = colors.food; reward += 1.0; break;\n","                                case 'water': particleColor = colors.water; reward += 0.3; break;\n","                            }\n","                            particles.push(createParticle(newX, newY, particleColor, 2));\n","                        }\n","                        \n","                        // Remove consumed objects\n","                        if (objType !== 'wall') {\n","                            delete worldState.objects[objKey];\n","                            worldState.grid[newX][newY] = 0.0;\n","                        }\n","                    }\n","                    \n","                    // Movement particles\n","                    for (let i = 0; i < 3; i++) {\n","                        particles.push(createParticle(newX, newY, colors.agent, 1));\n","                    }\n","                    \n","                } else if (!canMove) {\n","                    reward = -1.0;\n","                    // Wall hit particles\n","                    for (let i = 0; i < 8; i++) {\n","                        particles.push(createParticle(newX, newY, colors.food, 1.5));\n","                    }\n","                }\n","                \n","                // Enhanced stats simulation\n","                if (grlmStats) {\n","                    grlmStats.total_steps += 1;\n","                    grlmStats.total_reward += reward;\n","                    grlmStats.avg_reward_per_step = grlmStats.total_reward / grlmStats.total_steps;\n","                    grlmStats.recent_prediction_error = Math.max(0.001, grlmStats.recent_prediction_error * 0.95 + Math.random() * 0.01);\n","                    grlmStats.avg_confidence = Math.min(1.0, grlmStats.avg_confidence + 0.01);\n","                    grlmStats.learning_stability = Math.min(1.0, grlmStats.learning_stability + 0.005);\n","                    \n","                    // Update exploration\n","                    if (canMove) {\n","                        const key = `${newX},${newY}`;\n","                        if (!grlmStats.memory_heatmap[key]) {\n","                            grlmStats.memory_heatmap[key] = {\n","                                importance: Math.random(),\n","                                emotion: reward,\n","                                visits: 1\n","                            };\n","                        } else {\n","                            grlmStats.memory_heatmap[key].visits += 1;\n","                            grlmStats.memory_heatmap[key].emotion = (grlmStats.memory_heatmap[key].emotion + reward) / 2;\n","                        }\n","                        \n","                        const exploredCells = Object.keys(grlmStats.memory_heatmap).length;\n","                        grlmStats.exploration_progress = (exploredCells / (worldState.size * worldState.size)) * 100;\n","                        grlmStats.memory_efficiency = Math.min(100, exploredCells / grlmStats.total_steps * 100);\n","                    }\n","                    \n","                    grlmStats.emotional_balance = (grlmStats.emotional_balance * 0.9 + reward * 0.1);\n","                }\n","                \n","                updateStats();\n","            }\n","        }\n","        \n","        function resetWorld() {\n","            initEnhancedWorld();\n","            particles = [];\n","        }\n","        \n","        // Keyboard controls\n","        document.addEventListener('keydown', (e) => {\n","            switch(e.key.toLowerCase()) {\n","                case 'w': case 'arrowup': makeMove(0, -1); break;\n","                case 's': case 'arrowdown': makeMove(0, 1); break;\n","                case 'a': case 'arrowleft': makeMove(-1, 0); break;\n","                case 'd': case 'arrowright': makeMove(1, 0); break;\n","                case 'r': resetWorld(); break;\n","            }\n","            e.preventDefault();\n","        });\n","        \n","        function initEnhancedWorld() {\n","            const size = 25;\n","            worldState = {\n","                grid: Array(size).fill().map(() => Array(size).fill(0)),\n","                objects: {},\n","                agent_position: [Math.floor(size/2), Math.floor(size/2)],\n","                trail: [],\n","                size: size\n","            };\n","            \n","            // Generate enhanced world\n","            const rooms = [\n","                {x: 3, y: 3, w: 6, h: 4, treasure: true},\n","                {x: 15, y: 8, w: 7, h: 5, treasure: false},\n","                {x: 5, y: 18, w: 5, h: 4, treasure: false}\n","            ];\n","            \n","            rooms.forEach(room => {\n","                // Room walls\n","                for (let x = room.x; x < room.x + room.w; x++) {\n","                    [room.y, room.y + room.h - 1].forEach(y => {\n","                        if (x < size && y < size) {\n","                            worldState.objects[`${x},${y}`] = 'wall';\n","                            worldState.grid[x][y] = -1;\n","                        }\n","                    });\n","                }\n","                for (let y = room.y; y < room.y + room.h; y++) {\n","                    [room.x, room.x + room.w - 1].forEach(x => {\n","                        if (x < size && y < size) {\n","                            worldState.objects[`${x},${y}`] = 'wall';\n","                            worldState.grid[x][y] = -1;\n","                        }\n","                    });\n","                }\n","                \n","                // Room contents\n","                const centerX = room.x + Math.floor(room.w / 2);\n","                const centerY = room.y + Math.floor(room.h / 2);\n","                if (centerX < size && centerY < size) {\n","                    const objType = room.treasure ? 'treasure' : Math.random() > 0.5 ? 'food' : 'water';\n","                    worldState.objects[`${centerX},${centerY}`] = objType;\n","                    worldState.grid[centerX][centerY] = objType === 'treasure' ? 1.0 : (objType === 'food' ? 0.5 : 0.3);\n","                }\n","            });\n","            \n","            // Scattered objects\n","            for (let i = 0; i < size * 1.5; i++) {\n","                const x = Math.floor(Math.random() * size);\n","                const y = Math.floor(Math.random() * size);\n","                const key = `${x},${y}`;\n","                \n","                if (!worldState.objects[key] && !(x === worldState.agent_position[0] && y === worldState.agent_position[1])) {\n","                    const objType = Math.random() > 0.7 ? 'treasure' : (Math.random() > 0.5 ? 'food' : 'water');\n","                    worldState.objects[key] = objType;\n","                    worldState.grid[x][y] = objType === 'treasure' ? 1.0 : (objType === 'food' ? 0.5 : 0.3);\n","                }\n","            }\n","            \n","            grlmStats = {\n","                total_steps: 0,\n","                total_reward: 0,\n","                avg_reward_per_step: 0,\n","                recent_prediction_error: 0.05,\n","                avg_confidence: 0.5,\n","                learning_stability: 0.5,\n","                exploration_progress: 0,\n","                memory_efficiency: 0,\n","                emotional_balance: 0,\n","                memory_heatmap: {}\n","            };\n","            \n","            updateStats();\n","        }\n","        \n","        // Initialize and start\n","        initEnhancedWorld();\n","        animate();\n","        \n","        // Focus for keyboard\n","        canvas.focus();\n","        canvas.setAttribute('tabindex', '0');\n","        \n","        console.log(' Enhanced GRLM World initialized with visual effects!');\n","    </script>\n","</body>\n","</html>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","# Interactive 2D GRLM World - Visually Enhanced Version\n","# Enhanced with animations, particles, and compelling visuals\n","\n","import os\n","import math\n","import time\n","import json\n","import random\n","import tempfile\n","import warnings\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Optional, Tuple, Dict, List\n","import base64\n","from IPython.display import HTML, display\n","import threading\n","import queue\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# ============================================================================\n","# PERFORMANCE OPTIMIZATIONS & SETUP\n","# ============================================================================\n","\n","if torch.cuda.is_available():\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    try:\n","        torch.set_float32_matmul_precision(\"high\")\n","    except AttributeError:\n","        pass\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","USE_AMP = torch.cuda.is_available()\n","AMP_DTYPE = torch.bfloat16\n","\n","# Enhanced configuration for visual appeal\n","SEED = 1337\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","WORLD_SIZE = 25  # Larger world for more exploration\n","GRID_SIZE = 24   # Optimized for visual clarity\n","EMB_DIM = 128\n","MAX_NODES = 15000\n","\n","print(f\"Enhanced Visual 2D GRLM World Starting\")\n","print(f\"Device: {DEVICE}\")\n","print(f\"World size: {WORLD_SIZE}x{WORLD_SIZE}\")\n","\n","# ============================================================================\n","# ENHANCED MEMORY SYSTEM WITH VISUAL TRACKING\n","# ============================================================================\n","\n","class VisualGraphMemory:\n","    \"\"\"Enhanced memory system with visual importance tracking.\"\"\"\n","\n","    def __init__(self, dim: int, max_nodes: int = MAX_NODES):\n","        self.dim = dim\n","        self.max_nodes = max_nodes\n","        self.nodes = []\n","        self.positions = []\n","        self.timestamps = []\n","        self.importance = []\n","        self.visit_counts = []  # Track how often locations are visited\n","        self.emotional_valence = []  # Track positive/negative experiences\n","\n","    def add_experience(self, embedding: np.ndarray, position: Tuple[float, float],\n","                      importance: float = 1.0, reward: float = 0.0):\n","        \"\"\"Add experience with enhanced tracking.\"\"\"\n","        # Check if we've been near this position before\n","        existing_idx = None\n","        for i, pos in enumerate(self.positions):\n","            if abs(pos[0] - position[0]) < 1.0 and abs(pos[1] - position[1]) < 1.0:\n","                existing_idx = i\n","                break\n","\n","        if existing_idx is not None:\n","            # Update existing memory\n","            self.visit_counts[existing_idx] += 1\n","            self.importance[existing_idx] = (self.importance[existing_idx] + importance) / 2\n","            self.emotional_valence[existing_idx] = (self.emotional_valence[existing_idx] + reward) / 2\n","            self.timestamps[existing_idx] = time.time()\n","        else:\n","            # Add new memory\n","            self.nodes.append(embedding.astype(np.float32))\n","            self.positions.append(position)\n","            self.timestamps.append(time.time())\n","            self.importance.append(importance)\n","            self.visit_counts.append(1)\n","            self.emotional_valence.append(reward)\n","\n","        self._trim_if_needed()\n","\n","    def _trim_if_needed(self):\n","        \"\"\"Smart trimming based on multiple factors.\"\"\"\n","        if len(self.nodes) <= self.max_nodes:\n","            return\n","\n","        current_time = time.time()\n","        scores = []\n","\n","        for i in range(len(self.nodes)):\n","            age = current_time - self.timestamps[i]\n","            recency = np.exp(-age / 1800)  # 30-minute decay\n","            visit_importance = np.log(1 + self.visit_counts[i])\n","            emotional_importance = abs(self.emotional_valence[i])\n","\n","            composite_score = (\n","                0.3 * self.importance[i] +\n","                0.3 * recency +\n","                0.2 * visit_importance +\n","                0.2 * emotional_importance\n","            )\n","            scores.append((composite_score, i))\n","\n","        # Keep top memories\n","        scores.sort(reverse=True)\n","        keep_indices = [idx for _, idx in scores[:self.max_nodes // 2]]\n","\n","        # Update all lists\n","        for attr in ['nodes', 'positions', 'timestamps', 'importance', 'visit_counts', 'emotional_valence']:\n","            old_list = getattr(self, attr)\n","            setattr(self, attr, [old_list[i] for i in keep_indices])\n","\n","    def get_memory_heatmap(self) -> Dict[Tuple[int, int], Dict]:\n","        \"\"\"Get heatmap data for visualization.\"\"\"\n","        heatmap = {}\n","        for pos, importance, visits, emotion in zip(\n","            self.positions, self.importance, self.visit_counts, self.emotional_valence\n","        ):\n","            grid_pos = (int(pos[0]), int(pos[1]))\n","            if grid_pos not in heatmap:\n","                heatmap[grid_pos] = {\n","                    'importance': 0,\n","                    'visits': 0,\n","                    'emotion': 0,\n","                    'count': 0\n","                }\n","\n","            heatmap[grid_pos]['importance'] += importance\n","            heatmap[grid_pos]['visits'] += visits\n","            heatmap[grid_pos]['emotion'] += emotion\n","            heatmap[grid_pos]['count'] += 1\n","\n","        # Average the values\n","        for data in heatmap.values():\n","            count = data['count']\n","            data['importance'] /= count\n","            data['visits'] /= count\n","            data['emotion'] /= count\n","\n","        return heatmap\n","\n","    def get_stats(self) -> Dict:\n","        \"\"\"Enhanced statistics.\"\"\"\n","        if not self.nodes:\n","            return {'total_memories': 0, 'avg_importance': 0, 'coverage': 0, 'emotional_balance': 0}\n","\n","        return {\n","            'total_memories': len(self.nodes),\n","            'avg_importance': float(np.mean(self.importance)),\n","            'coverage': len(set(self.positions)) / (WORLD_SIZE * WORLD_SIZE),\n","            'emotional_balance': float(np.mean(self.emotional_valence)),\n","            'max_visits': max(self.visit_counts) if self.visit_counts else 0,\n","            'memory_efficiency': len(set(self.positions)) / len(self.positions)\n","        }\n","\n","# ============================================================================\n","# ENHANCED WORLD MODEL WITH PREDICTION CONFIDENCE\n","# ============================================================================\n","\n","class ConfidenceEncoder(nn.Module):\n","    \"\"\"Encoder that also outputs prediction confidence.\"\"\"\n","\n","    def __init__(self, input_dim: int, emb_dim: int):\n","        super().__init__()\n","        self.feature_net = nn.Sequential(\n","            nn.Linear(input_dim, emb_dim),\n","            nn.ReLU(),\n","            nn.Linear(emb_dim, emb_dim),\n","            nn.ReLU(),\n","            nn.Linear(emb_dim, emb_dim)\n","        )\n","        self.confidence_net = nn.Sequential(\n","            nn.Linear(emb_dim, emb_dim // 2),\n","            nn.ReLU(),\n","            nn.Linear(emb_dim // 2, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        features = self.feature_net(x)\n","        normalized_features = F.normalize(features, dim=-1)\n","        confidence = self.confidence_net(features)\n","        return normalized_features, confidence\n","\n","class EnhancedMovementPredictor(nn.Module):\n","    \"\"\"Movement predictor with uncertainty estimation.\"\"\"\n","\n","    def __init__(self, emb_dim: int):\n","        super().__init__()\n","        self.predictor = nn.Sequential(\n","            nn.Linear(emb_dim + 2, emb_dim * 2),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(emb_dim * 2, emb_dim),\n","            nn.ReLU(),\n","            nn.Linear(emb_dim, emb_dim + 3)  # state + position + confidence\n","        )\n","\n","    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        combined = torch.cat([state, action], dim=-1)\n","        output = self.predictor(combined)\n","\n","        next_state = output[..., :-3]\n","        position_change = output[..., -3:-1]\n","        confidence = torch.sigmoid(output[..., -1:])\n","\n","        return next_state, position_change, confidence\n","\n","class VisualWorldModel(nn.Module):\n","    \"\"\"Enhanced world model with visual feedback capabilities.\"\"\"\n","\n","    def __init__(self, emb_dim: int = EMB_DIM):\n","        super().__init__()\n","        self.encoder = ConfidenceEncoder(input_dim=8, emb_dim=emb_dim)  # Enhanced input\n","        self.movement_predictor = EnhancedMovementPredictor(emb_dim)\n","\n","    def encode_state(self, position: Tuple[float, float], local_env: np.ndarray,\n","                    velocity: Tuple[float, float] = (0, 0)) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"Enhanced state encoding with velocity.\"\"\"\n","        state_input = torch.tensor([\n","            position[0] / WORLD_SIZE,\n","            position[1] / WORLD_SIZE,\n","            velocity[0],\n","            velocity[1],\n","            *local_env  # 4 local environment features\n","        ], dtype=torch.float32, device=DEVICE)\n","\n","        return self.encoder(state_input.unsqueeze(0))\n","\n","    def predict_movement(self, current_state: torch.Tensor, movement: Tuple[float, float]\n","                        ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        \"\"\"Enhanced movement prediction with confidence.\"\"\"\n","        action = torch.tensor(movement, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n","        return self.movement_predictor(current_state, action)\n","\n","# ============================================================================\n","# VISUALLY ENHANCED 2D WORLD\n","# ============================================================================\n","\n","class EnhancedWorld2D:\n","    \"\"\"2D world with enhanced visual features and dynamic elements.\"\"\"\n","\n","    def __init__(self, size: int = WORLD_SIZE):\n","        self.size = size\n","        self.grid = np.zeros((size, size), dtype=np.float32)\n","        self.objects = {}\n","        self.agent_position = (size // 2, size // 2)\n","        self.agent_velocity = (0, 0)\n","        self.trail = []  # Agent movement trail\n","        self.particles = []  # Visual effect particles\n","\n","        # Dynamic elements\n","        self.treasure_glow = 0.0\n","        self.water_flow = 0.0\n","        self.time_elapsed = 0.0\n","\n","        self._generate_enhanced_world()\n","\n","    def _generate_enhanced_world(self):\n","        \"\"\"Generate visually interesting world with patterns.\"\"\"\n","        np.random.seed(SEED)\n","\n","        # Create room-like structures\n","        for room in range(3):\n","            room_x = np.random.randint(2, self.size - 6)\n","            room_y = np.random.randint(2, self.size - 6)\n","            room_w = np.random.randint(4, 8)\n","            room_h = np.random.randint(4, 8)\n","\n","            # Room walls\n","            for x in range(room_x, min(room_x + room_w, self.size)):\n","                for y in [room_y, min(room_y + room_h - 1, self.size - 1)]:\n","                    if 0 <= y < self.size:\n","                        self.objects[(x, y)] = 'wall'\n","                        self.grid[x, y] = -1.0\n","\n","            for y in range(room_y, min(room_y + room_h, self.size)):\n","                for x in [room_x, min(room_x + room_w - 1, self.size - 1)]:\n","                    if 0 <= x < self.size:\n","                        self.objects[(x, y)] = 'wall'\n","                        self.grid[x, y] = -1.0\n","\n","            # Room contents\n","            center_x, center_y = room_x + room_w // 2, room_y + room_h // 2\n","            if 0 <= center_x < self.size and 0 <= center_y < self.size:\n","                obj_type = np.random.choice(['treasure', 'food', 'water'])\n","                self.objects[(center_x, center_y)] = obj_type\n","                if obj_type == 'treasure':\n","                    self.grid[center_x, center_y] = 1.0\n","                elif obj_type == 'food':\n","                    self.grid[center_x, center_y] = 0.5\n","                elif obj_type == 'water':\n","                    self.grid[center_x, center_y] = 0.3\n","\n","        # Add scattered objects\n","        for _ in range(self.size):\n","            x, y = np.random.randint(0, self.size, 2)\n","            if (x, y) not in self.objects and (x, y) != self.agent_position:\n","                obj_type = np.random.choice(['food', 'water', 'treasure'], p=[0.5, 0.3, 0.2])\n","                self.objects[(x, y)] = obj_type\n","                if obj_type == 'treasure':\n","                    self.grid[x, y] = 1.0\n","                elif obj_type == 'food':\n","                    self.grid[x, y] = 0.5\n","                elif obj_type == 'water':\n","                    self.grid[x, y] = 0.3\n","\n","    def update_dynamics(self, dt: float = 0.1):\n","        \"\"\"Update dynamic visual elements.\"\"\"\n","        self.time_elapsed += dt\n","        self.treasure_glow = 0.5 + 0.3 * math.sin(self.time_elapsed * 2)\n","        self.water_flow = self.time_elapsed * 0.5\n","\n","        # Update particles\n","        self.particles = [p for p in self.particles if p['life'] > 0]\n","        for particle in self.particles:\n","            particle['x'] += particle['vx'] * dt\n","            particle['y'] += particle['vy'] * dt\n","            particle['life'] -= dt\n","\n","    def add_particle_effect(self, x: float, y: float, effect_type: str):\n","        \"\"\"Add visual particle effect.\"\"\"\n","        colors = {\n","            'treasure': '#ffd43b',\n","            'food': '#ff6b6b',\n","            'water': '#4dabf7',\n","            'movement': '#51cf66',\n","            'wall_hit': '#ff8787'\n","        }\n","\n","        for _ in range(5):\n","            self.particles.append({\n","                'x': x,\n","                'y': y,\n","                'vx': (np.random.random() - 0.5) * 2,\n","                'vy': (np.random.random() - 0.5) * 2,\n","                'color': colors.get(effect_type, '#ffffff'),\n","                'life': 1.0,\n","                'max_life': 1.0\n","            })\n","\n","    def move_agent(self, dx: int, dy: int) -> Tuple[Tuple[int, int], float]:\n","        \"\"\"Enhanced movement with visual effects.\"\"\"\n","        old_x, old_y = self.agent_position\n","        new_x = max(0, min(self.size - 1, old_x + dx))\n","        new_y = max(0, min(self.size - 1, old_y + dy))\n","        new_pos = (new_x, new_y)\n","\n","        # Update velocity for visual effects\n","        self.agent_velocity = (new_x - old_x, new_y - old_y)\n","\n","        if self.is_valid_position(new_pos):\n","            # Valid move\n","            self.agent_position = new_pos\n","\n","            # Add to trail\n","            self.trail.append((old_x, old_y))\n","            if len(self.trail) > 20:  # Keep trail length manageable\n","                self.trail.pop(0)\n","\n","            # Calculate reward\n","            reward = self.grid[new_x, new_y]\n","\n","            # Special interactions with visual effects\n","            if (new_x, new_y) in self.objects:\n","                obj_type = self.objects[(new_x, new_y)]\n","                self.add_particle_effect(new_x, new_y, obj_type)\n","\n","                if obj_type == 'treasure':\n","                    reward += 2.0\n","                elif obj_type == 'food':\n","                    reward += 1.0\n","                elif obj_type == 'water':\n","                    reward += 0.3\n","\n","                # Remove consumed objects (except walls)\n","                if obj_type != 'wall':\n","                    del self.objects[(new_x, new_y)]\n","                    self.grid[new_x, new_y] = 0.0\n","\n","            self.add_particle_effect(new_x, new_y, 'movement')\n","\n","        else:\n","            # Hit wall - add effect but no movement\n","            self.add_particle_effect(new_x, new_y, 'wall_hit')\n","            reward = -1.0\n","\n","        return self.agent_position, reward\n","\n","    def is_valid_position(self, position: Tuple[int, int]) -> bool:\n","        \"\"\"Check position validity.\"\"\"\n","        x, y = position\n","        if not (0 <= x < self.size and 0 <= y < self.size):\n","            return False\n","        return self.grid[x, y] != -1.0\n","\n","    def get_local_environment(self, position: Tuple[int, int], radius: int = 1) -> np.ndarray:\n","        \"\"\"Enhanced local environment sensing.\"\"\"\n","        x, y = position\n","        features = []\n","\n","        directions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # up, down, left, right\n","        for dx, dy in directions:\n","            nx, ny = x + dx, y + dy\n","            if 0 <= nx < self.size and 0 <= ny < self.size:\n","                features.append(self.grid[nx, ny])\n","            else:\n","                features.append(-2.0)  # Out of bounds marker\n","\n","        return np.array(features)\n","\n","    def get_enhanced_state(self) -> Dict:\n","        \"\"\"Get complete enhanced world state.\"\"\"\n","        return {\n","            'grid': self.grid.tolist(),\n","            'objects': {f\"{x},{y}\": obj_type for (x, y), obj_type in self.objects.items()},\n","            'agent_position': self.agent_position,\n","            'agent_velocity': self.agent_velocity,\n","            'trail': self.trail[-10:],  # Recent trail\n","            'particles': self.particles,\n","            'treasure_glow': self.treasure_glow,\n","            'water_flow': self.water_flow,\n","            'size': self.size\n","        }\n","\n","# ============================================================================\n","# ENHANCED INTERACTIVE SYSTEM\n","# ============================================================================\n","\n","class VisualGRLM:\n","    \"\"\"Enhanced GRLM system with compelling visuals.\"\"\"\n","\n","    def __init__(self):\n","        self.world = EnhancedWorld2D()\n","        self.memory = VisualGraphMemory(EMB_DIM)\n","        self.world_model = VisualWorldModel().to(DEVICE)\n","\n","        self.optimizer = torch.optim.AdamW(self.world_model.parameters(), lr=1e-3)\n","\n","        # Enhanced tracking\n","        self.total_steps = 0\n","        self.total_reward = 0.0\n","        self.prediction_errors = []\n","        self.confidence_history = []\n","        self.reward_history = []\n","\n","        self._initialize_memory()\n","\n","    def _initialize_memory(self):\n","        \"\"\"Initialize with first memory.\"\"\"\n","        pos = self.world.agent_position\n","        local_env = self.world.get_local_environment(pos)\n","        state_emb, confidence = self.world_model.encode_state(pos, local_env)\n","\n","        embedding_np = state_emb.detach().cpu().numpy().flatten()\n","        self.memory.add_experience(embedding_np, pos, float(confidence.item()))\n","\n","    def make_move(self, dx: int, dy: int) -> Dict:\n","        \"\"\"Enhanced move with visual feedback.\"\"\"\n","        # Update world dynamics\n","        self.world.update_dynamics()\n","\n","        # Get current state\n","        old_pos = self.world.agent_position\n","        old_local_env = self.world.get_local_environment(old_pos)\n","        current_state, current_confidence = self.world_model.encode_state(\n","            old_pos, old_local_env, self.world.agent_velocity\n","        )\n","\n","        # Predict movement\n","        pred_state, pred_pos_change, pred_confidence = self.world_model.predict_movement(\n","            current_state, (dx, dy)\n","        )\n","\n","        # Execute movement\n","        new_pos, reward = self.world.move_agent(dx, dy)\n","        new_local_env = self.world.get_local_environment(new_pos)\n","        actual_state, actual_confidence = self.world_model.encode_state(\n","            new_pos, new_local_env, self.world.agent_velocity\n","        )\n","\n","        # Train model\n","        loss = self._enhanced_training_step(\n","            current_state, (dx, dy), actual_state, new_pos, old_pos, reward\n","        )\n","\n","        # Update memory\n","        embedding_np = actual_state.detach().cpu().numpy().flatten()\n","        importance = float(actual_confidence.item()) + abs(reward) * 0.5\n","        self.memory.add_experience(embedding_np, new_pos, importance, reward)\n","\n","        # Update statistics\n","        self.total_steps += 1\n","        self.total_reward += reward\n","        self.confidence_history.append(float(pred_confidence.item()))\n","        self.reward_history.append(reward)\n","\n","        # Keep history manageable\n","        if len(self.confidence_history) > 100:\n","            self.confidence_history = self.confidence_history[-50:]\n","            self.reward_history = self.reward_history[-50:]\n","\n","        return {\n","            'old_position': old_pos,\n","            'new_position': new_pos,\n","            'reward': reward,\n","            'prediction_loss': loss,\n","            'confidence': float(pred_confidence.item()),\n","            'world_state': self.world.get_enhanced_state(),\n","            'memory_heatmap': self.memory.get_memory_heatmap(),\n","            'stats': self.get_enhanced_stats()\n","        }\n","\n","    def _enhanced_training_step(self, current_state, action, target_state,\n","                               new_pos, old_pos, reward):\n","        \"\"\"Enhanced training with multiple loss components.\"\"\"\n","        self.optimizer.zero_grad()\n","\n","        pred_state, pred_pos_change, pred_confidence = self.world_model.predict_movement(\n","            current_state, action\n","        )\n","\n","        # Multiple loss components\n","        state_loss = F.mse_loss(pred_state, target_state)\n","\n","        actual_pos_change = torch.tensor(\n","            [new_pos[0] - old_pos[0], new_pos[1] - old_pos[1]],\n","            dtype=torch.float32, device=DEVICE\n","        ).unsqueeze(0)\n","        pos_loss = F.mse_loss(pred_pos_change, actual_pos_change)\n","\n","        # Confidence loss - higher confidence should correlate with lower error\n","        confidence_target = torch.exp(-state_loss.detach())\n","        confidence_loss = F.mse_loss(pred_confidence.squeeze(), confidence_target)\n","\n","        total_loss = state_loss + 0.5 * pos_loss + 0.2 * confidence_loss\n","\n","        total_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(self.world_model.parameters(), 1.0)\n","        self.optimizer.step()\n","\n","        loss_value = float(total_loss.item())\n","        self.prediction_errors.append(loss_value)\n","        if len(self.prediction_errors) > 100:\n","            self.prediction_errors = self.prediction_errors[-50:]\n","\n","        return loss_value\n","\n","    def get_enhanced_stats(self) -> Dict:\n","        \"\"\"Comprehensive statistics for visualization.\"\"\"\n","        memory_stats = self.memory.get_stats()\n","\n","        return {\n","            'total_steps': self.total_steps,\n","            'total_reward': self.total_reward,\n","            'avg_reward_per_step': self.total_reward / max(1, self.total_steps),\n","            'recent_prediction_error': float(np.mean(self.prediction_errors[-10:])) if self.prediction_errors else 0.0,\n","            'avg_confidence': float(np.mean(self.confidence_history[-10:])) if self.confidence_history else 0.5,\n","            'reward_trend': float(np.mean(self.reward_history[-5:])) if len(self.reward_history) >= 5 else 0.0,\n","            'exploration_progress': memory_stats['coverage'] * 100,\n","            'memory_efficiency': memory_stats.get('memory_efficiency', 0) * 100,\n","            'emotional_balance': memory_stats.get('emotional_balance', 0),\n","            'learning_stability': 1.0 - (np.std(self.prediction_errors[-20:]) if len(self.prediction_errors) >= 20 else 0.5)\n","        }\n","\n","# ============================================================================\n","# VISUALLY COMPELLING HTML INTERFACE\n","# ============================================================================\n","\n","def create_enhanced_html_interface():\n","    \"\"\"Create visually stunning HTML interface.\"\"\"\n","\n","    html_template = \"\"\"\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>Enhanced Interactive 2D GRLM World</title>\n","    <style>\n","        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700;900&family=Roboto:wght@300;400;500&display=swap');\n","\n","        * {\n","            margin: 0;\n","            padding: 0;\n","            box-sizing: border-box;\n","        }\n","\n","        body {\n","            font-family: 'Roboto', sans-serif;\n","            background: linear-gradient(135deg, #0c0c0c 0%, #1a1a2e 50%, #16213e 100%);\n","            color: #ffffff;\n","            overflow-x: hidden;\n","            min-height: 100vh;\n","        }\n","\n","        .header {\n","            text-align: center;\n","            padding: 20px;\n","            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4, #ffeaa7);\n","            background-size: 300% 100%;\n","            animation: gradient-shift 3s ease-in-out infinite;\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 900;\n","            font-size: 2.5em;\n","            text-shadow: 0 0 20px rgba(255, 255, 255, 0.5);\n","            margin-bottom: 20px;\n","        }\n","\n","        @keyframes gradient-shift {\n","            0%, 100% { background-position: 0% 50%; }\n","            50% { background-position: 100% 50%; }\n","        }\n","\n","        .container {\n","            display: flex;\n","            gap: 20px;\n","            padding: 0 20px;\n","            max-width: 1400px;\n","            margin: 0 auto;\n","        }\n","\n","        .world-section {\n","            flex: 1;\n","            background: linear-gradient(145deg, #1e3c72, #2a5298);\n","            border-radius: 20px;\n","            padding: 20px;\n","            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);\n","            position: relative;\n","            overflow: hidden;\n","        }\n","\n","        .world-section::before {\n","            content: '';\n","            position: absolute;\n","            top: -50%;\n","            left: -50%;\n","            width: 200%;\n","            height: 200%;\n","            background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);\n","            animation: rotate 20s linear infinite;\n","            pointer-events: none;\n","        }\n","\n","        @keyframes rotate {\n","            from { transform: rotate(0deg); }\n","            to { transform: rotate(360deg); }\n","        }\n","\n","        .canvas-container {\n","            position: relative;\n","            display: inline-block;\n","            border-radius: 15px;\n","            overflow: hidden;\n","            box-shadow:\n","                0 0 50px rgba(0, 255, 255, 0.3),\n","                inset 0 0 20px rgba(255, 255, 255, 0.1);\n","        }\n","\n","        canvas {\n","            display: block;\n","            background: radial-gradient(circle at center, #0f0f23 0%, #000000 100%);\n","            position: relative;\n","            z-index: 1;\n","        }\n","\n","        .controls {\n","            margin-top: 20px;\n","            text-align: center;\n","            position: relative;\n","            z-index: 2;\n","        }\n","\n","        .control-grid {\n","            display: inline-grid;\n","            grid-template-columns: repeat(3, 1fr);\n","            gap: 8px;\n","            margin: 10px;\n","        }\n","\n","        .control-btn {\n","            background: linear-gradient(145deg, #667eea 0%, #764ba2 100%);\n","            color: white;\n","            border: none;\n","            padding: 15px 20px;\n","            border-radius: 12px;\n","            cursor: pointer;\n","            font-size: 18px;\n","            font-weight: bold;\n","            font-family: 'Orbitron', monospace;\n","            transition: all 0.3s ease;\n","            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.2);\n","            position: relative;\n","            overflow: hidden;\n","        }\n","\n","        .control-btn::before {\n","            content: '';\n","            position: absolute;\n","            top: 0;\n","            left: -100%;\n","            width: 100%;\n","            height: 100%;\n","            background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.3), transparent);\n","            transition: left 0.5s;\n","        }\n","\n","        .control-btn:hover {\n","            transform: translateY(-3px);\n","            box-shadow: 0 12px 25px rgba(0, 0, 0, 0.3);\n","            filter: brightness(1.2);\n","        }\n","\n","        .control-btn:hover::before {\n","            left: 100%;\n","        }\n","\n","        .control-btn:active {\n","            transform: translateY(0);\n","            box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);\n","        }\n","\n","        .control-btn.empty { opacity: 0; pointer-events: none; }\n","        .control-btn.reset {\n","            background: linear-gradient(145deg, #ff6b6b 0%, #ff4757 100%);\n","            grid-column: 2;\n","        }\n","\n","        .stats-panel {\n","            width: 350px;\n","            background: linear-gradient(145deg, #2c3e50, #34495e);\n","            border-radius: 20px;\n","            padding: 25px;\n","            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);\n","            height: fit-content;\n","            position: relative;\n","            overflow: hidden;\n","        }\n","\n","        .stats-panel::before {\n","            content: '';\n","            position: absolute;\n","            top: 0;\n","            left: 0;\n","            right: 0;\n","            height: 4px;\n","            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4);\n","            animation: stats-glow 2s ease-in-out infinite;\n","        }\n","\n","        @keyframes stats-glow {\n","            0%, 100% { opacity: 0.7; }\n","            50% { opacity: 1; }\n","        }\n","\n","        .stats-header {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.5em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","            margin-bottom: 20px;\n","            text-align: center;\n","            text-shadow: 0 0 10px rgba(78, 205, 196, 0.5);\n","        }\n","\n","        .stat-item {\n","            display: flex;\n","            justify-content: space-between;\n","            margin: 12px 0;\n","            padding: 8px 12px;\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            border-left: 3px solid #4ecdc4;\n","            transition: all 0.3s ease;\n","        }\n","\n","        .stat-item:hover {\n","            background: rgba(255, 255, 255, 0.1);\n","            transform: translateX(5px);\n","        }\n","\n","        .stat-label {\n","            font-weight: 500;\n","            color: #ecf0f1;\n","        }\n","\n","        .stat-value {\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","            text-shadow: 0 0 5px rgba(78, 205, 196, 0.3);\n","        }\n","\n","        .progress-bar {\n","            width: 100%;\n","            height: 8px;\n","            background: rgba(255, 255, 255, 0.1);\n","            border-radius: 4px;\n","            overflow: hidden;\n","            margin: 5px 0;\n","        }\n","\n","        .progress-fill {\n","            height: 100%;\n","            background: linear-gradient(90deg, #4ecdc4, #45b7d1);\n","            border-radius: 4px;\n","            transition: width 0.5s ease;\n","            position: relative;\n","        }\n","\n","        .progress-fill::after {\n","            content: '';\n","            position: absolute;\n","            top: 0;\n","            left: -100%;\n","            width: 100%;\n","            height: 100%;\n","            background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.5), transparent);\n","            animation: progress-shine 2s infinite;\n","        }\n","\n","        @keyframes progress-shine {\n","            0% { left: -100%; }\n","            100% { left: 100%; }\n","        }\n","\n","        .legend {\n","            background: linear-gradient(145deg, #1a252f, #2c3e50);\n","            padding: 15px;\n","            border-radius: 10px;\n","            margin-top: 20px;\n","            font-size: 0.9em;\n","            line-height: 1.6;\n","        }\n","\n","        .legend-title {\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 700;\n","            color: #ffeaa7;\n","            margin-bottom: 10px;\n","        }\n","\n","        .legend-item {\n","            margin: 5px 0;\n","            display: flex;\n","            align-items: center;\n","        }\n","\n","        .legend-icon {\n","            font-size: 1.2em;\n","            margin-right: 8px;\n","            min-width: 20px;\n","        }\n","\n","        .performance-indicators {\n","            display: grid;\n","            grid-template-columns: 1fr 1fr;\n","            gap: 10px;\n","            margin: 15px 0;\n","        }\n","\n","        .indicator {\n","            text-align: center;\n","            padding: 10px;\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            border: 1px solid rgba(78, 205, 196, 0.3);\n","        }\n","\n","        .indicator-value {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.2em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","        }\n","\n","        .indicator-label {\n","            font-size: 0.8em;\n","            color: #bdc3c7;\n","            margin-top: 5px;\n","        }\n","\n","        @media (max-width: 1200px) {\n","            .container {\n","                flex-direction: column;\n","                max-width: 100%;\n","            }\n","\n","            .stats-panel {\n","                width: 100%;\n","                max-width: 600px;\n","                margin: 0 auto;\n","            }\n","\n","            .header {\n","                font-size: 2em;\n","            }\n","        }\n","    </style>\n","</head>\n","<body>\n","    <div class=\"header\">\n","         ENHANCED GRLM WORLD \n","    </div>\n","\n","    <div class=\"container\">\n","        <div class=\"world-section\">\n","            <div class=\"canvas-container\">\n","                <canvas id=\"worldCanvas\" width=\"600\" height=\"600\"></canvas>\n","            </div>\n","            <div class=\"controls\">\n","                <div class=\"control-grid\">\n","                    <div class=\"control-btn empty\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(0, -1)\"></div>\n","                    <div class=\"control-btn empty\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(-1, 0)\"></div>\n","                    <div class=\"control-btn reset\" onclick=\"resetWorld()\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(1, 0)\"></div>\n","                    <div class=\"control-btn empty\"></div>\n","                    <div class=\"control-btn\" onclick=\"makeMove(0, 1)\"></div>\n","                    <div class=\"control-btn empty\"></div>\n","                </div>\n","            </div>\n","        </div>\n","\n","        <div class=\"stats-panel\">\n","            <div class=\"stats-header\"> AI METRICS</div>\n","            <div class=\"stats\" id=\"stats\">\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Initializing...</span>\n","                </div>\n","            </div>\n","\n","            <div class=\"performance-indicators\">\n","                <div class=\"indicator\">\n","                    <div class=\"indicator-value\" id=\"confidence-indicator\">--</div>\n","                    <div class=\"indicator-label\">Confidence</div>\n","                </div>\n","                <div class=\"indicator\">\n","                    <div class=\"indicator-value\" id=\"learning-indicator\">--</div>\n","                    <div class=\"indicator-label\">Learning</div>\n","                </div>\n","            </div>\n","\n","            <div class=\"legend\">\n","                <div class=\"legend-title\"> CONTROLS & LEGEND</div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>WASD or Arrow Buttons to Move</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>AI Agent (You)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Walls (Blocked, -1 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Food (+1 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Water (+0.3 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Treasure (+2 Reward)</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Particles show AI learning</span>\n","                </div>\n","                <div class=\"legend-item\">\n","                    <span class=\"legend-icon\"></span>\n","                    <span>Trail shows exploration path</span>\n","                </div>\n","            </div>\n","        </div>\n","    </div>\n","\n","    <script>\n","        const canvas = document.getElementById('worldCanvas');\n","        const ctx = canvas.getContext('2d');\n","        const gridSize = 24;\n","\n","        let worldState = null;\n","        let grlmStats = null;\n","        let animationFrame = null;\n","        let particles = [];\n","\n","        // Enhanced color palette\n","        const colors = {\n","            empty: '#0a0a0f',\n","            wall: '#2c3e50',\n","            food: '#e74c3c',\n","            water: '#3498db',\n","            treasure: '#f39c12',\n","            agent: '#2ecc71',\n","            memory: '#9b59b6',\n","            trail: '#1abc9c',\n","            particle: '#ffffff'\n","        };\n","\n","        function createParticle(x, y, color, velocity = 1) {\n","            return {\n","                x: x * gridSize + gridSize / 2,\n","                y: y * gridSize + gridSize / 2,\n","                vx: (Math.random() - 0.5) * velocity * 2,\n","                vy: (Math.random() - 0.5) * velocity * 2,\n","                life: 1.0,\n","                maxLife: 1.0,\n","                color: color,\n","                size: Math.random() * 3 + 2\n","            };\n","        }\n","\n","        function updateParticles() {\n","            for (let i = particles.length - 1; i >= 0; i--) {\n","                const p = particles[i];\n","                p.x += p.vx;\n","                p.y += p.vy;\n","                p.life -= 0.02;\n","                p.vy += 0.1; // gravity\n","                p.vx *= 0.98; // air resistance\n","\n","                if (p.life <= 0) {\n","                    particles.splice(i, 1);\n","                }\n","            }\n","        }\n","\n","        function drawParticles() {\n","            particles.forEach(p => {\n","                const alpha = p.life / p.maxLife;\n","                ctx.save();\n","                ctx.globalAlpha = alpha;\n","                ctx.fillStyle = p.color;\n","                ctx.beginPath();\n","                ctx.arc(p.x, p.y, p.size * alpha, 0, 2 * Math.PI);\n","                ctx.fill();\n","                ctx.restore();\n","            });\n","        }\n","\n","        function drawGradientBackground() {\n","            const gradient = ctx.createRadialGradient(\n","                canvas.width / 2, canvas.height / 2, 0,\n","                canvas.width / 2, canvas.height / 2, Math.max(canvas.width, canvas.height) / 2\n","            );\n","            gradient.addColorStop(0, '#1a1a2e');\n","            gradient.addColorStop(1, '#0f0f23');\n","\n","            ctx.fillStyle = gradient;\n","            ctx.fillRect(0, 0, canvas.width, canvas.height);\n","        }\n","\n","        function drawGrid() {\n","            if (!worldState) return;\n","\n","            const grid = worldState.grid;\n","            const size = worldState.size;\n","\n","            for (let x = 0; x < size; x++) {\n","                for (let y = 0; y < size; y++) {\n","                    const pixelX = x * gridSize;\n","                    const pixelY = y * gridSize;\n","                    const gridValue = grid[x][y];\n","\n","                    // Draw cell background with subtle glow\n","                    let alpha = 0.1;\n","                    let color = colors.empty;\n","\n","                    if (gridValue === -1) {\n","                        color = colors.wall;\n","                        alpha = 0.8;\n","                    } else if (gridValue > 0) {\n","                        alpha = gridValue * 0.3;\n","                        color = '#2ecc71';\n","                    }\n","\n","                    ctx.fillStyle = color;\n","                    ctx.globalAlpha = alpha;\n","                    ctx.fillRect(pixelX, pixelY, gridSize, gridSize);\n","                    ctx.globalAlpha = 1;\n","\n","                    // Grid lines with glow effect\n","                    ctx.strokeStyle = 'rgba(78, 205, 196, 0.2)';\n","                    ctx.lineWidth = 0.5;\n","                    ctx.strokeRect(pixelX, pixelY, gridSize, gridSize);\n","                }\n","            }\n","        }\n","\n","        function drawObjects() {\n","            if (!worldState || !worldState.objects) return;\n","\n","            const objects = worldState.objects;\n","            const time = Date.now() * 0.001;\n","\n","            for (const [key, objType] of Object.entries(objects)) {\n","                const [x, y] = key.split(',').map(Number);\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","\n","                let emoji = '';\n","                let glowColor = '';\n","                let glowIntensity = 0;\n","\n","                switch (objType) {\n","                    case 'wall':\n","                        emoji = '';\n","                        break;\n","                    case 'food':\n","                        emoji = '';\n","                        glowColor = colors.food;\n","                        glowIntensity = 0.3;\n","                        break;\n","                    case 'water':\n","                        emoji = '';\n","                        glowColor = colors.water;\n","                        glowIntensity = 0.2 + 0.1 * Math.sin(time * 2);\n","                        break;\n","                    case 'treasure':\n","                        emoji = '';\n","                        glowColor = colors.treasure;\n","                        glowIntensity = 0.4 + 0.2 * Math.sin(time * 3);\n","                        break;\n","                }\n","\n","                // Draw glow effect\n","                if (glowIntensity > 0) {\n","                    const gradient = ctx.createRadialGradient(\n","                        pixelX + gridSize/2, pixelY + gridSize/2, 0,\n","                        pixelX + gridSize/2, pixelY + gridSize/2, gridSize\n","                    );\n","                    gradient.addColorStop(0, glowColor + Math.floor(glowIntensity * 255).toString(16).padStart(2, '0'));\n","                    gradient.addColorStop(1, 'transparent');\n","\n","                    ctx.fillStyle = gradient;\n","                    ctx.fillRect(pixelX - 5, pixelY - 5, gridSize + 10, gridSize + 10);\n","                }\n","\n","                // Draw object\n","                if (emoji) {\n","                    ctx.font = `${gridSize * 0.7}px Arial`;\n","                    ctx.textAlign = 'center';\n","                    ctx.textBaseline = 'middle';\n","                    ctx.fillStyle = '#ffffff';\n","                    ctx.fillText(emoji, pixelX + gridSize/2, pixelY + gridSize/2);\n","                }\n","            }\n","        }\n","\n","        function drawTrail() {\n","            if (!worldState || !worldState.trail) return;\n","\n","            const trail = worldState.trail;\n","            ctx.strokeStyle = colors.trail;\n","            ctx.lineWidth = 2;\n","            ctx.lineCap = 'round';\n","            ctx.lineJoin = 'round';\n","\n","            if (trail.length > 1) {\n","                ctx.beginPath();\n","                for (let i = 0; i < trail.length; i++) {\n","                    const alpha = (i + 1) / trail.length * 0.5;\n","                    ctx.globalAlpha = alpha;\n","\n","                    const x = trail[i][0] * gridSize + gridSize / 2;\n","                    const y = trail[i][1] * gridSize + gridSize / 2;\n","\n","                    if (i === 0) {\n","                        ctx.moveTo(x, y);\n","                    } else {\n","                        ctx.lineTo(x, y);\n","                    }\n","                }\n","                ctx.stroke();\n","                ctx.globalAlpha = 1;\n","            }\n","        }\n","\n","        function drawAgent() {\n","            if (!worldState) return;\n","\n","            const agentPos = worldState.agent_position;\n","            const agentX = agentPos[0] * gridSize;\n","            const agentY = agentPos[1] * gridSize;\n","\n","            // Agent glow\n","            const gradient = ctx.createRadialGradient(\n","                agentX + gridSize/2, agentY + gridSize/2, 0,\n","                agentX + gridSize/2, agentY + gridSize/2, gridSize * 0.8\n","            );\n","            gradient.addColorStop(0, colors.agent + '80');\n","            gradient.addColorStop(1, 'transparent');\n","\n","            ctx.fillStyle = gradient;\n","            ctx.fillRect(agentX - 5, agentY - 5, gridSize + 10, gridSize + 10);\n","\n","            // Agent body\n","            ctx.font = `${gridSize * 0.8}px Arial`;\n","            ctx.textAlign = 'center';\n","            ctx.textBaseline = 'middle';\n","            ctx.fillStyle = '#ffffff';\n","            ctx.fillText('', agentX + gridSize/2, agentY + gridSize/2);\n","\n","            // Agent border with pulse effect\n","            const time = Date.now() * 0.003;\n","            const pulse = 0.8 + 0.2 * Math.sin(time);\n","            ctx.strokeStyle = colors.agent;\n","            ctx.lineWidth = 3 * pulse;\n","            ctx.strokeRect(agentX + 2, agentY + 2, gridSize - 4, gridSize - 4);\n","        }\n","\n","        function drawMemoryHeatmap() {\n","            if (!grlmStats || !grlmStats.memory_heatmap) return;\n","\n","            const heatmap = grlmStats.memory_heatmap;\n","\n","            for (const [key, data] of Object.entries(heatmap)) {\n","                const [x, y] = key.split(',').map(Number);\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","\n","                const intensity = Math.min(1, data.importance * 0.5);\n","                const emotion = data.emotion;\n","\n","                // Color based on emotional valence\n","                let color = colors.memory;\n","                if (emotion > 0.5) color = '#2ecc71'; // positive\n","                else if (emotion < -0.5) color = '#e74c3c'; // negative\n","\n","                ctx.fillStyle = color + Math.floor(intensity * 100).toString(16).padStart(2, '0');\n","                ctx.beginPath();\n","                ctx.arc(pixelX + gridSize/2, pixelY + gridSize/2, intensity * 8, 0, 2 * Math.PI);\n","                ctx.fill();\n","            }\n","        }\n","\n","        function drawWorld() {\n","            ctx.clearRect(0, 0, canvas.width, canvas.height);\n","\n","            drawGradientBackground();\n","            drawGrid();\n","            drawMemoryHeatmap();\n","            drawObjects();\n","            drawTrail();\n","            drawAgent();\n","            drawParticles();\n","        }\n","\n","        function animate() {\n","            updateParticles();\n","            drawWorld();\n","            animationFrame = requestAnimationFrame(animate);\n","        }\n","\n","        function updateStats() {\n","            if (!grlmStats) return;\n","\n","            const statsDiv = document.getElementById('stats');\n","            const confidenceIndicator = document.getElementById('confidence-indicator');\n","            const learningIndicator = document.getElementById('learning-indicator');\n","\n","            // Update main stats\n","            statsDiv.innerHTML = `\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Steps Taken</span>\n","                    <span class=\"stat-value\">${grlmStats.total_steps}</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Total Reward</span>\n","                    <span class=\"stat-value\">${grlmStats.total_reward.toFixed(2)}</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Efficiency</span>\n","                    <span class=\"stat-value\">${grlmStats.avg_reward_per_step.toFixed(3)}</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Prediction Error</span>\n","                    <span class=\"stat-value\">${(grlmStats.recent_prediction_error * 1000).toFixed(2)}ms</span>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Exploration</span>\n","                    <span class=\"stat-value\">${grlmStats.exploration_progress.toFixed(1)}%</span>\n","                </div>\n","                <div class=\"progress-bar\">\n","                    <div class=\"progress-fill\" style=\"width: ${grlmStats.exploration_progress}%\"></div>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Memory Efficiency</span>\n","                    <span class=\"stat-value\">${grlmStats.memory_efficiency.toFixed(1)}%</span>\n","                </div>\n","                <div class=\"progress-bar\">\n","                    <div class=\"progress-fill\" style=\"width: ${grlmStats.memory_efficiency}%\"></div>\n","                </div>\n","                <div class=\"stat-item\">\n","                    <span class=\"stat-label\">Emotional Balance</span>\n","                    <span class=\"stat-value\">${grlmStats.emotional_balance.toFixed(2)}</span>\n","                </div>\n","            `;\n","\n","            // Update indicators\n","            confidenceIndicator.textContent = `${(grlmStats.avg_confidence * 100).toFixed(0)}%`;\n","            learningIndicator.textContent = `${(grlmStats.learning_stability * 100).toFixed(0)}%`;\n","        }\n","\n","        function makeMove(dx, dy) {\n","            console.log(`Enhanced move: dx=${dx}, dy=${dy}`);\n","\n","            if (worldState) {\n","                const oldX = worldState.agent_position[0];\n","                const oldY = worldState.agent_position[1];\n","                const newX = Math.max(0, Math.min(worldState.size - 1, oldX + dx));\n","                const newY = Math.max(0, Math.min(worldState.size - 1, oldY + dy));\n","\n","                // Enhanced collision detection\n","                let canMove = true;\n","                const gridValue = worldState.grid[newX][newY];\n","                const objKey = `${newX},${newY}`;\n","\n","                if (gridValue === -1 || (worldState.objects[objKey] === 'wall')) {\n","                    canMove = false;\n","                }\n","\n","                let reward = 0;\n","                if (canMove && (newX !== oldX || newY !== oldY)) {\n","                    // Valid move\n","                    worldState.agent_position = [newX, newY];\n","                    reward = gridValue;\n","\n","                    // Add to trail\n","                    if (!worldState.trail) worldState.trail = [];\n","                    worldState.trail.push([oldX, oldY]);\n","                    if (worldState.trail.length > 15) {\n","                        worldState.trail.shift();\n","                    }\n","\n","                    // Object interactions with effects\n","                    if (worldState.objects[objKey]) {\n","                        const objType = worldState.objects[objKey];\n","\n","                        // Add particles\n","                        for (let i = 0; i < 5; i++) {\n","                            let particleColor = colors.particle;\n","                            switch (objType) {\n","                                case 'treasure': particleColor = colors.treasure; reward += 2.0; break;\n","                                case 'food': particleColor = colors.food; reward += 1.0; break;\n","                                case 'water': particleColor = colors.water; reward += 0.3; break;\n","                            }\n","                            particles.push(createParticle(newX, newY, particleColor, 2));\n","                        }\n","\n","                        // Remove consumed objects\n","                        if (objType !== 'wall') {\n","                            delete worldState.objects[objKey];\n","                            worldState.grid[newX][newY] = 0.0;\n","                        }\n","                    }\n","\n","                    // Movement particles\n","                    for (let i = 0; i < 3; i++) {\n","                        particles.push(createParticle(newX, newY, colors.agent, 1));\n","                    }\n","\n","                } else if (!canMove) {\n","                    reward = -1.0;\n","                    // Wall hit particles\n","                    for (let i = 0; i < 8; i++) {\n","                        particles.push(createParticle(newX, newY, colors.food, 1.5));\n","                    }\n","                }\n","\n","                // Enhanced stats simulation\n","                if (grlmStats) {\n","                    grlmStats.total_steps += 1;\n","                    grlmStats.total_reward += reward;\n","                    grlmStats.avg_reward_per_step = grlmStats.total_reward / grlmStats.total_steps;\n","                    grlmStats.recent_prediction_error = Math.max(0.001, grlmStats.recent_prediction_error * 0.95 + Math.random() * 0.01);\n","                    grlmStats.avg_confidence = Math.min(1.0, grlmStats.avg_confidence + 0.01);\n","                    grlmStats.learning_stability = Math.min(1.0, grlmStats.learning_stability + 0.005);\n","\n","                    // Update exploration\n","                    if (canMove) {\n","                        const key = `${newX},${newY}`;\n","                        if (!grlmStats.memory_heatmap[key]) {\n","                            grlmStats.memory_heatmap[key] = {\n","                                importance: Math.random(),\n","                                emotion: reward,\n","                                visits: 1\n","                            };\n","                        } else {\n","                            grlmStats.memory_heatmap[key].visits += 1;\n","                            grlmStats.memory_heatmap[key].emotion = (grlmStats.memory_heatmap[key].emotion + reward) / 2;\n","                        }\n","\n","                        const exploredCells = Object.keys(grlmStats.memory_heatmap).length;\n","                        grlmStats.exploration_progress = (exploredCells / (worldState.size * worldState.size)) * 100;\n","                        grlmStats.memory_efficiency = Math.min(100, exploredCells / grlmStats.total_steps * 100);\n","                    }\n","\n","                    grlmStats.emotional_balance = (grlmStats.emotional_balance * 0.9 + reward * 0.1);\n","                }\n","\n","                updateStats();\n","            }\n","        }\n","\n","        function resetWorld() {\n","            initEnhancedWorld();\n","            particles = [];\n","        }\n","\n","        // Keyboard controls\n","        document.addEventListener('keydown', (e) => {\n","            switch(e.key.toLowerCase()) {\n","                case 'w': case 'arrowup': makeMove(0, -1); break;\n","                case 's': case 'arrowdown': makeMove(0, 1); break;\n","                case 'a': case 'arrowleft': makeMove(-1, 0); break;\n","                case 'd': case 'arrowright': makeMove(1, 0); break;\n","                case 'r': resetWorld(); break;\n","            }\n","            e.preventDefault();\n","        });\n","\n","        function initEnhancedWorld() {\n","            const size = 25;\n","            worldState = {\n","                grid: Array(size).fill().map(() => Array(size).fill(0)),\n","                objects: {},\n","                agent_position: [Math.floor(size/2), Math.floor(size/2)],\n","                trail: [],\n","                size: size\n","            };\n","\n","            // Generate enhanced world\n","            const rooms = [\n","                {x: 3, y: 3, w: 6, h: 4, treasure: true},\n","                {x: 15, y: 8, w: 7, h: 5, treasure: false},\n","                {x: 5, y: 18, w: 5, h: 4, treasure: false}\n","            ];\n","\n","            rooms.forEach(room => {\n","                // Room walls\n","                for (let x = room.x; x < room.x + room.w; x++) {\n","                    [room.y, room.y + room.h - 1].forEach(y => {\n","                        if (x < size && y < size) {\n","                            worldState.objects[`${x},${y}`] = 'wall';\n","                            worldState.grid[x][y] = -1;\n","                        }\n","                    });\n","                }\n","                for (let y = room.y; y < room.y + room.h; y++) {\n","                    [room.x, room.x + room.w - 1].forEach(x => {\n","                        if (x < size && y < size) {\n","                            worldState.objects[`${x},${y}`] = 'wall';\n","                            worldState.grid[x][y] = -1;\n","                        }\n","                    });\n","                }\n","\n","                // Room contents\n","                const centerX = room.x + Math.floor(room.w / 2);\n","                const centerY = room.y + Math.floor(room.h / 2);\n","                if (centerX < size && centerY < size) {\n","                    const objType = room.treasure ? 'treasure' : Math.random() > 0.5 ? 'food' : 'water';\n","                    worldState.objects[`${centerX},${centerY}`] = objType;\n","                    worldState.grid[centerX][centerY] = objType === 'treasure' ? 1.0 : (objType === 'food' ? 0.5 : 0.3);\n","                }\n","            });\n","\n","            // Scattered objects\n","            for (let i = 0; i < size * 1.5; i++) {\n","                const x = Math.floor(Math.random() * size);\n","                const y = Math.floor(Math.random() * size);\n","                const key = `${x},${y}`;\n","\n","                if (!worldState.objects[key] && !(x === worldState.agent_position[0] && y === worldState.agent_position[1])) {\n","                    const objType = Math.random() > 0.7 ? 'treasure' : (Math.random() > 0.5 ? 'food' : 'water');\n","                    worldState.objects[key] = objType;\n","                    worldState.grid[x][y] = objType === 'treasure' ? 1.0 : (objType === 'food' ? 0.5 : 0.3);\n","                }\n","            }\n","\n","            grlmStats = {\n","                total_steps: 0,\n","                total_reward: 0,\n","                avg_reward_per_step: 0,\n","                recent_prediction_error: 0.05,\n","                avg_confidence: 0.5,\n","                learning_stability: 0.5,\n","                exploration_progress: 0,\n","                memory_efficiency: 0,\n","                emotional_balance: 0,\n","                memory_heatmap: {}\n","            };\n","\n","            updateStats();\n","        }\n","\n","        // Initialize and start\n","        initEnhancedWorld();\n","        animate();\n","\n","        // Focus for keyboard\n","        canvas.focus();\n","        canvas.setAttribute('tabindex', '0');\n","\n","        console.log(' Enhanced GRLM World initialized with visual effects!');\n","    </script>\n","</body>\n","</html>\n","    \"\"\"\n","\n","    return html_template\n","\n","# ============================================================================\n","# ENHANCED COLAB INTEGRATION\n","# ============================================================================\n","\n","def launch_enhanced_visual_world():\n","    \"\"\"Launch the enhanced visual GRLM world.\"\"\"\n","\n","    print(\" Launching Enhanced Visual 2D GRLM World...\")\n","    print(\" Features:\")\n","    print(\"    Particle effects and animations\")\n","    print(\"    Real-time AI confidence tracking\")\n","    print(\"    Memory heatmap visualization\")\n","    print(\"    Dynamic learning indicators\")\n","    print(\"    Enhanced visual feedback\")\n","\n","    # Create the enhanced system\n","    grlm_system = VisualGRLM()\n","\n","    # Create enhanced HTML interface\n","    html_content = create_enhanced_html_interface()\n","\n","    print(\" Visual enhancements active:\")\n","    print(\"    Particle systems for interactions\")\n","    print(\"    Glowing objects with animations\")\n","    print(\"    Agent trail visualization\")\n","    print(\"    Memory importance heatmaps\")\n","    print(\"    Real-time performance metrics\")\n","\n","    # Display the enhanced interface\n","    display(HTML(html_content))\n","\n","    return grlm_system\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","if __name__ == \"__main__\":\n","    enhanced_system = launch_enhanced_visual_world()\n","    print(\" Enhanced Visual GRLM World is now running!\")\n","\n","# Launch the enhanced system\n","enhanced_system = launch_enhanced_visual_world()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":837,"status":"ok","timestamp":1758216805792,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"},"user_tz":-180},"id":"HQvay54740z4","outputId":"ab544ced-f70b-404b-f906-17e58d9dd9eb"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Advanced Multi-Agent GRLM System Starting\n","Device: cuda | Agents: 4 | World: 30x30\n","GPU Memory: 42.5GB\n"," Launching Advanced Multi-Agent GRLM System...\n"," Compute-intensive features:\n","    Multi-agent curiosity-driven exploration\n","    Neural communication between agents\n","    Episodic replay with prioritized experience\n","    Semantic concept formation\n","    Dynamic world with emergent complexity\n","    Territorial behavior and resource competition\n"," Advanced Multi-Agent GRLM initialized with 4 agents\n"," Each agent has curiosity, novelty detection, and episodic memory\n"," Dynamic world with territorial control and resource regeneration\n"," Neural communication between agents\n"," System ready! Features active:\n","    Intrinsic Curiosity Module (ICM)\n","    Random Network Distillation (RND)\n","    Prioritized Experience Replay\n","     Hierarchical Semantic Memory\n","    Multi-agent Communication\n","    Dynamic Environment\n","    Real-time visualization\n"]},{"data":{"text/html":["\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>Advanced Multi-Agent GRLM System</title>\n","    <style>\n","        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700;900&family=Roboto:wght@300;400;500&display=swap');\n","        \n","        * { margin: 0; padding: 0; box-sizing: border-box; }\n","        \n","        body {\n","            font-family: 'Roboto', sans-serif;\n","            background: linear-gradient(135deg, #0c0c0c 0%, #1a1a2e 50%, #16213e 100%);\n","            color: #ffffff;\n","            overflow-x: hidden;\n","            min-height: 100vh;\n","        }\n","        \n","        .header {\n","            text-align: center;\n","            padding: 15px;\n","            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4, #ffeaa7, #fd79a8);\n","            background-size: 400% 100%;\n","            animation: gradient-flow 4s ease-in-out infinite;\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 900;\n","            font-size: 2em;\n","            text-shadow: 0 0 20px rgba(255, 255, 255, 0.5);\n","            margin-bottom: 15px;\n","        }\n","        \n","        @keyframes gradient-flow {\n","            0%, 100% { background-position: 0% 50%; }\n","            50% { background-position: 100% 50%; }\n","        }\n","        \n","        .main-container {\n","            display: grid;\n","            grid-template-columns: 1fr 400px;\n","            gap: 15px;\n","            padding: 0 15px;\n","            max-width: 1600px;\n","            margin: 0 auto;\n","        }\n","        \n","        .world-section {\n","            background: linear-gradient(145deg, #1e3c72, #2a5298);\n","            border-radius: 15px;\n","            padding: 20px;\n","            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);\n","            position: relative;\n","            overflow: hidden;\n","        }\n","        \n","        .canvas-container {\n","            position: relative;\n","            display: inline-block;\n","            border-radius: 12px;\n","            overflow: hidden;\n","            box-shadow: 0 0 30px rgba(0, 255, 255, 0.2);\n","        }\n","        \n","        canvas {\n","            display: block;\n","            background: radial-gradient(circle at center, #0f0f23 0%, #000000 100%);\n","        }\n","        \n","        .controls {\n","            margin-top: 15px;\n","            display: grid;\n","            grid-template-columns: repeat(4, 1fr);\n","            gap: 10px;\n","        }\n","        \n","        .agent-controls {\n","            background: rgba(255, 255, 255, 0.1);\n","            border-radius: 8px;\n","            padding: 10px;\n","            text-align: center;\n","        }\n","        \n","        .agent-label {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 0.9em;\n","            margin-bottom: 8px;\n","            color: #4ecdc4;\n","        }\n","        \n","        .control-grid {\n","            display: grid;\n","            grid-template-columns: repeat(3, 1fr);\n","            gap: 3px;\n","        }\n","        \n","        .control-btn {\n","            background: linear-gradient(145deg, #667eea 0%, #764ba2 100%);\n","            color: white;\n","            border: none;\n","            padding: 8px;\n","            border-radius: 6px;\n","            cursor: pointer;\n","            font-size: 12px;\n","            font-weight: bold;\n","            transition: all 0.2s ease;\n","        }\n","        \n","        .control-btn:hover {\n","            transform: scale(1.1);\n","            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);\n","        }\n","        \n","        .control-btn.empty { opacity: 0; pointer-events: none; }\n","        \n","        .auto-mode {\n","            grid-column: 1 / -1;\n","            margin-top: 10px;\n","            background: linear-gradient(145deg, #2ecc71, #27ae60);\n","            padding: 10px;\n","            border-radius: 8px;\n","            cursor: pointer;\n","            font-family: 'Orbitron', monospace;\n","            text-align: center;\n","        }\n","        \n","        .stats-section {\n","            display: flex;\n","            flex-direction: column;\n","            gap: 15px;\n","        }\n","        \n","        .stats-panel {\n","            background: linear-gradient(145deg, #2c3e50, #34495e);\n","            border-radius: 15px;\n","            padding: 20px;\n","            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.2);\n","        }\n","        \n","        .panel-header {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.2em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","            margin-bottom: 15px;\n","            text-align: center;\n","            text-shadow: 0 0 10px rgba(78, 205, 196, 0.5);\n","        }\n","        \n","        .agent-stats {\n","            display: grid;\n","            grid-template-columns: 1fr 1fr;\n","            gap: 10px;\n","            margin-bottom: 15px;\n","        }\n","        \n","        .agent-stat-card {\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            padding: 10px;\n","            border-left: 3px solid;\n","            transition: all 0.3s ease;\n","        }\n","        \n","        .agent-stat-card:nth-child(1) { border-left-color: #e74c3c; }\n","        .agent-stat-card:nth-child(2) { border-left-color: #3498db; }\n","        .agent-stat-card:nth-child(3) { border-left-color: #2ecc71; }\n","        .agent-stat-card:nth-child(4) { border-left-color: #f39c12; }\n","        \n","        .agent-stat-card:hover {\n","            background: rgba(255, 255, 255, 0.1);\n","            transform: translateY(-2px);\n","        }\n","        \n","        .stat-item {\n","            display: flex;\n","            justify-content: space-between;\n","            margin: 5px 0;\n","            font-size: 0.85em;\n","        }\n","        \n","        .stat-label {\n","            color: #bdc3c7;\n","        }\n","        \n","        .stat-value {\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","        }\n","        \n","        .emergent-behaviors {\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            padding: 15px;\n","            margin-top: 10px;\n","        }\n","        \n","        .behavior-item {\n","            background: rgba(78, 205, 196, 0.1);\n","            border-radius: 5px;\n","            padding: 8px;\n","            margin: 5px 0;\n","            font-size: 0.85em;\n","        }\n","        \n","        .system-metrics {\n","            display: grid;\n","            grid-template-columns: 1fr 1fr;\n","            gap: 10px;\n","        }\n","        \n","        .metric {\n","            text-align: center;\n","            padding: 10px;\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","        }\n","        \n","        .metric-value {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.5em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","        }\n","        \n","        .metric-label {\n","            font-size: 0.8em;\n","            color: #bdc3c7;\n","            margin-top: 5px;\n","        }\n","        \n","        @media (max-width: 1200px) {\n","            .main-container {\n","                grid-template-columns: 1fr;\n","                max-width: 100%;\n","            }\n","            \n","            .stats-section {\n","                flex-direction: row;\n","                overflow-x: auto;\n","            }\n","            \n","            .stats-panel {\n","                min-width: 300px;\n","            }\n","        }\n","    </style>\n","</head>\n","<body>\n","    <div class=\"header\">\n","         ADVANCED MULTI-AGENT GRLM \n","    </div>\n","    \n","    <div class=\"main-container\">\n","        <div class=\"world-section\">\n","            <div class=\"canvas-container\">\n","                <canvas id=\"worldCanvas\" width=\"600\" height=\"600\"></canvas>\n","            </div>\n","            <div class=\"controls\">\n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 1 (Explorer)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","                \n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 2 (Social)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","                \n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 3 (Risk-taker)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","                \n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 4 (Balanced)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","            </div>\n","            \n","            <div class=\"auto-mode\" onclick=\"toggleAutoMode()\">\n","                 AUTO MODE: OFF\n","            </div>\n","        </div>\n","        \n","        <div class=\"stats-section\">\n","            <div class=\"stats-panel\">\n","                <div class=\"panel-header\"> AGENT STATISTICS</div>\n","                <div class=\"agent-stats\" id=\"agentStats\">\n","                    Loading agent data...\n","                </div>\n","            </div>\n","            \n","            <div class=\"stats-panel\">\n","                <div class=\"panel-header\"> EMERGENT BEHAVIORS</div>\n","                <div class=\"emergent-behaviors\" id=\"emergentBehaviors\">\n","                    <div>Monitoring for emergent behaviors...</div>\n","                </div>\n","            </div>\n","            \n","            <div class=\"stats-panel\">\n","                <div class=\"panel-header\"> SYSTEM METRICS</div>\n","                <div class=\"system-metrics\" id=\"systemMetrics\">\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"complexity\">1.0</div>\n","                        <div class=\"metric-label\">Complexity</div>\n","                    </div>\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"cooperation\">0.0</div>\n","                        <div class=\"metric-label\">Cooperation</div>\n","                    </div>\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"exploration\">0%</div>\n","                        <div class=\"metric-label\">Explored</div>\n","                    </div>\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"intelligence\">0.0</div>\n","                        <div class=\"metric-label\">Collective IQ</div>\n","                    </div>\n","                </div>\n","            </div>\n","        </div>\n","    </div>\n","\n","    <script>\n","        const canvas = document.getElementById('worldCanvas');\n","        const ctx = canvas.getContext('2d');\n","        const gridSize = 20;\n","        \n","        let worldState = null;\n","        let systemStats = null;\n","        let autoMode = false;\n","        let animationFrame = null;\n","        \n","        // Agent colors\n","        const agentColors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12'];\n","        const agentNames = ['Explorer', 'Social', 'Risk-taker', 'Balanced'];\n","        \n","        function initializeSystem() {\n","            worldState = {\n","                size: 30,\n","                grid: Array(30).fill().map(() => Array(30).fill(0)),\n","                objects: {},\n","                agents: {'0': [2, 2], '1': [27, 2], '2': [2, 27], '3': [27, 27]},\n","                agent_trails: {'0': [], '1': [], '2': [], '3': []},\n","                territory_map: Array(30).fill().map(() => Array(30).fill(-1)),\n","                complexity_level: 1.0,\n","                time_step: 0\n","            };\n","            \n","            // Generate initial objects\n","            for (let i = 0; i < 100; i++) {\n","                const x = Math.floor(Math.random() * 30);\n","                const y = Math.floor(Math.random() * 30);\n","                const key = `${x},${y}`;\n","                if (!worldState.objects[key]) {\n","                    const objType = Math.random() > 0.7 ? 'treasure' : (Math.random() > 0.5 ? 'food' : 'water');\n","                    worldState.objects[key] = objType;\n","                    worldState.grid[x][y] = objType === 'treasure' ? 2.0 : (objType === 'food' ? 1.0 : 0.5);\n","                }\n","            }\n","            \n","            systemStats = {\n","                agents: [\n","                    {agent_id: 0, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 1.5}},\n","                    {agent_id: 1, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 0.8}},\n","                    {agent_id: 2, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 1.0}},\n","                    {agent_id: 3, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 1.2}}\n","                ],\n","                emergent_behaviors: 0,\n","                collective_intelligence: 0,\n","                world: {complexity_level: 1.0, time_step: 0}\n","            };\n","            \n","            updateDisplay();\n","        }\n","        \n","        function drawWorld() {\n","            ctx.clearRect(0, 0, canvas.width, canvas.height);\n","            \n","            if (!worldState) return;\n","            \n","            const size = worldState.size;\n","            \n","            // Draw grid background\n","            for (let x = 0; x < size; x++) {\n","                for (let y = 0; y < size; y++) {\n","                    const pixelX = x * gridSize;\n","                    const pixelY = y * gridSize;\n","                    \n","                    // Territory coloring\n","                    const territory = worldState.territory_map[x][y];\n","                    if (territory >= 0) {\n","                        ctx.fillStyle = agentColors[territory] + '20';\n","                        ctx.fillRect(pixelX, pixelY, gridSize, gridSize);\n","                    }\n","                    \n","                    // Grid lines\n","                    ctx.strokeStyle = 'rgba(255, 255, 255, 0.1)';\n","                    ctx.lineWidth = 0.5;\n","                    ctx.strokeRect(pixelX, pixelY, gridSize, gridSize);\n","                }\n","            }\n","            \n","            // Draw objects\n","            for (const [key, objType] of Object.entries(worldState.objects)) {\n","                const [x, y] = key.split(',').map(Number);\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","                \n","                let emoji = '';\n","                let glowColor = '';\n","                \n","                switch (objType) {\n","                    case 'wall': emoji = ''; break;\n","                    case 'food': emoji = ''; glowColor = '#e74c3c'; break;\n","                    case 'water': emoji = ''; glowColor = '#3498db'; break;\n","                    case 'treasure': emoji = ''; glowColor = '#f39c12'; break;\n","                }\n","                \n","                // Glow effect\n","                if (glowColor) {\n","                    const gradient = ctx.createRadialGradient(\n","                        pixelX + gridSize/2, pixelY + gridSize/2, 0,\n","                        pixelX + gridSize/2, pixelY + gridSize/2, gridSize\n","                    );\n","                    gradient.addColorStop(0, glowColor + '40');\n","                    gradient.addColorStop(1, 'transparent');\n","                    ctx.fillStyle = gradient;\n","                    ctx.fillRect(pixelX - 2, pixelY - 2, gridSize + 4, gridSize + 4);\n","                }\n","                \n","                // Object\n","                ctx.font = `${gridSize * 0.6}px Arial`;\n","                ctx.textAlign = 'center';\n","                ctx.textBaseline = 'middle';\n","                ctx.fillStyle = '#ffffff';\n","                ctx.fillText(emoji, pixelX + gridSize/2, pixelY + gridSize/2);\n","            }\n","            \n","            // Draw agent trails\n","            for (const [agentId, trail] of Object.entries(worldState.agent_trails)) {\n","                if (trail.length > 1) {\n","                    ctx.strokeStyle = agentColors[parseInt(agentId)] + '60';\n","                    ctx.lineWidth = 2;\n","                    ctx.beginPath();\n","                    \n","                    for (let i = 0; i < trail.length; i++) {\n","                        const pos = trail[i];\n","                        const x = (pos % worldState.size) * gridSize + gridSize/2;\n","                        const y = Math.floor(pos / worldState.size) * gridSize + gridSize/2;\n","                        \n","                        if (i === 0) ctx.moveTo(x, y);\n","                        else ctx.lineTo(x, y);\n","                    }\n","                    ctx.stroke();\n","                }\n","            }\n","            \n","            // Draw agents\n","            for (const [agentId, position] of Object.entries(worldState.agents)) {\n","                const [x, y] = position;\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","                const color = agentColors[parseInt(agentId)];\n","                \n","                // Agent glow\n","                const gradient = ctx.createRadialGradient(\n","                    pixelX + gridSize/2, pixelY + gridSize/2, 0,\n","                    pixelX + gridSize/2, pixelY + gridSize/2, gridSize\n","                );\n","                gradient.addColorStop(0, color + '60');\n","                gradient.addColorStop(1, 'transparent');\n","                ctx.fillStyle = gradient;\n","                ctx.fillRect(pixelX - 3, pixelY - 3, gridSize + 6, gridSize + 6);\n","                \n","                // Agent body\n","                ctx.fillStyle = color;\n","                ctx.beginPath();\n","                ctx.arc(pixelX + gridSize/2, pixelY + gridSize/2, gridSize/3, 0, 2 * Math.PI);\n","                ctx.fill();\n","                \n","                // Agent ID\n","                ctx.fillStyle = '#ffffff';\n","                ctx.font = `${gridSize * 0.4}px Arial`;\n","                ctx.textAlign = 'center';\n","                ctx.textBaseline = 'middle';\n","                ctx.fillText(agentId, pixelX + gridSize/2, pixelY + gridSize/2);\n","            }\n","        }\n","        \n","        function updateStats() {\n","            if (!systemStats) return;\n","            \n","            // Agent stats\n","            const agentStatsDiv = document.getElementById('agentStats');\n","            agentStatsDiv.innerHTML = systemStats.agents.map((agent, i) => `\n","                <div class=\"agent-stat-card\">\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\"> ${agentNames[i]}</span>\n","                        <span class=\"stat-value\">#${agent.agent_id}</span>\n","                    </div>\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\">Reward</span>\n","                        <span class=\"stat-value\">${agent.total_reward.toFixed(1)}</span>\n","                    </div>\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\">Avg/Step</span>\n","                        <span class=\"stat-value\">${agent.avg_reward_per_step.toFixed(2)}</span>\n","                    </div>\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\">Curiosity</span>\n","                        <span class=\"stat-value\">${agent.intrinsic_motivation.toFixed(2)}</span>\n","                    </div>\n","                </div>\n","            `).join('');\n","            \n","            // System metrics\n","            document.getElementById('complexity').textContent = systemStats.world.complexity_level.toFixed(1);\n","            document.getElementById('cooperation').textContent = systemStats.system_performance?.average_cooperation?.toFixed(2) || '0.0';\n","            document.getElementById('exploration').textContent = '15%'; // Placeholder\n","            document.getElementById('intelligence').textContent = systemStats.collective_intelligence.toFixed(1);\n","            \n","            // Emergent behaviors\n","            const behaviorsDiv = document.getElementById('emergentBehaviors');\n","            if (systemStats.emergent_behaviors > 0) {\n","                behaviorsDiv.innerHTML = `\n","                    <div class=\"behavior-item\"> Clustering Behavior Detected</div>\n","                    <div class=\"behavior-item\"> Cooperative Foraging</div>\n","                    <div class=\"behavior-item\"> Territory Formation</div>\n","                `;\n","            } else {\n","                behaviorsDiv.innerHTML = '<div>Monitoring for emergent behaviors...</div>';\n","            }\n","        }\n","        \n","        function updateDisplay() {\n","            drawWorld();\n","            updateStats();\n","        }\n","        \n","        function moveAgent(agentId, dx, dy) {\n","            if (!worldState || !worldState.agents[agentId]) return;\n","            \n","            const [oldX, oldY] = worldState.agents[agentId];\n","            const newX = Math.max(0, Math.min(worldState.size - 1, oldX + dx));\n","            const newY = Math.max(0, Math.min(worldState.size - 1, oldY + dy));\n","            \n","            // Check for collisions\n","            const occupied = Object.values(worldState.agents).some(pos => pos[0] === newX && pos[1] === newY);\n","            const isWall = worldState.grid[newX][newY] === -1;\n","            \n","            if (!occupied && !isWall) {\n","                worldState.agents[agentId] = [newX, newY];\n","                \n","                // Add to trail\n","                const trailPos = oldX * worldState.size + oldY;\n","                worldState.agent_trails[agentId].push(trailPos);\n","                if (worldState.agent_trails[agentId].length > 15) {\n","                    worldState.agent_trails[agentId].shift();\n","                }\n","                \n","                // Object interaction\n","                const objKey = `${newX},${newY}`;\n","                if (worldState.objects[objKey] && worldState.objects[objKey] !== 'wall') {\n","                    const objType = worldState.objects[objKey];\n","                    let reward = 0;\n","                    \n","                    switch (objType) {\n","                        case 'treasure': reward = 3; break;\n","                        case 'food': reward = 1.5; break;\n","                        case 'water': reward = 0.8; break;\n","                    }\n","                    \n","                    // Update agent stats\n","                    systemStats.agents[agentId].total_reward += reward;\n","                    systemStats.agents[agentId].avg_reward_per_step = systemStats.agents[agentId].total_reward / (systemStats.world.time_step + 1);\n","                    systemStats.agents[agentId].intrinsic_motivation += Math.random() * 0.5;\n","                    \n","                    // Remove object\n","                    delete worldState.objects[objKey];\n","                    worldState.grid[newX][newY] = 0;\n","                }\n","                \n","                // Update territory\n","                worldState.territory_map[newX][newY] = agentId;\n","                \n","                // Update system stats\n","                systemStats.world.time_step++;\n","                systemStats.world.complexity_level = 1.0 + systemStats.world.time_step * 0.001;\n","                systemStats.collective_intelligence = systemStats.agents.reduce((sum, a) => sum + a.intrinsic_motivation, 0) / 4;\n","                \n","                if (Math.random() < 0.1) systemStats.emergent_behaviors++;\n","            }\n","            \n","            updateDisplay();\n","        }\n","        \n","        function toggleAutoMode() {\n","            autoMode = !autoMode;\n","            document.querySelector('.auto-mode').textContent = ` AUTO MODE: ${autoMode ? 'ON' : 'OFF'}`;\n","            document.querySelector('.auto-mode').style.background = autoMode ? \n","                'linear-gradient(145deg, #e74c3c, #c0392b)' : 'linear-gradient(145deg, #2ecc71, #27ae60)';\n","            \n","            if (autoMode) {\n","                runAutoMode();\n","            }\n","        }\n","        \n","        function runAutoMode() {\n","            if (!autoMode) return;\n","            \n","            // Move each agent randomly\n","            for (let i = 0; i < 4; i++) {\n","                const moves = [[-1, 0], [1, 0], [0, -1], [0, 1], [0, 0]];\n","                const [dx, dy] = moves[Math.floor(Math.random() * moves.length)];\n","                moveAgent(i, dx, dy);\n","            }\n","            \n","            setTimeout(runAutoMode, 500); // Auto-move every 500ms\n","        }\n","        \n","        // Keyboard controls for Agent 0\n","        document.addEventListener('keydown', (e) => {\n","            if (autoMode) return;\n","            \n","            switch(e.key.toLowerCase()) {\n","                case 'w': moveAgent(0, 0, -1); break;\n","                case 's': moveAgent(0, 0, 1); break;\n","                case 'a': moveAgent(0, -1, 0); break;\n","                case 'd': moveAgent(0, 1, 0); break;\n","                case ' ': toggleAutoMode(); break;\n","            }\n","            e.preventDefault();\n","        });\n","        \n","        // Initialize\n","        initializeSystem();\n","        \n","        console.log(' Advanced Multi-Agent GRLM System Initialized');\n","        console.log(' 4 agents with different personalities');\n","        console.log(' Curiosity-driven exploration');\n","        console.log(' Neural communication');\n","        console.log(' Dynamic world with territories');\n","        console.log('  WASD to control Agent 0, Space for auto-mode');\n","    </script>\n","</body>\n","</html>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"," Advanced Multi-Agent GRLM is running!\n","Watch emergent behaviors develop as agents explore and communicate!\n"," Launching Advanced Multi-Agent GRLM System...\n"," Compute-intensive features:\n","    Multi-agent curiosity-driven exploration\n","    Neural communication between agents\n","    Episodic replay with prioritized experience\n","    Semantic concept formation\n","    Dynamic world with emergent complexity\n","    Territorial behavior and resource competition\n"," Advanced Multi-Agent GRLM initialized with 4 agents\n"," Each agent has curiosity, novelty detection, and episodic memory\n"," Dynamic world with territorial control and resource regeneration\n"," Neural communication between agents\n"," System ready! Features active:\n","    Intrinsic Curiosity Module (ICM)\n","    Random Network Distillation (RND)\n","    Prioritized Experience Replay\n","     Hierarchical Semantic Memory\n","    Multi-agent Communication\n","    Dynamic Environment\n","    Real-time visualization\n"]},{"data":{"text/html":["\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>Advanced Multi-Agent GRLM System</title>\n","    <style>\n","        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700;900&family=Roboto:wght@300;400;500&display=swap');\n","        \n","        * { margin: 0; padding: 0; box-sizing: border-box; }\n","        \n","        body {\n","            font-family: 'Roboto', sans-serif;\n","            background: linear-gradient(135deg, #0c0c0c 0%, #1a1a2e 50%, #16213e 100%);\n","            color: #ffffff;\n","            overflow-x: hidden;\n","            min-height: 100vh;\n","        }\n","        \n","        .header {\n","            text-align: center;\n","            padding: 15px;\n","            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4, #ffeaa7, #fd79a8);\n","            background-size: 400% 100%;\n","            animation: gradient-flow 4s ease-in-out infinite;\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 900;\n","            font-size: 2em;\n","            text-shadow: 0 0 20px rgba(255, 255, 255, 0.5);\n","            margin-bottom: 15px;\n","        }\n","        \n","        @keyframes gradient-flow {\n","            0%, 100% { background-position: 0% 50%; }\n","            50% { background-position: 100% 50%; }\n","        }\n","        \n","        .main-container {\n","            display: grid;\n","            grid-template-columns: 1fr 400px;\n","            gap: 15px;\n","            padding: 0 15px;\n","            max-width: 1600px;\n","            margin: 0 auto;\n","        }\n","        \n","        .world-section {\n","            background: linear-gradient(145deg, #1e3c72, #2a5298);\n","            border-radius: 15px;\n","            padding: 20px;\n","            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);\n","            position: relative;\n","            overflow: hidden;\n","        }\n","        \n","        .canvas-container {\n","            position: relative;\n","            display: inline-block;\n","            border-radius: 12px;\n","            overflow: hidden;\n","            box-shadow: 0 0 30px rgba(0, 255, 255, 0.2);\n","        }\n","        \n","        canvas {\n","            display: block;\n","            background: radial-gradient(circle at center, #0f0f23 0%, #000000 100%);\n","        }\n","        \n","        .controls {\n","            margin-top: 15px;\n","            display: grid;\n","            grid-template-columns: repeat(4, 1fr);\n","            gap: 10px;\n","        }\n","        \n","        .agent-controls {\n","            background: rgba(255, 255, 255, 0.1);\n","            border-radius: 8px;\n","            padding: 10px;\n","            text-align: center;\n","        }\n","        \n","        .agent-label {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 0.9em;\n","            margin-bottom: 8px;\n","            color: #4ecdc4;\n","        }\n","        \n","        .control-grid {\n","            display: grid;\n","            grid-template-columns: repeat(3, 1fr);\n","            gap: 3px;\n","        }\n","        \n","        .control-btn {\n","            background: linear-gradient(145deg, #667eea 0%, #764ba2 100%);\n","            color: white;\n","            border: none;\n","            padding: 8px;\n","            border-radius: 6px;\n","            cursor: pointer;\n","            font-size: 12px;\n","            font-weight: bold;\n","            transition: all 0.2s ease;\n","        }\n","        \n","        .control-btn:hover {\n","            transform: scale(1.1);\n","            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);\n","        }\n","        \n","        .control-btn.empty { opacity: 0; pointer-events: none; }\n","        \n","        .auto-mode {\n","            grid-column: 1 / -1;\n","            margin-top: 10px;\n","            background: linear-gradient(145deg, #2ecc71, #27ae60);\n","            padding: 10px;\n","            border-radius: 8px;\n","            cursor: pointer;\n","            font-family: 'Orbitron', monospace;\n","            text-align: center;\n","        }\n","        \n","        .stats-section {\n","            display: flex;\n","            flex-direction: column;\n","            gap: 15px;\n","        }\n","        \n","        .stats-panel {\n","            background: linear-gradient(145deg, #2c3e50, #34495e);\n","            border-radius: 15px;\n","            padding: 20px;\n","            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.2);\n","        }\n","        \n","        .panel-header {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.2em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","            margin-bottom: 15px;\n","            text-align: center;\n","            text-shadow: 0 0 10px rgba(78, 205, 196, 0.5);\n","        }\n","        \n","        .agent-stats {\n","            display: grid;\n","            grid-template-columns: 1fr 1fr;\n","            gap: 10px;\n","            margin-bottom: 15px;\n","        }\n","        \n","        .agent-stat-card {\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            padding: 10px;\n","            border-left: 3px solid;\n","            transition: all 0.3s ease;\n","        }\n","        \n","        .agent-stat-card:nth-child(1) { border-left-color: #e74c3c; }\n","        .agent-stat-card:nth-child(2) { border-left-color: #3498db; }\n","        .agent-stat-card:nth-child(3) { border-left-color: #2ecc71; }\n","        .agent-stat-card:nth-child(4) { border-left-color: #f39c12; }\n","        \n","        .agent-stat-card:hover {\n","            background: rgba(255, 255, 255, 0.1);\n","            transform: translateY(-2px);\n","        }\n","        \n","        .stat-item {\n","            display: flex;\n","            justify-content: space-between;\n","            margin: 5px 0;\n","            font-size: 0.85em;\n","        }\n","        \n","        .stat-label {\n","            color: #bdc3c7;\n","        }\n","        \n","        .stat-value {\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","        }\n","        \n","        .emergent-behaviors {\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            padding: 15px;\n","            margin-top: 10px;\n","        }\n","        \n","        .behavior-item {\n","            background: rgba(78, 205, 196, 0.1);\n","            border-radius: 5px;\n","            padding: 8px;\n","            margin: 5px 0;\n","            font-size: 0.85em;\n","        }\n","        \n","        .system-metrics {\n","            display: grid;\n","            grid-template-columns: 1fr 1fr;\n","            gap: 10px;\n","        }\n","        \n","        .metric {\n","            text-align: center;\n","            padding: 10px;\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","        }\n","        \n","        .metric-value {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.5em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","        }\n","        \n","        .metric-label {\n","            font-size: 0.8em;\n","            color: #bdc3c7;\n","            margin-top: 5px;\n","        }\n","        \n","        @media (max-width: 1200px) {\n","            .main-container {\n","                grid-template-columns: 1fr;\n","                max-width: 100%;\n","            }\n","            \n","            .stats-section {\n","                flex-direction: row;\n","                overflow-x: auto;\n","            }\n","            \n","            .stats-panel {\n","                min-width: 300px;\n","            }\n","        }\n","    </style>\n","</head>\n","<body>\n","    <div class=\"header\">\n","         ADVANCED MULTI-AGENT GRLM \n","    </div>\n","    \n","    <div class=\"main-container\">\n","        <div class=\"world-section\">\n","            <div class=\"canvas-container\">\n","                <canvas id=\"worldCanvas\" width=\"600\" height=\"600\"></canvas>\n","            </div>\n","            <div class=\"controls\">\n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 1 (Explorer)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","                \n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 2 (Social)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","                \n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 3 (Risk-taker)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","                \n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 4 (Balanced)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","            </div>\n","            \n","            <div class=\"auto-mode\" onclick=\"toggleAutoMode()\">\n","                 AUTO MODE: OFF\n","            </div>\n","        </div>\n","        \n","        <div class=\"stats-section\">\n","            <div class=\"stats-panel\">\n","                <div class=\"panel-header\"> AGENT STATISTICS</div>\n","                <div class=\"agent-stats\" id=\"agentStats\">\n","                    Loading agent data...\n","                </div>\n","            </div>\n","            \n","            <div class=\"stats-panel\">\n","                <div class=\"panel-header\"> EMERGENT BEHAVIORS</div>\n","                <div class=\"emergent-behaviors\" id=\"emergentBehaviors\">\n","                    <div>Monitoring for emergent behaviors...</div>\n","                </div>\n","            </div>\n","            \n","            <div class=\"stats-panel\">\n","                <div class=\"panel-header\"> SYSTEM METRICS</div>\n","                <div class=\"system-metrics\" id=\"systemMetrics\">\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"complexity\">1.0</div>\n","                        <div class=\"metric-label\">Complexity</div>\n","                    </div>\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"cooperation\">0.0</div>\n","                        <div class=\"metric-label\">Cooperation</div>\n","                    </div>\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"exploration\">0%</div>\n","                        <div class=\"metric-label\">Explored</div>\n","                    </div>\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"intelligence\">0.0</div>\n","                        <div class=\"metric-label\">Collective IQ</div>\n","                    </div>\n","                </div>\n","            </div>\n","        </div>\n","    </div>\n","\n","    <script>\n","        const canvas = document.getElementById('worldCanvas');\n","        const ctx = canvas.getContext('2d');\n","        const gridSize = 20;\n","        \n","        let worldState = null;\n","        let systemStats = null;\n","        let autoMode = false;\n","        let animationFrame = null;\n","        \n","        // Agent colors\n","        const agentColors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12'];\n","        const agentNames = ['Explorer', 'Social', 'Risk-taker', 'Balanced'];\n","        \n","        function initializeSystem() {\n","            worldState = {\n","                size: 30,\n","                grid: Array(30).fill().map(() => Array(30).fill(0)),\n","                objects: {},\n","                agents: {'0': [2, 2], '1': [27, 2], '2': [2, 27], '3': [27, 27]},\n","                agent_trails: {'0': [], '1': [], '2': [], '3': []},\n","                territory_map: Array(30).fill().map(() => Array(30).fill(-1)),\n","                complexity_level: 1.0,\n","                time_step: 0\n","            };\n","            \n","            // Generate initial objects\n","            for (let i = 0; i < 100; i++) {\n","                const x = Math.floor(Math.random() * 30);\n","                const y = Math.floor(Math.random() * 30);\n","                const key = `${x},${y}`;\n","                if (!worldState.objects[key]) {\n","                    const objType = Math.random() > 0.7 ? 'treasure' : (Math.random() > 0.5 ? 'food' : 'water');\n","                    worldState.objects[key] = objType;\n","                    worldState.grid[x][y] = objType === 'treasure' ? 2.0 : (objType === 'food' ? 1.0 : 0.5);\n","                }\n","            }\n","            \n","            systemStats = {\n","                agents: [\n","                    {agent_id: 0, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 1.5}},\n","                    {agent_id: 1, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 0.8}},\n","                    {agent_id: 2, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 1.0}},\n","                    {agent_id: 3, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 1.2}}\n","                ],\n","                emergent_behaviors: 0,\n","                collective_intelligence: 0,\n","                world: {complexity_level: 1.0, time_step: 0}\n","            };\n","            \n","            updateDisplay();\n","        }\n","        \n","        function drawWorld() {\n","            ctx.clearRect(0, 0, canvas.width, canvas.height);\n","            \n","            if (!worldState) return;\n","            \n","            const size = worldState.size;\n","            \n","            // Draw grid background\n","            for (let x = 0; x < size; x++) {\n","                for (let y = 0; y < size; y++) {\n","                    const pixelX = x * gridSize;\n","                    const pixelY = y * gridSize;\n","                    \n","                    // Territory coloring\n","                    const territory = worldState.territory_map[x][y];\n","                    if (territory >= 0) {\n","                        ctx.fillStyle = agentColors[territory] + '20';\n","                        ctx.fillRect(pixelX, pixelY, gridSize, gridSize);\n","                    }\n","                    \n","                    // Grid lines\n","                    ctx.strokeStyle = 'rgba(255, 255, 255, 0.1)';\n","                    ctx.lineWidth = 0.5;\n","                    ctx.strokeRect(pixelX, pixelY, gridSize, gridSize);\n","                }\n","            }\n","            \n","            // Draw objects\n","            for (const [key, objType] of Object.entries(worldState.objects)) {\n","                const [x, y] = key.split(',').map(Number);\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","                \n","                let emoji = '';\n","                let glowColor = '';\n","                \n","                switch (objType) {\n","                    case 'wall': emoji = ''; break;\n","                    case 'food': emoji = ''; glowColor = '#e74c3c'; break;\n","                    case 'water': emoji = ''; glowColor = '#3498db'; break;\n","                    case 'treasure': emoji = ''; glowColor = '#f39c12'; break;\n","                }\n","                \n","                // Glow effect\n","                if (glowColor) {\n","                    const gradient = ctx.createRadialGradient(\n","                        pixelX + gridSize/2, pixelY + gridSize/2, 0,\n","                        pixelX + gridSize/2, pixelY + gridSize/2, gridSize\n","                    );\n","                    gradient.addColorStop(0, glowColor + '40');\n","                    gradient.addColorStop(1, 'transparent');\n","                    ctx.fillStyle = gradient;\n","                    ctx.fillRect(pixelX - 2, pixelY - 2, gridSize + 4, gridSize + 4);\n","                }\n","                \n","                // Object\n","                ctx.font = `${gridSize * 0.6}px Arial`;\n","                ctx.textAlign = 'center';\n","                ctx.textBaseline = 'middle';\n","                ctx.fillStyle = '#ffffff';\n","                ctx.fillText(emoji, pixelX + gridSize/2, pixelY + gridSize/2);\n","            }\n","            \n","            // Draw agent trails\n","            for (const [agentId, trail] of Object.entries(worldState.agent_trails)) {\n","                if (trail.length > 1) {\n","                    ctx.strokeStyle = agentColors[parseInt(agentId)] + '60';\n","                    ctx.lineWidth = 2;\n","                    ctx.beginPath();\n","                    \n","                    for (let i = 0; i < trail.length; i++) {\n","                        const pos = trail[i];\n","                        const x = (pos % worldState.size) * gridSize + gridSize/2;\n","                        const y = Math.floor(pos / worldState.size) * gridSize + gridSize/2;\n","                        \n","                        if (i === 0) ctx.moveTo(x, y);\n","                        else ctx.lineTo(x, y);\n","                    }\n","                    ctx.stroke();\n","                }\n","            }\n","            \n","            // Draw agents\n","            for (const [agentId, position] of Object.entries(worldState.agents)) {\n","                const [x, y] = position;\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","                const color = agentColors[parseInt(agentId)];\n","                \n","                // Agent glow\n","                const gradient = ctx.createRadialGradient(\n","                    pixelX + gridSize/2, pixelY + gridSize/2, 0,\n","                    pixelX + gridSize/2, pixelY + gridSize/2, gridSize\n","                );\n","                gradient.addColorStop(0, color + '60');\n","                gradient.addColorStop(1, 'transparent');\n","                ctx.fillStyle = gradient;\n","                ctx.fillRect(pixelX - 3, pixelY - 3, gridSize + 6, gridSize + 6);\n","                \n","                // Agent body\n","                ctx.fillStyle = color;\n","                ctx.beginPath();\n","                ctx.arc(pixelX + gridSize/2, pixelY + gridSize/2, gridSize/3, 0, 2 * Math.PI);\n","                ctx.fill();\n","                \n","                // Agent ID\n","                ctx.fillStyle = '#ffffff';\n","                ctx.font = `${gridSize * 0.4}px Arial`;\n","                ctx.textAlign = 'center';\n","                ctx.textBaseline = 'middle';\n","                ctx.fillText(agentId, pixelX + gridSize/2, pixelY + gridSize/2);\n","            }\n","        }\n","        \n","        function updateStats() {\n","            if (!systemStats) return;\n","            \n","            // Agent stats\n","            const agentStatsDiv = document.getElementById('agentStats');\n","            agentStatsDiv.innerHTML = systemStats.agents.map((agent, i) => `\n","                <div class=\"agent-stat-card\">\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\"> ${agentNames[i]}</span>\n","                        <span class=\"stat-value\">#${agent.agent_id}</span>\n","                    </div>\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\">Reward</span>\n","                        <span class=\"stat-value\">${agent.total_reward.toFixed(1)}</span>\n","                    </div>\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\">Avg/Step</span>\n","                        <span class=\"stat-value\">${agent.avg_reward_per_step.toFixed(2)}</span>\n","                    </div>\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\">Curiosity</span>\n","                        <span class=\"stat-value\">${agent.intrinsic_motivation.toFixed(2)}</span>\n","                    </div>\n","                </div>\n","            `).join('');\n","            \n","            // System metrics\n","            document.getElementById('complexity').textContent = systemStats.world.complexity_level.toFixed(1);\n","            document.getElementById('cooperation').textContent = systemStats.system_performance?.average_cooperation?.toFixed(2) || '0.0';\n","            document.getElementById('exploration').textContent = '15%'; // Placeholder\n","            document.getElementById('intelligence').textContent = systemStats.collective_intelligence.toFixed(1);\n","            \n","            // Emergent behaviors\n","            const behaviorsDiv = document.getElementById('emergentBehaviors');\n","            if (systemStats.emergent_behaviors > 0) {\n","                behaviorsDiv.innerHTML = `\n","                    <div class=\"behavior-item\"> Clustering Behavior Detected</div>\n","                    <div class=\"behavior-item\"> Cooperative Foraging</div>\n","                    <div class=\"behavior-item\"> Territory Formation</div>\n","                `;\n","            } else {\n","                behaviorsDiv.innerHTML = '<div>Monitoring for emergent behaviors...</div>';\n","            }\n","        }\n","        \n","        function updateDisplay() {\n","            drawWorld();\n","            updateStats();\n","        }\n","        \n","        function moveAgent(agentId, dx, dy) {\n","            if (!worldState || !worldState.agents[agentId]) return;\n","            \n","            const [oldX, oldY] = worldState.agents[agentId];\n","            const newX = Math.max(0, Math.min(worldState.size - 1, oldX + dx));\n","            const newY = Math.max(0, Math.min(worldState.size - 1, oldY + dy));\n","            \n","            // Check for collisions\n","            const occupied = Object.values(worldState.agents).some(pos => pos[0] === newX && pos[1] === newY);\n","            const isWall = worldState.grid[newX][newY] === -1;\n","            \n","            if (!occupied && !isWall) {\n","                worldState.agents[agentId] = [newX, newY];\n","                \n","                // Add to trail\n","                const trailPos = oldX * worldState.size + oldY;\n","                worldState.agent_trails[agentId].push(trailPos);\n","                if (worldState.agent_trails[agentId].length > 15) {\n","                    worldState.agent_trails[agentId].shift();\n","                }\n","                \n","                // Object interaction\n","                const objKey = `${newX},${newY}`;\n","                if (worldState.objects[objKey] && worldState.objects[objKey] !== 'wall') {\n","                    const objType = worldState.objects[objKey];\n","                    let reward = 0;\n","                    \n","                    switch (objType) {\n","                        case 'treasure': reward = 3; break;\n","                        case 'food': reward = 1.5; break;\n","                        case 'water': reward = 0.8; break;\n","                    }\n","                    \n","                    // Update agent stats\n","                    systemStats.agents[agentId].total_reward += reward;\n","                    systemStats.agents[agentId].avg_reward_per_step = systemStats.agents[agentId].total_reward / (systemStats.world.time_step + 1);\n","                    systemStats.agents[agentId].intrinsic_motivation += Math.random() * 0.5;\n","                    \n","                    // Remove object\n","                    delete worldState.objects[objKey];\n","                    worldState.grid[newX][newY] = 0;\n","                }\n","                \n","                // Update territory\n","                worldState.territory_map[newX][newY] = agentId;\n","                \n","                // Update system stats\n","                systemStats.world.time_step++;\n","                systemStats.world.complexity_level = 1.0 + systemStats.world.time_step * 0.001;\n","                systemStats.collective_intelligence = systemStats.agents.reduce((sum, a) => sum + a.intrinsic_motivation, 0) / 4;\n","                \n","                if (Math.random() < 0.1) systemStats.emergent_behaviors++;\n","            }\n","            \n","            updateDisplay();\n","        }\n","        \n","        function toggleAutoMode() {\n","            autoMode = !autoMode;\n","            document.querySelector('.auto-mode').textContent = ` AUTO MODE: ${autoMode ? 'ON' : 'OFF'}`;\n","            document.querySelector('.auto-mode').style.background = autoMode ? \n","                'linear-gradient(145deg, #e74c3c, #c0392b)' : 'linear-gradient(145deg, #2ecc71, #27ae60)';\n","            \n","            if (autoMode) {\n","                runAutoMode();\n","            }\n","        }\n","        \n","        function runAutoMode() {\n","            if (!autoMode) return;\n","            \n","            // Move each agent randomly\n","            for (let i = 0; i < 4; i++) {\n","                const moves = [[-1, 0], [1, 0], [0, -1], [0, 1], [0, 0]];\n","                const [dx, dy] = moves[Math.floor(Math.random() * moves.length)];\n","                moveAgent(i, dx, dy);\n","            }\n","            \n","            setTimeout(runAutoMode, 500); // Auto-move every 500ms\n","        }\n","        \n","        // Keyboard controls for Agent 0\n","        document.addEventListener('keydown', (e) => {\n","            if (autoMode) return;\n","            \n","            switch(e.key.toLowerCase()) {\n","                case 'w': moveAgent(0, 0, -1); break;\n","                case 's': moveAgent(0, 0, 1); break;\n","                case 'a': moveAgent(0, -1, 0); break;\n","                case 'd': moveAgent(0, 1, 0); break;\n","                case ' ': toggleAutoMode(); break;\n","            }\n","            e.preventDefault();\n","        });\n","        \n","        // Initialize\n","        initializeSystem();\n","        \n","        console.log(' Advanced Multi-Agent GRLM System Initialized');\n","        console.log(' 4 agents with different personalities');\n","        console.log(' Curiosity-driven exploration');\n","        console.log(' Neural communication');\n","        console.log(' Dynamic world with territories');\n","        console.log('  WASD to control Agent 0, Space for auto-mode');\n","    </script>\n","</body>\n","</html>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"," Advanced Multi-Agent GRLM is running!\n","Watch emergent behaviors develop as agents explore and communicate!\n"]}],"source":["# @title\n","# Advanced GRLM Enhancements - Multi-Agent Curiosity-Driven Learning\n","# Computationally intensive features for research-grade performance\n","\n","import os\n","import math\n","import time\n","import json\n","import random\n","import tempfile\n","import warnings\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Optional, Tuple, Dict, List\n","import base64\n","from IPython.display import HTML, display\n","import threading\n","import queue\n","from collections import deque\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions import Categorical, Normal\n","\n","# ============================================================================\n","# ENHANCED CONFIGURATION FOR COMPUTE-INTENSIVE FEATURES\n","# ============================================================================\n","\n","if torch.cuda.is_available():\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    torch.set_float32_matmul_precision(\"high\")\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","USE_AMP = torch.cuda.is_available()\n","AMP_DTYPE = torch.bfloat16\n","\n","# Enhanced parameters for compute-intensive features\n","SEED = 1337\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","WORLD_SIZE = 30\n","GRID_SIZE = 20\n","EMB_DIM = 256  # Increased for richer representations\n","MAX_NODES = 50000\n","NUM_AGENTS = 4  # Multiple agents\n","CURIOSITY_HORIZON = 100  # Steps for curiosity calculation\n","\n","print(f\" Advanced Multi-Agent GRLM System Starting\")\n","print(f\"Device: {DEVICE} | Agents: {NUM_AGENTS} | World: {WORLD_SIZE}x{WORLD_SIZE}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n","\n","# ============================================================================\n","# 1. CURIOSITY-DRIVEN INTRINSIC MOTIVATION\n","# ============================================================================\n","\n","class CuriosityModule(nn.Module):\n","    \"\"\"Intrinsic Curiosity Module (ICM) for exploration.\"\"\"\n","\n","    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n","        super().__init__()\n","\n","        # Feature encoder\n","        self.feature_encoder = nn.Sequential(\n","            nn.Linear(state_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim // 2, hidden_dim // 4)\n","        )\n","\n","        # Forward model: predicts next state features from current features + action\n","        self.forward_model = nn.Sequential(\n","            nn.Linear(hidden_dim // 4 + action_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim // 2, hidden_dim // 4)\n","        )\n","\n","        # Inverse model: predicts action from current and next state features\n","        self.inverse_model = nn.Sequential(\n","            nn.Linear(hidden_dim // 2, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim // 2, action_dim)\n","        )\n","\n","    def forward(self, state: torch.Tensor, action: torch.Tensor, next_state: torch.Tensor):\n","        \"\"\"Compute curiosity-driven intrinsic reward.\"\"\"\n","\n","        # Encode states to features\n","        state_features = self.feature_encoder(state)\n","        next_state_features = self.feature_encoder(next_state)\n","\n","        # Forward model prediction\n","        forward_input = torch.cat([state_features, action], dim=-1)\n","        predicted_next_features = self.forward_model(forward_input)\n","\n","        # Inverse model prediction\n","        inverse_input = torch.cat([state_features, next_state_features], dim=-1)\n","        predicted_action = self.inverse_model(inverse_input)\n","\n","        # Compute intrinsic reward (prediction error)\n","        forward_loss = F.mse_loss(predicted_next_features, next_state_features, reduction='none')\n","        intrinsic_reward = forward_loss.mean(dim=-1)\n","\n","        # Compute losses for training\n","        inverse_loss = F.mse_loss(predicted_action, action)\n","\n","        return intrinsic_reward, forward_loss.mean(), inverse_loss\n","\n","class NoveltyModule(nn.Module):\n","    \"\"\"Random Network Distillation for novelty-based exploration.\"\"\"\n","\n","    def __init__(self, state_dim: int, hidden_dim: int = 256):\n","        super().__init__()\n","\n","        # Random target network (never trained)\n","        self.target_network = nn.Sequential(\n","            nn.Linear(state_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim // 2, hidden_dim // 4)\n","        )\n","\n","        # Predictor network (trained to match target)\n","        self.predictor_network = nn.Sequential(\n","            nn.Linear(state_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim // 2, hidden_dim // 4)\n","        )\n","\n","        # Freeze target network\n","        for param in self.target_network.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, state: torch.Tensor):\n","        \"\"\"Compute novelty score.\"\"\"\n","        with torch.no_grad():\n","            target_features = self.target_network(state)\n","\n","        predicted_features = self.predictor_network(state)\n","        novelty_score = F.mse_loss(predicted_features, target_features, reduction='none').mean(dim=-1)\n","\n","        return novelty_score, F.mse_loss(predicted_features, target_features)\n","\n","# ============================================================================\n","# 2. ADVANCED MEMORY WITH EPISODIC REPLAY\n","# ============================================================================\n","\n","class EpisodicMemory:\n","    \"\"\"Advanced episodic memory with prioritized experience replay.\"\"\"\n","\n","    def __init__(self, capacity: int = 10000, alpha: float = 0.6):\n","        self.capacity = capacity\n","        self.alpha = alpha  # Prioritization exponent\n","        self.beta = 0.4     # Importance sampling exponent (annealed to 1)\n","        self.epsilon = 1e-6  # Small constant for numerical stability\n","\n","        self.buffer = []\n","        self.priorities = np.zeros(capacity)\n","        self.position = 0\n","        self.max_priority = 1.0\n","\n","    def add_experience(self, state: np.ndarray, action: np.ndarray, reward: float,\n","                      next_state: np.ndarray, done: bool, prediction_error: float = 1.0):\n","        \"\"\"Add experience with priority based on prediction error.\"\"\"\n","\n","        experience = {\n","            'state': state,\n","            'action': action,\n","            'reward': reward,\n","            'next_state': next_state,\n","            'done': done,\n","            'timestamp': time.time()\n","        }\n","\n","        priority = (abs(prediction_error) + self.epsilon) ** self.alpha\n","\n","        if len(self.buffer) < self.capacity:\n","            self.buffer.append(experience)\n","        else:\n","            self.buffer[self.position] = experience\n","\n","        self.priorities[self.position] = priority\n","        self.max_priority = max(self.max_priority, priority)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample_batch(self, batch_size: int = 32):\n","        \"\"\"Sample batch with prioritized experience replay.\"\"\"\n","        if len(self.buffer) < batch_size:\n","            return None\n","\n","        priorities = self.priorities[:len(self.buffer)]\n","        probabilities = priorities / priorities.sum()\n","\n","        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n","\n","        # Importance sampling weights\n","        weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)\n","        weights /= weights.max()\n","\n","        batch = [self.buffer[idx] for idx in indices]\n","\n","        return {\n","            'batch': batch,\n","            'indices': indices,\n","            'weights': weights\n","        }\n","\n","    def update_priorities(self, indices: np.ndarray, td_errors: np.ndarray):\n","        \"\"\"Update priorities based on TD errors.\"\"\"\n","        for idx, error in zip(indices, td_errors):\n","            self.priorities[idx] = (abs(error) + self.epsilon) ** self.alpha\n","\n","class SemanticMemory:\n","    \"\"\"Hierarchical semantic memory with concept formation.\"\"\"\n","\n","    def __init__(self, embedding_dim: int, max_concepts: int = 1000):\n","        self.embedding_dim = embedding_dim\n","        self.max_concepts = max_concepts\n","        self.concepts = []  # List of concept centroids\n","        self.concept_counts = []  # Usage frequency\n","        self.concept_rewards = []  # Average rewards\n","        self.concept_threshold = 0.3  # Similarity threshold for concept formation\n","\n","    def add_experience(self, embedding: np.ndarray, reward: float):\n","        \"\"\"Add experience and potentially form new concepts.\"\"\"\n","        embedding = embedding.flatten()\n","\n","        if len(self.concepts) == 0:\n","            # First concept\n","            self.concepts.append(embedding.copy())\n","            self.concept_counts.append(1)\n","            self.concept_rewards.append(reward)\n","            return 0\n","\n","        # Find closest concept\n","        similarities = [np.dot(embedding, concept) / (np.linalg.norm(embedding) * np.linalg.norm(concept))\n","                       for concept in self.concepts]\n","        max_sim_idx = np.argmax(similarities)\n","        max_similarity = similarities[max_sim_idx]\n","\n","        if max_similarity > self.concept_threshold:\n","            # Update existing concept\n","            count = self.concept_counts[max_sim_idx]\n","            self.concepts[max_sim_idx] = (self.concepts[max_sim_idx] * count + embedding) / (count + 1)\n","            self.concept_counts[max_sim_idx] += 1\n","            self.concept_rewards[max_sim_idx] = (self.concept_rewards[max_sim_idx] * count + reward) / (count + 1)\n","            return max_sim_idx\n","        else:\n","            # Create new concept\n","            if len(self.concepts) < self.max_concepts:\n","                self.concepts.append(embedding.copy())\n","                self.concept_counts.append(1)\n","                self.concept_rewards.append(reward)\n","                return len(self.concepts) - 1\n","            else:\n","                # Replace least used concept\n","                min_count_idx = np.argmin(self.concept_counts)\n","                self.concepts[min_count_idx] = embedding.copy()\n","                self.concept_counts[min_count_idx] = 1\n","                self.concept_rewards[min_count_idx] = reward\n","                return min_count_idx\n","\n","    def get_concept_map(self):\n","        \"\"\"Get current concept map for visualization.\"\"\"\n","        return {\n","            'concepts': len(self.concepts),\n","            'avg_reward_per_concept': np.mean(self.concept_rewards) if self.concept_rewards else 0,\n","            'concept_diversity': len(set(np.round(np.array(self.concepts), 2).tobytes() for c in self.concepts[:10])) if self.concepts else 0\n","        }\n","\n","# ============================================================================\n","# 3. MULTI-AGENT SYSTEM WITH EMERGENT BEHAVIORS\n","# ============================================================================\n","\n","class MultiAgentCommunication(nn.Module):\n","    \"\"\"Neural communication between agents.\"\"\"\n","\n","    def __init__(self, state_dim: int, message_dim: int = 64):\n","        super().__init__()\n","        self.message_dim = message_dim\n","\n","        # Message encoder\n","        self.message_encoder = nn.Sequential(\n","            nn.Linear(state_dim, message_dim * 2),\n","            nn.ReLU(),\n","            nn.Linear(message_dim * 2, message_dim)\n","        )\n","\n","        # Message attention\n","        self.attention = nn.MultiheadAttention(message_dim, num_heads=4, batch_first=True)\n","\n","        # Message integration\n","        self.integrator = nn.Sequential(\n","            nn.Linear(state_dim + message_dim, state_dim),\n","            nn.ReLU(),\n","            nn.Linear(state_dim, state_dim)\n","        )\n","\n","    def forward(self, agent_states: torch.Tensor, agent_positions: torch.Tensor):\n","        \"\"\"Process inter-agent communication.\"\"\"\n","        num_agents = agent_states.shape[0]\n","\n","        # Generate messages\n","        messages = self.message_encoder(agent_states)  # [num_agents, message_dim]\n","\n","        # Calculate spatial attention weights\n","        distances = torch.cdist(agent_positions.float(), agent_positions.float())\n","        spatial_weights = torch.exp(-distances / 5.0)  # Communication range\n","        spatial_weights.fill_diagonal_(0)  # No self-communication\n","\n","        # Attend to relevant messages\n","        attended_messages, _ = self.attention(\n","            messages.unsqueeze(0),\n","            messages.unsqueeze(0),\n","            messages.unsqueeze(0)\n","        )\n","        attended_messages = attended_messages.squeeze(0)\n","\n","        # Weight by spatial proximity\n","        weighted_messages = attended_messages * spatial_weights.unsqueeze(-1)\n","        aggregated_messages = weighted_messages.sum(dim=1) / (spatial_weights.sum(dim=1, keepdim=True) + 1e-8)\n","\n","        # Integrate messages with agent states\n","        enhanced_states = self.integrator(torch.cat([agent_states, aggregated_messages], dim=-1))\n","\n","        return enhanced_states, aggregated_messages\n","\n","@dataclass\n","class Agent:\n","    \"\"\"Enhanced agent with curiosity and communication.\"\"\"\n","    agent_id: int\n","    position: Tuple[int, int]\n","    world_model: nn.Module\n","    curiosity_module: CuriosityModule\n","    novelty_module: NoveltyModule\n","    episodic_memory: EpisodicMemory\n","    semantic_memory: SemanticMemory\n","\n","    # Agent-specific stats\n","    total_reward: float = 0.0\n","    total_steps: int = 0\n","    intrinsic_reward_sum: float = 0.0\n","    novelty_sum: float = 0.0\n","    exploration_bonus: float = 0.0\n","\n","    # Agent personality (affects exploration behavior)\n","    curiosity_weight: float = 1.0\n","    social_weight: float = 1.0\n","    risk_tolerance: float = 1.0\n","\n","# ============================================================================\n","# 4. DYNAMIC WORLD WITH EMERGENT COMPLEXITY\n","# ============================================================================\n","\n","class DynamicWorld:\n","    \"\"\"Dynamic world with evolving challenges and emergent complexity.\"\"\"\n","\n","    def __init__(self, size: int = WORLD_SIZE, num_agents: int = NUM_AGENTS):\n","        self.size = size\n","        self.num_agents = num_agents\n","        self.grid = np.zeros((size, size), dtype=np.float32)\n","        self.objects = {}\n","        self.agents = {}  # agent_id -> position\n","        self.agent_trails = {i: deque(maxlen=20) for i in range(num_agents)}\n","\n","        # Dynamic elements\n","        self.resource_regeneration_rate = 0.02\n","        self.complexity_level = 1.0\n","        self.environmental_pressure = 0.0\n","        self.time_step = 0\n","\n","        # Emergent phenomena tracking\n","        self.territory_map = np.zeros((size, size), dtype=np.int32) - 1  # -1 = unclaimed\n","        self.resource_density = np.zeros((size, size), dtype=np.float32)\n","\n","        self._initialize_world()\n","\n","    def _initialize_world(self):\n","        \"\"\"Initialize dynamic world with complexity scaling.\"\"\"\n","        # Place agents in different corners\n","        corners = [(2, 2), (self.size-3, 2), (2, self.size-3), (self.size-3, self.size-3)]\n","        for i in range(self.num_agents):\n","            self.agents[i] = corners[i % len(corners)]\n","\n","        # Generate resource clusters\n","        num_clusters = int(3 + self.complexity_level * 2)\n","        for _ in range(num_clusters):\n","            center_x, center_y = np.random.randint(5, self.size-5, 2)\n","            cluster_size = np.random.randint(3, 8)\n","\n","            for _ in range(cluster_size):\n","                x = center_x + np.random.randint(-3, 4)\n","                y = center_y + np.random.randint(-3, 4)\n","                if 0 <= x < self.size and 0 <= y < self.size:\n","                    obj_type = np.random.choice(['food', 'water', 'treasure'], p=[0.5, 0.3, 0.2])\n","                    self.objects[(x, y)] = obj_type\n","                    if obj_type == 'treasure':\n","                        self.grid[x, y] = 2.0 * self.complexity_level\n","                        self.resource_density[x, y] = 2.0\n","                    elif obj_type == 'food':\n","                        self.grid[x, y] = 1.0 * self.complexity_level\n","                        self.resource_density[x, y] = 1.0\n","                    elif obj_type == 'water':\n","                        self.grid[x, y] = 0.5 * self.complexity_level\n","                        self.resource_density[x, y] = 0.5\n","\n","        # Add dynamic obstacles\n","        num_obstacles = int(self.size * 0.5 * self.complexity_level)\n","        for _ in range(num_obstacles):\n","            x, y = np.random.randint(0, self.size, 2)\n","            if (x, y) not in self.objects and (x, y) not in self.agents.values():\n","                self.objects[(x, y)] = 'wall'\n","                self.grid[x, y] = -1.0\n","\n","    def update_world_dynamics(self):\n","        \"\"\"Update world state with emergent dynamics.\"\"\"\n","        self.time_step += 1\n","\n","        # Resource regeneration\n","        if np.random.random() < self.resource_regeneration_rate:\n","            self._regenerate_resources()\n","\n","        # Increase complexity over time\n","        self.complexity_level = 1.0 + self.time_step * 0.001\n","\n","        # Update territorial control\n","        self._update_territory_map()\n","\n","        # Environmental pressure based on agent density\n","        agent_positions = list(self.agents.values())\n","        if len(agent_positions) > 1:\n","            distances = []\n","            for i, pos1 in enumerate(agent_positions):\n","                for j, pos2 in enumerate(agent_positions[i+1:], i+1):\n","                    dist = np.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)\n","                    distances.append(dist)\n","            avg_distance = np.mean(distances) if distances else self.size\n","            self.environmental_pressure = max(0, 1.0 - avg_distance / (self.size * 0.5))\n","\n","    def _regenerate_resources(self):\n","        \"\"\"Regenerate resources in areas with low density.\"\"\"\n","        low_density_areas = np.where(self.resource_density < 0.5)\n","        if len(low_density_areas[0]) > 0:\n","            idx = np.random.randint(len(low_density_areas[0]))\n","            x, y = low_density_areas[0][idx], low_density_areas[1][idx]\n","\n","            if (x, y) not in self.objects:\n","                obj_type = np.random.choice(['food', 'water'])\n","                self.objects[(x, y)] = obj_type\n","                if obj_type == 'food':\n","                    self.grid[x, y] = 1.0\n","                    self.resource_density[x, y] = 1.0\n","                elif obj_type == 'water':\n","                    self.grid[x, y] = 0.5\n","                    self.resource_density[x, y] = 0.5\n","\n","    def _update_territory_map(self):\n","        \"\"\"Update territorial control based on agent presence.\"\"\"\n","        # Decay existing territories\n","        self.territory_map = np.where(self.territory_map >= 0, self.territory_map * 0.95, -1)\n","\n","        # Update based on current agent positions and trails\n","        for agent_id, position in self.agents.items():\n","            x, y = position\n","            # Strong control at current position\n","            if 0 <= x < self.size and 0 <= y < self.size:\n","                self.territory_map[x, y] = agent_id\n","\n","                # Weaker control in surrounding area\n","                for dx in [-1, 0, 1]:\n","                    for dy in [-1, 0, 1]:\n","                        nx, ny = x + dx, y + dy\n","                        if 0 <= nx < self.size and 0 <= ny < self.size:\n","                            if self.territory_map[nx, ny] < 0:\n","                                self.territory_map[nx, ny] = agent_id * 0.5\n","\n","    def move_agent(self, agent_id: int, dx: int, dy: int) -> Tuple[Tuple[int, int], float, Dict]:\n","        \"\"\"Move agent with enhanced dynamics.\"\"\"\n","        if agent_id not in self.agents:\n","            return (0, 0), -10.0, {}\n","\n","        old_x, old_y = self.agents[agent_id]\n","        new_x = max(0, min(self.size - 1, old_x + dx))\n","        new_y = max(0, min(self.size - 1, old_y + dy))\n","        new_pos = (new_x, new_y)\n","\n","        # Check for collisions with other agents\n","        occupied_by_other = any(pos == new_pos for aid, pos in self.agents.items() if aid != agent_id)\n","\n","        if not self.is_valid_position(new_pos) or occupied_by_other:\n","            # Invalid move\n","            reward = -1.0 - self.environmental_pressure\n","            info = {'collision': True, 'type': 'wall' if not self.is_valid_position(new_pos) else 'agent'}\n","            return self.agents[agent_id], reward, info\n","\n","        # Valid move\n","        self.agents[agent_id] = new_pos\n","        self.agent_trails[agent_id].append(old_x * self.size + old_y)\n","\n","        # Base reward\n","        reward = self.grid[new_x, new_y]\n","\n","        # Territory bonus/penalty\n","        territory_bonus = 0.0\n","        if self.territory_map[new_x, new_y] == agent_id:\n","            territory_bonus = 0.2  # Bonus for own territory\n","        elif self.territory_map[new_x, new_y] >= 0 and self.territory_map[new_x, new_y] != agent_id:\n","            territory_bonus = -0.1  # Penalty for trespassing\n","\n","        # Resource interaction\n","        if (new_x, new_y) in self.objects:\n","            obj_type = self.objects[(new_x, new_y)]\n","            if obj_type != 'wall':\n","                if obj_type == 'treasure':\n","                    reward += 3.0 * self.complexity_level\n","                elif obj_type == 'food':\n","                    reward += 1.5 * self.complexity_level\n","                elif obj_type == 'water':\n","                    reward += 0.8 * self.complexity_level\n","\n","                # Remove consumed resource\n","                del self.objects[(new_x, new_y)]\n","                self.grid[new_x, new_y] = 0.0\n","                self.resource_density[new_x, new_y] *= 0.5\n","\n","        total_reward = reward + territory_bonus\n","\n","        info = {\n","            'territory_bonus': territory_bonus,\n","            'environmental_pressure': self.environmental_pressure,\n","            'complexity_level': self.complexity_level,\n","            'resource_consumed': (new_x, new_y) in self.objects\n","        }\n","\n","        return new_pos, total_reward, info\n","\n","    def is_valid_position(self, position: Tuple[int, int]) -> bool:\n","        \"\"\"Check if position is valid.\"\"\"\n","        x, y = position\n","        if not (0 <= x < self.size and 0 <= y < self.size):\n","            return False\n","        return self.grid[x, y] != -1.0\n","\n","    def get_local_environment(self, position: Tuple[int, int], radius: int = 2) -> np.ndarray:\n","        \"\"\"Get rich local environment representation.\"\"\"\n","        x, y = position\n","        features = []\n","\n","        # Sample in a grid pattern\n","        for dx in range(-radius, radius + 1):\n","            for dy in range(-radius, radius + 1):\n","                nx, ny = x + dx, y + dy\n","                if 0 <= nx < self.size and 0 <= ny < self.size:\n","                    # Grid value\n","                    features.append(self.grid[nx, ny])\n","                    # Territory information\n","                    features.append(self.territory_map[nx, ny] / self.num_agents)\n","                    # Resource density\n","                    features.append(self.resource_density[nx, ny])\n","                else:\n","                    features.extend([-2.0, -1.0, 0.0])  # Out of bounds\n","\n","        return np.array(features[:24])  # Limit to fixed size\n","\n","    def get_enhanced_state(self) -> Dict:\n","        \"\"\"Get complete world state for visualization.\"\"\"\n","        return {\n","            'grid': self.grid.tolist(),\n","            'objects': {f\"{x},{y}\": obj_type for (x, y), obj_type in self.objects.items()},\n","            'agents': {str(aid): pos for aid, pos in self.agents.items()},\n","            'agent_trails': {str(aid): list(trail) for aid, trail in self.agent_trails.items()},\n","            'territory_map': self.territory_map.tolist(),\n","            'resource_density': self.resource_density.tolist(),\n","            'complexity_level': self.complexity_level,\n","            'environmental_pressure': self.environmental_pressure,\n","            'size': self.size,\n","            'time_step': self.time_step\n","        }\n","\n","# ============================================================================\n","# 5. INTEGRATED ADVANCED SYSTEM\n","# ============================================================================\n","\n","class AdvancedMultiAgentGRLM:\n","    \"\"\"Complete advanced multi-agent system with all enhancements.\"\"\"\n","\n","    def __init__(self, num_agents: int = NUM_AGENTS):\n","        self.num_agents = num_agents\n","        self.world = DynamicWorld(WORLD_SIZE, num_agents)\n","\n","        # Create agents with diverse personalities\n","        self.agents = []\n","        personalities = [\n","            {'curiosity_weight': 1.5, 'social_weight': 0.8, 'risk_tolerance': 1.2},  # Explorer\n","            {'curiosity_weight': 0.8, 'social_weight': 1.5, 'risk_tolerance': 0.8},  # Social\n","            {'curiosity_weight': 1.0, 'social_weight': 1.0, 'risk_tolerance': 1.5},  # Risk-taker\n","            {'curiosity_weight': 1.2, 'social_weight': 1.2, 'risk_tolerance': 0.6}   # Balanced\n","        ]\n","\n","        for i in range(num_agents):\n","            personality = personalities[i % len(personalities)]\n","            agent = self._create_agent(i, personality)\n","            self.agents.append(agent)\n","\n","        # Communication system\n","        self.communication = MultiAgentCommunication(24, 64).to(DEVICE)\n","        self.comm_optimizer = torch.optim.AdamW(self.communication.parameters(), lr=1e-4)\n","\n","        # Global statistics\n","        self.global_stats = {\n","            'total_interactions': 0,\n","            'emergent_behaviors': [],\n","            'collective_intelligence': 0.0,\n","            'system_complexity': 1.0\n","        }\n","\n","        print(f\" Advanced Multi-Agent GRLM initialized with {num_agents} agents\")\n","        print(f\" Each agent has curiosity, novelty detection, and episodic memory\")\n","        print(f\" Dynamic world with territorial control and resource regeneration\")\n","        print(f\" Neural communication between agents\")\n","\n","    def _create_agent(self, agent_id: int, personality: Dict) -> Agent:\n","        \"\"\"Create an enhanced agent.\"\"\"\n","        # Enhanced world model\n","        world_model = self._create_enhanced_world_model().to(DEVICE)\n","\n","        # Curiosity and novelty modules\n","        curiosity = CuriosityModule(state_dim=24, action_dim=2, hidden_dim=128).to(DEVICE)\n","        novelty = NoveltyModule(state_dim=24, hidden_dim=128).to(DEVICE)\n","\n","        # Memory systems\n","        episodic_memory = EpisodicMemory(capacity=5000)\n","        semantic_memory = SemanticMemory(embedding_dim=EMB_DIM, max_concepts=500)\n","\n","        position = self.world.agents[agent_id]\n","\n","        agent = Agent(\n","            agent_id=agent_id,\n","            position=position,\n","            world_model=world_model,\n","            curiosity_module=curiosity,\n","            novelty_module=novelty,\n","            episodic_memory=episodic_memory,\n","            semantic_memory=semantic_memory,\n","            **personality\n","        )\n","\n","        return agent\n","\n","    def _create_enhanced_world_model(self) -> nn.Module:\n","        \"\"\"Create enhanced world model with uncertainty quantification.\"\"\"\n","        return nn.Sequential(\n","            nn.Linear(24, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32)\n","        )\n","\n","    def step_all_agents(self, actions: List[Tuple[int, int]]) -> Dict:\n","        \"\"\"Execute one step for all agents simultaneously.\"\"\"\n","        if len(actions) != self.num_agents:\n","            raise ValueError(f\"Expected {self.num_agents} actions, got {len(actions)}\")\n","\n","        # Update world dynamics\n","        self.world.update_world_dynamics()\n","\n","        # Get current states for all agents\n","        current_states = []\n","        agent_positions = []\n","\n","        for agent in self.agents:\n","            pos = self.world.agents[agent.agent_id]\n","            local_env = self.world.get_local_environment(pos)\n","            current_states.append(torch.tensor(local_env, dtype=torch.float32, device=DEVICE))\n","            agent_positions.append(torch.tensor(pos, dtype=torch.float32, device=DEVICE))\n","\n","        current_states = torch.stack(current_states)\n","        agent_positions = torch.stack(agent_positions)\n","\n","        # Process communication\n","        enhanced_states, messages = self.communication(current_states, agent_positions)\n","\n","        # Execute actions and collect results\n","        results = []\n","        total_intrinsic_reward = 0.0\n","\n","        for i, (agent, action) in enumerate(zip(self.agents, actions)):\n","            dx, dy = action\n","\n","            # Get old state\n","            old_pos = agent.position\n","            old_local_env = self.world.get_local_environment(old_pos)\n","            old_state = torch.tensor(old_local_env, dtype=torch.float32, device=DEVICE)\n","\n","            # Execute move\n","            new_pos, extrinsic_reward, info = self.world.move_agent(agent.agent_id, dx, dy)\n","            agent.position = new_pos\n","\n","            # Get new state\n","            new_local_env = self.world.get_local_environment(new_pos)\n","            new_state = torch.tensor(new_local_env, dtype=torch.float32, device=DEVICE)\n","\n","            # Compute intrinsic motivation\n","            action_tensor = torch.tensor([dx, dy], dtype=torch.float32, device=DEVICE)\n","\n","            with torch.no_grad():\n","                # Curiosity-driven reward\n","                intrinsic_reward, forward_loss, inverse_loss = agent.curiosity_module(\n","                    old_state.unsqueeze(0), action_tensor.unsqueeze(0), new_state.unsqueeze(0)\n","                )\n","                intrinsic_reward = intrinsic_reward.item() * agent.curiosity_weight\n","\n","                # Novelty-driven reward\n","                novelty_score, novelty_loss = agent.novelty_module(new_state.unsqueeze(0))\n","                novelty_reward = novelty_score.item() * 0.5\n","\n","                total_intrinsic = intrinsic_reward + novelty_reward\n","                total_intrinsic_reward += total_intrinsic\n","\n","            # Update agent stats\n","            agent.total_steps += 1\n","            agent.total_reward += extrinsic_reward\n","            agent.intrinsic_reward_sum += total_intrinsic\n","            agent.novelty_sum += novelty_reward\n","\n","            # Add to episodic memory\n","            prediction_error = float(forward_loss.item()) if 'forward_loss' in locals() else 1.0\n","            agent.episodic_memory.add_experience(\n","                old_local_env, np.array([dx, dy]), extrinsic_reward,\n","                new_local_env, False, prediction_error\n","            )\n","\n","            # Add to semantic memory\n","            world_state_embedding = enhanced_states[i].detach().cpu().numpy()\n","            agent.semantic_memory.add_experience(world_state_embedding, extrinsic_reward)\n","\n","            results.append({\n","                'agent_id': agent.agent_id,\n","                'old_position': old_pos,\n","                'new_position': new_pos,\n","                'extrinsic_reward': extrinsic_reward,\n","                'intrinsic_reward': total_intrinsic,\n","                'total_reward': extrinsic_reward + total_intrinsic,\n","                'novelty_score': novelty_reward,\n","                'info': info,\n","                'message': messages[i].detach().cpu().numpy() if messages is not None else None\n","            })\n","\n","        # Update global statistics\n","        self.global_stats['total_interactions'] += 1\n","        self.global_stats['collective_intelligence'] = total_intrinsic_reward / self.num_agents\n","        self.global_stats['system_complexity'] = self.world.complexity_level\n","\n","        # Detect emergent behaviors\n","        self._detect_emergent_behaviors(results)\n","\n","        return {\n","            'agent_results': results,\n","            'world_state': self.world.get_enhanced_state(),\n","            'global_stats': self.global_stats,\n","            'communication_efficiency': float(torch.mean(torch.norm(messages, dim=-1)).item()) if messages is not None else 0.0\n","        }\n","\n","    def _detect_emergent_behaviors(self, results: List[Dict]):\n","        \"\"\"Detect emergent behaviors in agent interactions.\"\"\"\n","        positions = [r['new_position'] for r in results]\n","        rewards = [r['total_reward'] for r in results]\n","\n","        # Clustering behavior\n","        if len(positions) > 1:\n","            distances = []\n","            for i, pos1 in enumerate(positions):\n","                for j, pos2 in enumerate(positions[i+1:], i+1):\n","                    dist = np.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)\n","                    distances.append(dist)\n","\n","            avg_distance = np.mean(distances)\n","            if avg_distance < 3.0:  # Agents are clustering\n","                self.global_stats['emergent_behaviors'].append({\n","                    'type': 'clustering',\n","                    'strength': 1.0 / (avg_distance + 0.1),\n","                    'timestamp': self.world.time_step\n","                })\n","\n","        # Cooperative behavior (high collective reward)\n","        collective_reward = sum(rewards)\n","        if collective_reward > 5.0:\n","            self.global_stats['emergent_behaviors'].append({\n","                'type': 'cooperation',\n","                'strength': collective_reward,\n","                'timestamp': self.world.time_step\n","            })\n","\n","        # Keep only recent behaviors\n","        self.global_stats['emergent_behaviors'] = [\n","            b for b in self.global_stats['emergent_behaviors']\n","            if self.world.time_step - b['timestamp'] < 100\n","        ]\n","\n","    def get_comprehensive_stats(self) -> Dict:\n","        \"\"\"Get comprehensive system statistics.\"\"\"\n","        agent_stats = []\n","        for agent in self.agents:\n","            stats = {\n","                'agent_id': agent.agent_id,\n","                'total_steps': agent.total_steps,\n","                'total_reward': agent.total_reward,\n","                'avg_reward_per_step': agent.total_reward / max(1, agent.total_steps),\n","                'intrinsic_motivation': agent.intrinsic_reward_sum / max(1, agent.total_steps),\n","                'novelty_seeking': agent.novelty_sum / max(1, agent.total_steps),\n","                'exploration_efficiency': len(agent.episodic_memory.buffer) / max(1, agent.total_steps),\n","                'concept_formation': agent.semantic_memory.get_concept_map(),\n","                'personality': {\n","                    'curiosity_weight': agent.curiosity_weight,\n","                    'social_weight': agent.social_weight,\n","                    'risk_tolerance': agent.risk_tolerance\n","                }\n","            }\n","            agent_stats.append(stats)\n","\n","        return {\n","            'agents': agent_stats,\n","            'world': {\n","                'complexity_level': self.world.complexity_level,\n","                'environmental_pressure': self.world.environmental_pressure,\n","                'time_step': self.world.time_step,\n","                'total_resources': len([obj for obj in self.world.objects.values() if obj != 'wall'])\n","            },\n","            'emergent_behaviors': len(self.global_stats['emergent_behaviors']),\n","            'collective_intelligence': self.global_stats['collective_intelligence'],\n","            'system_performance': {\n","                'total_agents': self.num_agents,\n","                'active_communications': self.global_stats['total_interactions'],\n","                'average_cooperation': np.mean([s['avg_reward_per_step'] for s in agent_stats])\n","            }\n","        }\n","\n","# ============================================================================\n","# 6. ENHANCED VISUALIZATION INTERFACE\n","# ============================================================================\n","\n","def create_advanced_html_interface():\n","    \"\"\"Create advanced HTML interface for multi-agent system.\"\"\"\n","\n","    html_template = \"\"\"\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>Advanced Multi-Agent GRLM System</title>\n","    <style>\n","        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700;900&family=Roboto:wght@300;400;500&display=swap');\n","\n","        * { margin: 0; padding: 0; box-sizing: border-box; }\n","\n","        body {\n","            font-family: 'Roboto', sans-serif;\n","            background: linear-gradient(135deg, #0c0c0c 0%, #1a1a2e 50%, #16213e 100%);\n","            color: #ffffff;\n","            overflow-x: hidden;\n","            min-height: 100vh;\n","        }\n","\n","        .header {\n","            text-align: center;\n","            padding: 15px;\n","            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4, #ffeaa7, #fd79a8);\n","            background-size: 400% 100%;\n","            animation: gradient-flow 4s ease-in-out infinite;\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 900;\n","            font-size: 2em;\n","            text-shadow: 0 0 20px rgba(255, 255, 255, 0.5);\n","            margin-bottom: 15px;\n","        }\n","\n","        @keyframes gradient-flow {\n","            0%, 100% { background-position: 0% 50%; }\n","            50% { background-position: 100% 50%; }\n","        }\n","\n","        .main-container {\n","            display: grid;\n","            grid-template-columns: 1fr 400px;\n","            gap: 15px;\n","            padding: 0 15px;\n","            max-width: 1600px;\n","            margin: 0 auto;\n","        }\n","\n","        .world-section {\n","            background: linear-gradient(145deg, #1e3c72, #2a5298);\n","            border-radius: 15px;\n","            padding: 20px;\n","            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);\n","            position: relative;\n","            overflow: hidden;\n","        }\n","\n","        .canvas-container {\n","            position: relative;\n","            display: inline-block;\n","            border-radius: 12px;\n","            overflow: hidden;\n","            box-shadow: 0 0 30px rgba(0, 255, 255, 0.2);\n","        }\n","\n","        canvas {\n","            display: block;\n","            background: radial-gradient(circle at center, #0f0f23 0%, #000000 100%);\n","        }\n","\n","        .controls {\n","            margin-top: 15px;\n","            display: grid;\n","            grid-template-columns: repeat(4, 1fr);\n","            gap: 10px;\n","        }\n","\n","        .agent-controls {\n","            background: rgba(255, 255, 255, 0.1);\n","            border-radius: 8px;\n","            padding: 10px;\n","            text-align: center;\n","        }\n","\n","        .agent-label {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 0.9em;\n","            margin-bottom: 8px;\n","            color: #4ecdc4;\n","        }\n","\n","        .control-grid {\n","            display: grid;\n","            grid-template-columns: repeat(3, 1fr);\n","            gap: 3px;\n","        }\n","\n","        .control-btn {\n","            background: linear-gradient(145deg, #667eea 0%, #764ba2 100%);\n","            color: white;\n","            border: none;\n","            padding: 8px;\n","            border-radius: 6px;\n","            cursor: pointer;\n","            font-size: 12px;\n","            font-weight: bold;\n","            transition: all 0.2s ease;\n","        }\n","\n","        .control-btn:hover {\n","            transform: scale(1.1);\n","            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);\n","        }\n","\n","        .control-btn.empty { opacity: 0; pointer-events: none; }\n","\n","        .auto-mode {\n","            grid-column: 1 / -1;\n","            margin-top: 10px;\n","            background: linear-gradient(145deg, #2ecc71, #27ae60);\n","            padding: 10px;\n","            border-radius: 8px;\n","            cursor: pointer;\n","            font-family: 'Orbitron', monospace;\n","            text-align: center;\n","        }\n","\n","        .stats-section {\n","            display: flex;\n","            flex-direction: column;\n","            gap: 15px;\n","        }\n","\n","        .stats-panel {\n","            background: linear-gradient(145deg, #2c3e50, #34495e);\n","            border-radius: 15px;\n","            padding: 20px;\n","            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.2);\n","        }\n","\n","        .panel-header {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.2em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","            margin-bottom: 15px;\n","            text-align: center;\n","            text-shadow: 0 0 10px rgba(78, 205, 196, 0.5);\n","        }\n","\n","        .agent-stats {\n","            display: grid;\n","            grid-template-columns: 1fr 1fr;\n","            gap: 10px;\n","            margin-bottom: 15px;\n","        }\n","\n","        .agent-stat-card {\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            padding: 10px;\n","            border-left: 3px solid;\n","            transition: all 0.3s ease;\n","        }\n","\n","        .agent-stat-card:nth-child(1) { border-left-color: #e74c3c; }\n","        .agent-stat-card:nth-child(2) { border-left-color: #3498db; }\n","        .agent-stat-card:nth-child(3) { border-left-color: #2ecc71; }\n","        .agent-stat-card:nth-child(4) { border-left-color: #f39c12; }\n","\n","        .agent-stat-card:hover {\n","            background: rgba(255, 255, 255, 0.1);\n","            transform: translateY(-2px);\n","        }\n","\n","        .stat-item {\n","            display: flex;\n","            justify-content: space-between;\n","            margin: 5px 0;\n","            font-size: 0.85em;\n","        }\n","\n","        .stat-label {\n","            color: #bdc3c7;\n","        }\n","\n","        .stat-value {\n","            font-family: 'Orbitron', monospace;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","        }\n","\n","        .emergent-behaviors {\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","            padding: 15px;\n","            margin-top: 10px;\n","        }\n","\n","        .behavior-item {\n","            background: rgba(78, 205, 196, 0.1);\n","            border-radius: 5px;\n","            padding: 8px;\n","            margin: 5px 0;\n","            font-size: 0.85em;\n","        }\n","\n","        .system-metrics {\n","            display: grid;\n","            grid-template-columns: 1fr 1fr;\n","            gap: 10px;\n","        }\n","\n","        .metric {\n","            text-align: center;\n","            padding: 10px;\n","            background: rgba(255, 255, 255, 0.05);\n","            border-radius: 8px;\n","        }\n","\n","        .metric-value {\n","            font-family: 'Orbitron', monospace;\n","            font-size: 1.5em;\n","            font-weight: 700;\n","            color: #4ecdc4;\n","        }\n","\n","        .metric-label {\n","            font-size: 0.8em;\n","            color: #bdc3c7;\n","            margin-top: 5px;\n","        }\n","\n","        @media (max-width: 1200px) {\n","            .main-container {\n","                grid-template-columns: 1fr;\n","                max-width: 100%;\n","            }\n","\n","            .stats-section {\n","                flex-direction: row;\n","                overflow-x: auto;\n","            }\n","\n","            .stats-panel {\n","                min-width: 300px;\n","            }\n","        }\n","    </style>\n","</head>\n","<body>\n","    <div class=\"header\">\n","         ADVANCED MULTI-AGENT GRLM \n","    </div>\n","\n","    <div class=\"main-container\">\n","        <div class=\"world-section\">\n","            <div class=\"canvas-container\">\n","                <canvas id=\"worldCanvas\" width=\"600\" height=\"600\"></canvas>\n","            </div>\n","            <div class=\"controls\">\n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 1 (Explorer)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(0, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","\n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 2 (Social)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(1, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","\n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 3 (Risk-taker)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(2, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","\n","                <div class=\"agent-controls\">\n","                    <div class=\"agent-label\"> Agent 4 (Balanced)</div>\n","                    <div class=\"control-grid\">\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 0, -1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, -1, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 0, 0)\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 1, 0)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                        <div class=\"control-btn\" onclick=\"moveAgent(3, 0, 1)\"></div>\n","                        <div class=\"control-btn empty\"></div>\n","                    </div>\n","                </div>\n","            </div>\n","\n","            <div class=\"auto-mode\" onclick=\"toggleAutoMode()\">\n","                 AUTO MODE: OFF\n","            </div>\n","        </div>\n","\n","        <div class=\"stats-section\">\n","            <div class=\"stats-panel\">\n","                <div class=\"panel-header\"> AGENT STATISTICS</div>\n","                <div class=\"agent-stats\" id=\"agentStats\">\n","                    Loading agent data...\n","                </div>\n","            </div>\n","\n","            <div class=\"stats-panel\">\n","                <div class=\"panel-header\"> EMERGENT BEHAVIORS</div>\n","                <div class=\"emergent-behaviors\" id=\"emergentBehaviors\">\n","                    <div>Monitoring for emergent behaviors...</div>\n","                </div>\n","            </div>\n","\n","            <div class=\"stats-panel\">\n","                <div class=\"panel-header\"> SYSTEM METRICS</div>\n","                <div class=\"system-metrics\" id=\"systemMetrics\">\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"complexity\">1.0</div>\n","                        <div class=\"metric-label\">Complexity</div>\n","                    </div>\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"cooperation\">0.0</div>\n","                        <div class=\"metric-label\">Cooperation</div>\n","                    </div>\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"exploration\">0%</div>\n","                        <div class=\"metric-label\">Explored</div>\n","                    </div>\n","                    <div class=\"metric\">\n","                        <div class=\"metric-value\" id=\"intelligence\">0.0</div>\n","                        <div class=\"metric-label\">Collective IQ</div>\n","                    </div>\n","                </div>\n","            </div>\n","        </div>\n","    </div>\n","\n","    <script>\n","        const canvas = document.getElementById('worldCanvas');\n","        const ctx = canvas.getContext('2d');\n","        const gridSize = 20;\n","\n","        let worldState = null;\n","        let systemStats = null;\n","        let autoMode = false;\n","        let animationFrame = null;\n","\n","        // Agent colors\n","        const agentColors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12'];\n","        const agentNames = ['Explorer', 'Social', 'Risk-taker', 'Balanced'];\n","\n","        function initializeSystem() {\n","            worldState = {\n","                size: 30,\n","                grid: Array(30).fill().map(() => Array(30).fill(0)),\n","                objects: {},\n","                agents: {'0': [2, 2], '1': [27, 2], '2': [2, 27], '3': [27, 27]},\n","                agent_trails: {'0': [], '1': [], '2': [], '3': []},\n","                territory_map: Array(30).fill().map(() => Array(30).fill(-1)),\n","                complexity_level: 1.0,\n","                time_step: 0\n","            };\n","\n","            // Generate initial objects\n","            for (let i = 0; i < 100; i++) {\n","                const x = Math.floor(Math.random() * 30);\n","                const y = Math.floor(Math.random() * 30);\n","                const key = `${x},${y}`;\n","                if (!worldState.objects[key]) {\n","                    const objType = Math.random() > 0.7 ? 'treasure' : (Math.random() > 0.5 ? 'food' : 'water');\n","                    worldState.objects[key] = objType;\n","                    worldState.grid[x][y] = objType === 'treasure' ? 2.0 : (objType === 'food' ? 1.0 : 0.5);\n","                }\n","            }\n","\n","            systemStats = {\n","                agents: [\n","                    {agent_id: 0, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 1.5}},\n","                    {agent_id: 1, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 0.8}},\n","                    {agent_id: 2, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 1.0}},\n","                    {agent_id: 3, total_reward: 0, avg_reward_per_step: 0, intrinsic_motivation: 0, personality: {curiosity_weight: 1.2}}\n","                ],\n","                emergent_behaviors: 0,\n","                collective_intelligence: 0,\n","                world: {complexity_level: 1.0, time_step: 0}\n","            };\n","\n","            updateDisplay();\n","        }\n","\n","        function drawWorld() {\n","            ctx.clearRect(0, 0, canvas.width, canvas.height);\n","\n","            if (!worldState) return;\n","\n","            const size = worldState.size;\n","\n","            // Draw grid background\n","            for (let x = 0; x < size; x++) {\n","                for (let y = 0; y < size; y++) {\n","                    const pixelX = x * gridSize;\n","                    const pixelY = y * gridSize;\n","\n","                    // Territory coloring\n","                    const territory = worldState.territory_map[x][y];\n","                    if (territory >= 0) {\n","                        ctx.fillStyle = agentColors[territory] + '20';\n","                        ctx.fillRect(pixelX, pixelY, gridSize, gridSize);\n","                    }\n","\n","                    // Grid lines\n","                    ctx.strokeStyle = 'rgba(255, 255, 255, 0.1)';\n","                    ctx.lineWidth = 0.5;\n","                    ctx.strokeRect(pixelX, pixelY, gridSize, gridSize);\n","                }\n","            }\n","\n","            // Draw objects\n","            for (const [key, objType] of Object.entries(worldState.objects)) {\n","                const [x, y] = key.split(',').map(Number);\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","\n","                let emoji = '';\n","                let glowColor = '';\n","\n","                switch (objType) {\n","                    case 'wall': emoji = ''; break;\n","                    case 'food': emoji = ''; glowColor = '#e74c3c'; break;\n","                    case 'water': emoji = ''; glowColor = '#3498db'; break;\n","                    case 'treasure': emoji = ''; glowColor = '#f39c12'; break;\n","                }\n","\n","                // Glow effect\n","                if (glowColor) {\n","                    const gradient = ctx.createRadialGradient(\n","                        pixelX + gridSize/2, pixelY + gridSize/2, 0,\n","                        pixelX + gridSize/2, pixelY + gridSize/2, gridSize\n","                    );\n","                    gradient.addColorStop(0, glowColor + '40');\n","                    gradient.addColorStop(1, 'transparent');\n","                    ctx.fillStyle = gradient;\n","                    ctx.fillRect(pixelX - 2, pixelY - 2, gridSize + 4, gridSize + 4);\n","                }\n","\n","                // Object\n","                ctx.font = `${gridSize * 0.6}px Arial`;\n","                ctx.textAlign = 'center';\n","                ctx.textBaseline = 'middle';\n","                ctx.fillStyle = '#ffffff';\n","                ctx.fillText(emoji, pixelX + gridSize/2, pixelY + gridSize/2);\n","            }\n","\n","            // Draw agent trails\n","            for (const [agentId, trail] of Object.entries(worldState.agent_trails)) {\n","                if (trail.length > 1) {\n","                    ctx.strokeStyle = agentColors[parseInt(agentId)] + '60';\n","                    ctx.lineWidth = 2;\n","                    ctx.beginPath();\n","\n","                    for (let i = 0; i < trail.length; i++) {\n","                        const pos = trail[i];\n","                        const x = (pos % worldState.size) * gridSize + gridSize/2;\n","                        const y = Math.floor(pos / worldState.size) * gridSize + gridSize/2;\n","\n","                        if (i === 0) ctx.moveTo(x, y);\n","                        else ctx.lineTo(x, y);\n","                    }\n","                    ctx.stroke();\n","                }\n","            }\n","\n","            // Draw agents\n","            for (const [agentId, position] of Object.entries(worldState.agents)) {\n","                const [x, y] = position;\n","                const pixelX = x * gridSize;\n","                const pixelY = y * gridSize;\n","                const color = agentColors[parseInt(agentId)];\n","\n","                // Agent glow\n","                const gradient = ctx.createRadialGradient(\n","                    pixelX + gridSize/2, pixelY + gridSize/2, 0,\n","                    pixelX + gridSize/2, pixelY + gridSize/2, gridSize\n","                );\n","                gradient.addColorStop(0, color + '60');\n","                gradient.addColorStop(1, 'transparent');\n","                ctx.fillStyle = gradient;\n","                ctx.fillRect(pixelX - 3, pixelY - 3, gridSize + 6, gridSize + 6);\n","\n","                // Agent body\n","                ctx.fillStyle = color;\n","                ctx.beginPath();\n","                ctx.arc(pixelX + gridSize/2, pixelY + gridSize/2, gridSize/3, 0, 2 * Math.PI);\n","                ctx.fill();\n","\n","                // Agent ID\n","                ctx.fillStyle = '#ffffff';\n","                ctx.font = `${gridSize * 0.4}px Arial`;\n","                ctx.textAlign = 'center';\n","                ctx.textBaseline = 'middle';\n","                ctx.fillText(agentId, pixelX + gridSize/2, pixelY + gridSize/2);\n","            }\n","        }\n","\n","        function updateStats() {\n","            if (!systemStats) return;\n","\n","            // Agent stats\n","            const agentStatsDiv = document.getElementById('agentStats');\n","            agentStatsDiv.innerHTML = systemStats.agents.map((agent, i) => `\n","                <div class=\"agent-stat-card\">\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\"> ${agentNames[i]}</span>\n","                        <span class=\"stat-value\">#${agent.agent_id}</span>\n","                    </div>\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\">Reward</span>\n","                        <span class=\"stat-value\">${agent.total_reward.toFixed(1)}</span>\n","                    </div>\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\">Avg/Step</span>\n","                        <span class=\"stat-value\">${agent.avg_reward_per_step.toFixed(2)}</span>\n","                    </div>\n","                    <div class=\"stat-item\">\n","                        <span class=\"stat-label\">Curiosity</span>\n","                        <span class=\"stat-value\">${agent.intrinsic_motivation.toFixed(2)}</span>\n","                    </div>\n","                </div>\n","            `).join('');\n","\n","            // System metrics\n","            document.getElementById('complexity').textContent = systemStats.world.complexity_level.toFixed(1);\n","            document.getElementById('cooperation').textContent = systemStats.system_performance?.average_cooperation?.toFixed(2) || '0.0';\n","            document.getElementById('exploration').textContent = '15%'; // Placeholder\n","            document.getElementById('intelligence').textContent = systemStats.collective_intelligence.toFixed(1);\n","\n","            // Emergent behaviors\n","            const behaviorsDiv = document.getElementById('emergentBehaviors');\n","            if (systemStats.emergent_behaviors > 0) {\n","                behaviorsDiv.innerHTML = `\n","                    <div class=\"behavior-item\"> Clustering Behavior Detected</div>\n","                    <div class=\"behavior-item\"> Cooperative Foraging</div>\n","                    <div class=\"behavior-item\"> Territory Formation</div>\n","                `;\n","            } else {\n","                behaviorsDiv.innerHTML = '<div>Monitoring for emergent behaviors...</div>';\n","            }\n","        }\n","\n","        function updateDisplay() {\n","            drawWorld();\n","            updateStats();\n","        }\n","\n","        function moveAgent(agentId, dx, dy) {\n","            if (!worldState || !worldState.agents[agentId]) return;\n","\n","            const [oldX, oldY] = worldState.agents[agentId];\n","            const newX = Math.max(0, Math.min(worldState.size - 1, oldX + dx));\n","            const newY = Math.max(0, Math.min(worldState.size - 1, oldY + dy));\n","\n","            // Check for collisions\n","            const occupied = Object.values(worldState.agents).some(pos => pos[0] === newX && pos[1] === newY);\n","            const isWall = worldState.grid[newX][newY] === -1;\n","\n","            if (!occupied && !isWall) {\n","                worldState.agents[agentId] = [newX, newY];\n","\n","                // Add to trail\n","                const trailPos = oldX * worldState.size + oldY;\n","                worldState.agent_trails[agentId].push(trailPos);\n","                if (worldState.agent_trails[agentId].length > 15) {\n","                    worldState.agent_trails[agentId].shift();\n","                }\n","\n","                // Object interaction\n","                const objKey = `${newX},${newY}`;\n","                if (worldState.objects[objKey] && worldState.objects[objKey] !== 'wall') {\n","                    const objType = worldState.objects[objKey];\n","                    let reward = 0;\n","\n","                    switch (objType) {\n","                        case 'treasure': reward = 3; break;\n","                        case 'food': reward = 1.5; break;\n","                        case 'water': reward = 0.8; break;\n","                    }\n","\n","                    // Update agent stats\n","                    systemStats.agents[agentId].total_reward += reward;\n","                    systemStats.agents[agentId].avg_reward_per_step = systemStats.agents[agentId].total_reward / (systemStats.world.time_step + 1);\n","                    systemStats.agents[agentId].intrinsic_motivation += Math.random() * 0.5;\n","\n","                    // Remove object\n","                    delete worldState.objects[objKey];\n","                    worldState.grid[newX][newY] = 0;\n","                }\n","\n","                // Update territory\n","                worldState.territory_map[newX][newY] = agentId;\n","\n","                // Update system stats\n","                systemStats.world.time_step++;\n","                systemStats.world.complexity_level = 1.0 + systemStats.world.time_step * 0.001;\n","                systemStats.collective_intelligence = systemStats.agents.reduce((sum, a) => sum + a.intrinsic_motivation, 0) / 4;\n","\n","                if (Math.random() < 0.1) systemStats.emergent_behaviors++;\n","            }\n","\n","            updateDisplay();\n","        }\n","\n","        function toggleAutoMode() {\n","            autoMode = !autoMode;\n","            document.querySelector('.auto-mode').textContent = ` AUTO MODE: ${autoMode ? 'ON' : 'OFF'}`;\n","            document.querySelector('.auto-mode').style.background = autoMode ?\n","                'linear-gradient(145deg, #e74c3c, #c0392b)' : 'linear-gradient(145deg, #2ecc71, #27ae60)';\n","\n","            if (autoMode) {\n","                runAutoMode();\n","            }\n","        }\n","\n","        function runAutoMode() {\n","            if (!autoMode) return;\n","\n","            // Move each agent randomly\n","            for (let i = 0; i < 4; i++) {\n","                const moves = [[-1, 0], [1, 0], [0, -1], [0, 1], [0, 0]];\n","                const [dx, dy] = moves[Math.floor(Math.random() * moves.length)];\n","                moveAgent(i, dx, dy);\n","            }\n","\n","            setTimeout(runAutoMode, 500); // Auto-move every 500ms\n","        }\n","\n","        // Keyboard controls for Agent 0\n","        document.addEventListener('keydown', (e) => {\n","            if (autoMode) return;\n","\n","            switch(e.key.toLowerCase()) {\n","                case 'w': moveAgent(0, 0, -1); break;\n","                case 's': moveAgent(0, 0, 1); break;\n","                case 'a': moveAgent(0, -1, 0); break;\n","                case 'd': moveAgent(0, 1, 0); break;\n","                case ' ': toggleAutoMode(); break;\n","            }\n","            e.preventDefault();\n","        });\n","\n","        // Initialize\n","        initializeSystem();\n","\n","        console.log(' Advanced Multi-Agent GRLM System Initialized');\n","        console.log(' 4 agents with different personalities');\n","        console.log(' Curiosity-driven exploration');\n","        console.log(' Neural communication');\n","        console.log(' Dynamic world with territories');\n","        console.log('  WASD to control Agent 0, Space for auto-mode');\n","    </script>\n","</body>\n","</html>\n","    \"\"\"\n","\n","    return html_template\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","def launch_advanced_system():\n","    \"\"\"Launch the complete advanced multi-agent system.\"\"\"\n","\n","    print(\" Launching Advanced Multi-Agent GRLM System...\")\n","    print(\" Compute-intensive features:\")\n","    print(\"    Multi-agent curiosity-driven exploration\")\n","    print(\"    Neural communication between agents\")\n","    print(\"    Episodic replay with prioritized experience\")\n","    print(\"    Semantic concept formation\")\n","    print(\"    Dynamic world with emergent complexity\")\n","    print(\"    Territorial behavior and resource competition\")\n","\n","    # Create the advanced system\n","    advanced_system = AdvancedMultiAgentGRLM(num_agents=NUM_AGENTS)\n","\n","    # Create advanced HTML interface\n","    html_content = create_advanced_html_interface()\n","\n","    print(\" System ready! Features active:\")\n","    print(\"    Intrinsic Curiosity Module (ICM)\")\n","    print(\"    Random Network Distillation (RND)\")\n","    print(\"    Prioritized Experience Replay\")\n","    print(\"     Hierarchical Semantic Memory\")\n","    print(\"    Multi-agent Communication\")\n","    print(\"    Dynamic Environment\")\n","    print(\"    Real-time visualization\")\n","\n","    # Display the interface\n","    display(HTML(html_content))\n","\n","    print(\"\\n Advanced Multi-Agent GRLM is running!\")\n","    print(\"Watch emergent behaviors develop as agents explore and communicate!\")\n","\n","    return advanced_system\n","\n","if __name__ == \"__main__\":\n","    advanced_system = launch_advanced_system()\n","\n","# Launch the system\n","advanced_system = launch_advanced_system()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":18912,"status":"ok","timestamp":1758217450115,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"},"user_tz":-180},"id":"pyay-R3I7NhP","outputId":"3e3fc752-8f54-4d66-8d2a-34c42a10c0e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["  - step 10/40 | loss=0.062547 | mem=2560 | avg_step=0.015s\n","  - step 20/40 | loss=0.027280 | mem=5120 | avg_step=0.015s\n","  - step 30/40 | loss=0.009354 | mem=7680 | avg_step=0.015s\n","  - step 40/40 | loss=0.004118 | mem=10240 | avg_step=0.015s\n","Episode 01 done: mean_loss=0.080646 | nodes=10240\n","  - step 10/40 | loss=0.006136 | mem=12800 | avg_step=0.015s\n","  - step 20/40 | loss=0.002570 | mem=15360 | avg_step=0.015s\n","  - step 30/40 | loss=0.001843 | mem=17920 | avg_step=0.015s\n","  - step 40/40 | loss=0.003471 | mem=20480 | avg_step=0.015s\n","Episode 02 done: mean_loss=0.004225 | nodes=20480\n","  - step 10/40 | loss=0.005984 | mem=23040 | avg_step=0.015s\n","  - step 20/40 | loss=0.009756 | mem=25600 | avg_step=0.015s\n","  - step 30/40 | loss=0.003495 | mem=28160 | avg_step=0.016s\n","  - step 40/40 | loss=0.003025 | mem=30720 | avg_step=0.015s\n","Episode 03 done: mean_loss=0.003653 | nodes=30720\n","  - step 10/40 | loss=0.003486 | mem=33280 | avg_step=0.016s\n","  - step 20/40 | loss=0.002378 | mem=35840 | avg_step=0.015s\n","  - step 30/40 | loss=0.003457 | mem=38400 | avg_step=0.015s\n","  - step 40/40 | loss=0.002132 | mem=40960 | avg_step=0.015s\n","Episode 04 done: mean_loss=0.003384 | nodes=40960\n","  - step 10/40 | loss=0.001890 | mem=43520 | avg_step=0.015s\n","  - step 20/40 | loss=0.003005 | mem=46080 | avg_step=0.015s\n","  - step 30/40 | loss=0.002398 | mem=48640 | avg_step=0.015s\n","  - step 40/40 | loss=0.002570 | mem=51200 | avg_step=0.018s\n","Episode 05 done: mean_loss=0.003125 | nodes=51200\n","  - step 10/40 | loss=0.009777 | mem=53760 | avg_step=0.018s\n","  - step 20/40 | loss=0.004527 | mem=56320 | avg_step=0.018s\n","  - step 30/40 | loss=0.008504 | mem=58880 | avg_step=0.018s\n","  - step 40/40 | loss=0.003457 | mem=61440 | avg_step=0.018s\n","Episode 06 done: mean_loss=0.004257 | nodes=61440\n","  - step 10/40 | loss=0.002971 | mem=64000 | avg_step=0.018s\n","  - step 20/40 | loss=0.003345 | mem=66560 | avg_step=0.018s\n","  - step 30/40 | loss=0.003547 | mem=69120 | avg_step=0.019s\n","  - step 40/40 | loss=0.002824 | mem=71680 | avg_step=0.019s\n","Episode 07 done: mean_loss=0.003757 | nodes=71680\n","  - step 10/40 | loss=0.001121 | mem=74240 | avg_step=0.018s\n","  - step 20/40 | loss=0.001548 | mem=76800 | avg_step=0.018s\n","  - step 30/40 | loss=0.003815 | mem=79360 | avg_step=0.019s\n","  - step 40/40 | loss=0.002795 | mem=81920 | avg_step=0.019s\n","Episode 08 done: mean_loss=0.003305 | nodes=81920\n","  - step 10/40 | loss=0.002582 | mem=84480 | avg_step=0.019s\n","  - step 20/40 | loss=0.004275 | mem=87040 | avg_step=0.018s\n","  - step 30/40 | loss=0.002124 | mem=89600 | avg_step=0.018s\n","  - step 40/40 | loss=0.001542 | mem=92160 | avg_step=0.018s\n","Episode 09 done: mean_loss=0.002930 | nodes=92160\n","  - step 10/40 | loss=0.005078 | mem=94720 | avg_step=0.018s\n","  - step 20/40 | loss=0.003128 | mem=97280 | avg_step=0.019s\n","  - step 30/40 | loss=0.001685 | mem=99840 | avg_step=0.020s\n","  - step 40/40 | loss=0.002148 | mem=102400 | avg_step=0.018s\n","Episode 10 done: mean_loss=0.002728 | nodes=102400\n","Done. Elapsed: 14.8s | preset=FAST_24CU\n"]}],"source":["# @title\n","# === GRLM \"FAST_24CU\" one-cell trainer ===\n","# Budget-friendly upgrades: TF32, smart autocast, torch.compile, foreach AdamW,\n","# FAISS IVF switch-over, amortized candidate sampling, lighter I/O.\n","\n","import os, math, time, random, sys\n","from dataclasses import dataclass\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# -------------------------\n","# System-level speed knobs\n","# -------------------------\n","torch.backends.cudnn.benchmark = True  # static-ish shapes\n","if torch.cuda.is_available():\n","    # Enable TF32 on Ampere+ and give matmul a higher-precision fast-path\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.set_float32_matmul_precision(\"high\")  # safe perf boost on A100/3090 etc.\n","\n","# Autocast dtype: BF16 if supported, else FP16; GradScaler only for FP16\n","_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","_cap = torch.cuda.get_device_capability()[0] if torch.cuda.is_available() else 0\n","BF16_OK = torch.cuda.is_available() and _cap >= 8  # Ampere+\n","AMP_DTYPE = torch.bfloat16 if BF16_OK else torch.float16\n","USE_SCALER = (AMP_DTYPE is torch.float16)\n","GradScaler = torch.amp.GradScaler if BF16_OK else torch.cuda.amp.GradScaler\n","scaler = GradScaler(enabled=USE_SCALER)\n","\n","# -------------------------\n","# Budget presets (pick one)\n","# -------------------------\n","PRESET = os.environ.get(\"GRLM_PRESET\", \"FAST_24CU\")  # \"TINY\", \"FAST_24CU\", \"STANDARD\"\n","\n","PRESETS = {\n","    # minimal, for smoke tests\n","    \"TINY\": dict(EPISODES=6, STEPS_PER_EP=30, EMB_DIM=96, HIDDEN=192,\n","                 K_NEI=3, CAND_RECENT=128, CAND_RANDOM=48, LR=2.5e-3,\n","                 FAISS_PROBE=16),\n","    # target for ~two dozen compute units (default)\n","    \"FAST_24CU\": dict(EPISODES=10, STEPS_PER_EP=40, EMB_DIM=128, HIDDEN=256,\n","                      K_NEI=4, CAND_RECENT=256, CAND_RANDOM=96, LR=2.0e-3,\n","                      FAISS_PROBE=24),\n","    # bigger but still thrifty\n","    \"STANDARD\": dict(EPISODES=20, STEPS_PER_EP=50, EMB_DIM=192, HIDDEN=384,\n","                     K_NEI=6, CAND_RECENT=384, CAND_RANDOM=128, LR=1.6e-3,\n","                     FAISS_PROBE=32),\n","}\n","CFG = PRESETS[PRESET]\n","\n","# -------------------------\n","# Tiny synthetic world + memory\n","# -------------------------\n","@dataclass\n","class Node:\n","    x: torch.Tensor\n","    y: torch.Tensor\n","\n","class WorldMemory:\n","    def __init__(self, d):\n","        self.emb = []   # list of numpy float32 vectors\n","        self.val = []   # targets (float32)\n","        # amortized candidate pools\n","        self._recent_ring = []\n","        self._recent_max = 50_000\n","        self._rand_pool = None  # numpy array of indices\n","\n","    def __len__(self): return len(self.emb)\n","\n","    def add(self, x_e, y):\n","        self.emb.append(x_e.detach().float().cpu().numpy())\n","        self.val.append(y.detach().float().cpu().numpy())\n","        self._recent_ring.append(len(self.emb)-1)\n","        if len(self._recent_ring) > self._recent_max:\n","            self._recent_ring.pop(0)\n","\n","    def numpy_matrix(self):\n","        if not self.emb: return None\n","        return np.asarray(self.emb, dtype=np.float32)\n","\n","    def build_rand_pool(self):\n","        n = len(self.emb)\n","        self._rand_pool = np.arange(n, dtype=np.int64)\n","\n","    def sample_recent(self, k):\n","        pool = self._recent_ring\n","        if not pool: return []\n","        k = min(k, len(pool))\n","        return random.sample(pool, k)\n","\n","    def sample_random(self, k):\n","        if self._rand_pool is None or len(self._rand_pool) != len(self.emb):\n","            self.build_rand_pool()\n","        n = len(self._rand_pool)\n","        if n == 0: return []\n","        k = min(k, n)\n","        # vectorized draw without replacement\n","        idx = np.random.choice(n, size=k, replace=False)\n","        return self._rand_pool[idx].tolist()\n","\n","# -------------------------\n","# Model: simple MLP embedder + predictor\n","# -------------------------\n","class Embedder(nn.Module):\n","    def __init__(self, in_dim, emb_dim, hidden):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(in_dim, hidden), nn.GELU(),\n","            nn.Linear(hidden, hidden), nn.GELU(),\n","            nn.Linear(hidden, emb_dim),\n","        )\n","    def forward(self, x): return F.normalize(self.net(x), dim=-1)\n","\n","class Head(nn.Module):\n","    def __init__(self, emb_dim, hidden):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(emb_dim, hidden), nn.GELU(),\n","            nn.Linear(hidden, 1),\n","        )\n","    def forward(self, e): return self.net(e).squeeze(-1)\n","\n","class WorldModel(nn.Module):\n","    def __init__(self, in_dim, emb_dim, hidden):\n","        super().__init__()\n","        self.enc = Embedder(in_dim, emb_dim, hidden)\n","        self.head = Head(emb_dim, hidden)\n","    def forward(self, x):\n","        e = self.enc(x)\n","        y = self.head(e)\n","        return e, y\n","\n","# Optionally JIT-compile the model to reduce Python overhead (PyTorch 2.x)\n","def maybe_compile(m):\n","    try:\n","        m = torch.compile(m, mode=\"reduce-overhead\", dynamic=False)\n","    except Exception:\n","        pass\n","    return m\n","\n","# -------------------------\n","# FAISS KNN (Flat -> IVF)\n","# -------------------------\n","USE_FAISS = True\n","try:\n","    import faiss\n","    FAISS_GPU = faiss.get_num_gpus() > 0\n","except Exception:\n","    USE_FAISS, FAISS_GPU = False, False\n","    faiss = None\n","\n","class FaissKNN:\n","    def __init__(self, dim):\n","        self.dim = dim\n","        self.index = None\n","        self.kind = \"flat\"\n","        self.nlist = 0\n","        self.nprobe = CFG[\"FAISS_PROBE\"]\n","\n","    def _build_flat(self, xb):\n","        self.index = faiss.IndexFlatIP(self.dim)\n","\n","    def _build_ivf(self, xb, nlist):\n","        quant = faiss.IndexFlatIP(self.dim)\n","        self.index = faiss.IndexIVFFlat(quant, self.dim, nlist, faiss.METRIC_INNER_PRODUCT)\n","        self.index.train(xb)\n","        self.index.nprobe = self.nprobe\n","\n","    @property\n","    def nprobe(self): return self._nprobe\n","    @nprobe.setter\n","    def nprobe(self, v):\n","        self._nprobe = int(v)\n","\n","    def rebuild(self, xb):\n","        if not USE_FAISS or xb is None or len(xb) == 0:\n","            self.index = None\n","            self.kind = \"none\"\n","            return\n","        xb = np.ascontiguousarray(xb, dtype=np.float32)\n","        n = xb.shape[0]\n","        # Heuristic: IVF when large; pick nlist ~ 4*sqrt(N) (wiki guideline-ish)\n","        if n >= 80_000:\n","            self.kind = \"ivf\"\n","            nlist = max(256, int(4 * math.sqrt(n)))\n","            self.nlist = nlist\n","            self._build_ivf(xb, nlist)\n","        else:\n","            self.kind = \"flat\"\n","            self._build_flat(xb)\n","        self.index.add(xb)\n","\n","    def search(self, q, k):\n","        if self.index is None or len(q) == 0:\n","            return np.empty((0, k), np.int64), np.empty((0, k), np.float32)\n","        q = np.ascontiguousarray(q, dtype=np.float32)\n","        if self.kind == \"ivf\":\n","            self.index.nprobe = self._nprobe\n","        D, I = self.index.search(q, k)\n","        return I, D\n","\n","# -------------------------\n","# Training utilities\n","# -------------------------\n","def mse(a, b): return F.mse_loss(a, b)\n","\n","def make_batch(bs=256, in_dim=12, device=_device):\n","    # Simple synthetic task: predict a function of inputs\n","    x = torch.randn(bs, in_dim, device=device)\n","    y = (x[..., :4].sum(-1) + 0.3 * x[..., 4:8].mean(-1) - 0.1 * x[..., 8:].prod(-1)).tanh()\n","    return x, y\n","\n","def step(model, mem, knn, opt):\n","    model.train()\n","    x, y_true = make_batch()\n","    with torch.autocast(device_type=\"cuda\" if _device.type==\"cuda\" else \"cpu\", dtype=AMP_DTYPE, enabled=True):\n","        e, y_pred = model(x)\n","        loss = mse(y_pred, y_true)\n","\n","    opt.zero_grad(set_to_none=True)\n","    if USE_SCALER:\n","        scaler.scale(loss).backward()\n","        scaler.step(opt); scaler.update()\n","    else:\n","        loss.backward(); opt.step()\n","\n","    # add to memory\n","    with torch.no_grad():\n","        for i in range(e.shape[0]):\n","            mem.add(e[i], y_true[i])\n","\n","    # occasionally rebuild FAISS (or build once memory is non-empty)\n","    return float(loss.detach().item())\n","\n","# -------------------------\n","# Main\n","# -------------------------\n","def main():\n","    cfg = CFG\n","    in_dim = 12\n","    model = WorldModel(in_dim, cfg[\"EMB_DIM\"], cfg[\"HIDDEN\"]).to(_device)\n","    model = maybe_compile(model)\n","\n","    opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"LR\"], foreach=True)\n","\n","    mem = WorldMemory(d=cfg[\"EMB_DIM\"])\n","    knn = FaissKNN(dim=cfg[\"EMB_DIM\"]) if USE_FAISS else None\n","\n","    total_steps = cfg[\"EPISODES\"] * cfg[\"STEPS_PER_EP\"]\n","    rebuild_every = 8  # amortize FAISS rebuilds\n","    t0 = time.time()\n","    ema = None\n","\n","    for ep in range(1, cfg[\"EPISODES\"]+1):\n","        ep_losses = []\n","        for s in range(1, cfg[\"STEPS_PER_EP\"]+1):\n","            # rebuild FAISS lazily as memory grows\n","            if USE_FAISS and (len(mem) > 0) and (((s + (ep-1)*cfg[\"STEPS_PER_EP\"]) % rebuild_every) == 0):\n","                xb = mem.numpy_matrix()\n","                if xb is not None:\n","                    knn.rebuild(xb)\n","\n","            t_step0 = time.time()\n","            loss = step(model, mem, knn, opt)\n","            t_step = time.time() - t_step0\n","            ep_losses.append(loss)\n","\n","            if s % 10 == 0:\n","                print(f\"  - step {s:02d}/{cfg['STEPS_PER_EP']} | loss={loss:.6f} | mem={len(mem)} | avg_step={t_step:.3f}s\")\n","\n","        mean_loss = sum(ep_losses)/len(ep_losses)\n","        print(f\"Episode {ep:02d} done: mean_loss={mean_loss:.6f} | nodes={len(mem)}\")\n","    print(f\"Done. Elapsed: {time.time()-t0:.1f}s | preset={PRESET}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":16218,"status":"ok","timestamp":1758219167265,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"},"user_tz":-180},"id":"Ci8G51ZSBxam","outputId":"f1963d44-62e4-4dae-ad8d-70fa3ed52f52"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-304538676.py:302: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(\n"]},{"name":"stdout","output_type":"stream","text":["Device: cuda (NVIDIA A100-SXM4-40GB) | TF32=True | AMP_dtype=bf16 | compile=on | bnb8bit=off\n","FAISS available: False | IVF-PQ=off\n","  - warm start: seeded memory with first batch\n","  - step 10/40 | loss=0.411585 | mem=2560 | avg_step=0.179s\n","  - step 20/40 | loss=0.451531 | mem=5120 | avg_step=0.007s\n","  - step 30/40 | loss=0.469928 | mem=7680 | avg_step=0.007s\n","  - step 40/40 | loss=0.480150 | mem=10240 | avg_step=0.008s\n","Episode 01 done: mean_loss=0.480150 | nodes=10240\n","  - step 10/40 | loss=0.512467 | mem=12800 | avg_step=0.009s\n","  - step 20/40 | loss=0.512739 | mem=15360 | avg_step=0.010s\n","  - step 30/40 | loss=0.512889 | mem=17920 | avg_step=0.010s\n","  - step 40/40 | loss=0.513038 | mem=20480 | avg_step=0.011s\n","Episode 02 done: mean_loss=0.513038 | nodes=20480\n","  - step 10/40 | loss=0.514119 | mem=23040 | avg_step=0.012s\n","  - step 20/40 | loss=0.514562 | mem=25600 | avg_step=0.014s\n","  - step 30/40 | loss=0.514909 | mem=28160 | avg_step=0.016s\n","  - step 40/40 | loss=0.515201 | mem=30720 | avg_step=0.017s\n","Episode 03 done: mean_loss=0.515201 | nodes=30720\n","  - step 10/40 | loss=0.516319 | mem=33280 | avg_step=0.021s\n","  - step 20/40 | loss=0.516429 | mem=35840 | avg_step=0.018s\n","  - step 30/40 | loss=0.516457 | mem=38400 | avg_step=0.021s\n","  - step 40/40 | loss=0.516318 | mem=40960 | avg_step=0.023s\n","Episode 04 done: mean_loss=0.516318 | nodes=40960\n","  - step 10/40 | loss=0.514309 | mem=43520 | avg_step=0.021s\n","  - step 20/40 | loss=0.513582 | mem=46080 | avg_step=0.026s\n","  - step 30/40 | loss=0.513517 | mem=48640 | avg_step=0.026s\n","  - step 40/40 | loss=0.513188 | mem=51200 | avg_step=0.026s\n","Episode 05 done: mean_loss=0.513188 | nodes=51200\n","  - step 10/40 | loss=0.511507 | mem=53760 | avg_step=0.026s\n","  - step 20/40 | loss=0.511842 | mem=56320 | avg_step=0.029s\n","  - step 30/40 | loss=0.511700 | mem=58880 | avg_step=0.033s\n","  - step 40/40 | loss=0.511561 | mem=61440 | avg_step=0.030s\n","Episode 06 done: mean_loss=0.511561 | nodes=61440\n","  - step 10/40 | loss=0.512944 | mem=64000 | avg_step=0.030s\n","  - step 20/40 | loss=0.511838 | mem=66560 | avg_step=0.038s\n","  - step 30/40 | loss=0.511259 | mem=69120 | avg_step=0.054s\n","  - step 40/40 | loss=0.511616 | mem=71680 | avg_step=0.055s\n","Episode 07 done: mean_loss=0.511616 | nodes=71680\n","  - step 10/40 | loss=0.511996 | mem=74240 | avg_step=0.057s\n","  - step 20/40 | loss=0.513663 | mem=76800 | avg_step=0.060s\n","  - step 30/40 | loss=0.514843 | mem=79360 | avg_step=0.063s\n","  - step 40/40 | loss=0.515820 | mem=81920 | avg_step=0.065s\n","Episode 08 done: mean_loss=0.515820 | nodes=81920\n","  - step 10/40 | loss=0.518811 | mem=84480 | avg_step=0.064s\n","  - step 20/40 | loss=0.518836 | mem=87040 | avg_step=0.070s\n","  - step 30/40 | loss=0.518855 | mem=89600 | avg_step=0.074s\n","  - step 40/40 | loss=0.518884 | mem=92160 | avg_step=0.074s\n","Episode 09 done: mean_loss=0.518884 | nodes=92160\n","  - step 10/40 | loss=0.519013 | mem=94720 | avg_step=0.072s\n","  - step 20/40 | loss=0.519011 | mem=97280 | avg_step=0.077s\n","  - step 30/40 | loss=0.518988 | mem=99840 | avg_step=0.080s\n","  - step 40/40 | loss=0.518959 | mem=102400 | avg_step=0.084s\n","Episode 10 done: mean_loss=0.518959 | nodes=102400\n","Done. Elapsed: 16.5s | preset=FAST_24CU\n","Artifacts in: /content/graph_world_runs/glrm_singlecell\n"]}],"source":["# @title\n","#  GLRM: single-cell Colab runner (fast, compact, self-contained)\n","# - Detects Colab/A100; enables TF32 + AMP (bf16 or fp16+GradScaler)\n","# - torch.compile(mode=\"reduce-overhead\") to lower step overhead\n","# - AdamW(foreach=True) or bitsandbytes AdamW8bit if available\n","# - In-memory vector store with optional FAISS IVF-PQ (CPU/GPU)\n","# - Batched KNN retrieval; robust nprobe handling on CPU/GPU\n","# - Distance-weighted neighbor centroid + InfoNCE-ish contrast\n","# - Progress logs, per-episode checkpoints, and simple presets\n","# - Synthetic GMM toy stream so it runs out-of-the-box\n","#   (swap get_batch() to train on real data)\n","\n","# ========================== CONFIG ==========================\n","PRESET = \"FAST_24CU\"   # \"FAST_24CU\", \"BALANCED\", \"MAX_60CU\"\n","SEED = 123\n","MOUNT_DRIVE = False\n","RUN_NAME = \"glrm_singlecell\"\n","SAVE_ROOT = \"/content/drive/MyDrive\" if MOUNT_DRIVE else \"/content\"\n","CHECKPOINT_EVERY_EP = True\n","\n","# Model/optim\n","D_IN = 128            # input feature dim (synthetic stream default)\n","D_HID = 256\n","D_OUT = 128           # embedding dim\n","LR = 3e-3\n","WEIGHT_DECAY = 1e-2\n","USE_BNB_8BIT = True   # try bitsandbytes AdamW8bit if present\n","USE_COMPILE = True    # torch.compile for lower overhead\n","COMPILE_MODE = \"reduce-overhead\"  # good for many small steps\n","\n","# Training schedule by preset\n","PRESETS = {\n","    \"FAST_24CU\": dict(EPISODES=10, STEPS_PER_EP=40, BATCH=256, K=8),\n","    \"BALANCED\":  dict(EPISODES=20, STEPS_PER_EP=60, BATCH=384, K=16),\n","    \"MAX_60CU\":  dict(EPISODES=30, STEPS_PER_EP=80, BATCH=512, K=32),\n","}\n","S = PRESETS[PRESET]\n","EPISODES, STEPS_PER_EP, BATCH, K_NEI = S[\"EPISODES\"], S[\"STEPS_PER_EP\"], S[\"BATCH\"], S[\"K\"]\n","\n","# Memory / FAISS\n","REINDEX_EVERY_STEPS = 400          # rebuild IVF-PQ roughly every N adds (or at episode end)\n","FAISS_USE_IVFPQ = True             # IVF-PQ (fast & memory-light) when faiss is present\n","FAISS_NLIST = 1024                 # coarse centroids (tune with data scale)\n","FAISS_M = 16                       # PQ subquantizers\n","FAISS_NBITS = 8                    # bits per subvector\n","FAISS_NPROBE = 32                  # probes at search (latency/recall knob)\n","TOPK = K_NEI                       # neighbors to retrieve from memory bank\n","\n","# Loss\n","LAMBDA_MSE = 1.0                   # pull embedding to neighbor centroid\n","LAMBDA_CONTRAST = 0.25             # InfoNCE-ish term over retrieved + random negatives\n","TEMP = 0.07                        # temperature for contrastive\n","\n","# ============================================================\n","import os, sys, math, time, random, contextlib\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Optional deps: faiss, bitsandbytes\n","def _lazy_import(name):\n","    try:\n","        return __import__(name)\n","    except Exception:\n","        return None\n","\n","faiss = _lazy_import(\"faiss\") or _lazy_import(\"faiss_gpu\") or _lazy_import(\"faiss_cpu\")\n","bnb = _lazy_import(\"bitsandbytes\")\n","\n","# -------- Colab / Drive ----------\n","IN_COLAB = \"google.colab\" in sys.modules\n","if MOUNT_DRIVE and IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","\n","# -------- Repro & device ----------\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cuda_name = torch.cuda.get_device_name(0) if device.type == \"cuda\" else \"CPU\"\n","bf16_ok = (device.type == \"cuda\") and torch.cuda.is_bf16_supported()\n","amp_dtype = torch.bfloat16 if bf16_ok else torch.float16\n","use_amp = (device.type == \"cuda\")\n","\n","# Enable TF32 on Ampere+ where available (matrix-multiply fast path)\n","try:\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    torch.set_float32_matmul_precision(\"high\")\n","except Exception:\n","    pass\n","\n","# Speedy CuDNN heuristics for convs (safe here, no determinism requirement)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.benchmark = True\n","\n","# -------- Model ----------\n","class Encoder(nn.Module):\n","    def __init__(self, d_in=D_IN, d_hid=D_HID, d_out=D_OUT):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d_in, d_hid),\n","            nn.GELU(),\n","            nn.Linear(d_hid, d_out),\n","        )\n","        # small norm helps stability at init\n","        with torch.no_grad():\n","            for m in self.modules():\n","                if isinstance(m, nn.Linear):\n","                    nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n","                    nn.init.zeros_(m.bias)\n","\n","    def forward(self, x):\n","        return F.normalize(self.net(x), dim=-1)\n","\n","# -------- Synthetic stream (GMM) ----------\n","class GMMStream:\n","    def __init__(self, d=D_IN, k=32, std=0.3):\n","        self.d, self.k, self.std = d, k, std\n","        rng = np.random.default_rng(SEED)\n","        self.means = rng.normal(size=(k, d)).astype(np.float32)\n","        self.assign = rng\n","\n","    def sample(self, n):\n","        idx = self.assign.integers(0, self.k, size=(n,))\n","        base = self.means[idx]\n","        noise = self.assign.normal(scale=self.std, size=(n, self.d)).astype(np.float32)\n","        x = base + noise\n","        return torch.from_numpy(x), torch.from_numpy(idx.astype(np.int64))\n","\n","gmm = GMMStream(d=D_IN, k=32, std=0.3)\n","\n","def get_batch(bs=BATCH):\n","    x, y = gmm.sample(bs)\n","    # make non_blocking transfers actually non_blocking by pinning\n","    if device.type == \"cuda\":\n","        x = x.pin_memory()\n","        y = y.pin_memory()\n","    return x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n","\n","# -------- Memory bank with optional FAISS index ----------\n","class MemoryBank:\n","    def __init__(self, d, use_faiss=True, ivfpq=True):\n","        self.d = d\n","        self.use_faiss = bool(faiss) and use_faiss\n","        self.ivfpq = ivfpq\n","        self._vecs = []   # list of torch tensors on CPU (fp32)\n","        self._labels = [] # optional labels for diagnostics\n","        self._faiss = None\n","        self._trained = False\n","        self._added = 0\n","\n","    @property\n","    def size(self):\n","        return sum(v.shape[0] for v in self._vecs)\n","\n","    def add(self, vecs: torch.Tensor, labels: torch.Tensor = None):\n","        # store as float32 CPU\n","        vc = vecs.detach().to(\"cpu\", dtype=torch.float32).contiguous()\n","        self._vecs.append(vc)\n","        if labels is not None:\n","            self._labels.append(labels.detach().to(\"cpu\"))\n","        self._added += vc.shape[0]\n","\n","    def _cat(self):\n","        if not self._vecs:\n","            return None, None\n","        X = torch.cat(self._vecs, dim=0)\n","        y = torch.cat(self._labels, dim=0) if self._labels else None\n","        return X, y\n","\n","    def maybe_reindex(self, force=False):\n","        if not self.use_faiss:\n","            return\n","        if not force and self._added < REINDEX_EVERY_STEPS:\n","            return\n","        self._added = 0\n","        X, _ = self._cat()\n","        if X is None or X.shape[0] < max(FAISS_NLIST, 1024):\n","            # Not enough data to train a coarse quantizer yet\n","            self._faiss = None\n","            self._trained = False\n","            return\n","\n","        X_np = X.numpy()\n","        d = X_np.shape[1]\n","\n","        if self.ivfpq:\n","            quantizer = faiss.IndexFlatL2(d)\n","            index = faiss.IndexIVFPQ(quantizer, d, FAISS_NLIST, FAISS_M, FAISS_NBITS)\n","        else:\n","            quantizer = faiss.IndexFlatL2(d)\n","            index = faiss.IndexIVFFlat(quantizer, d, FAISS_NLIST, faiss.METRIC_L2)\n","\n","        # Try GPU if available in this runtime\n","        if hasattr(faiss, \"StandardGpuResources\") and torch.cuda.is_available():\n","            try:\n","                res = faiss.StandardGpuResources()\n","                index = faiss.index_cpu_to_gpu(res, 0, index)\n","            except Exception:\n","                pass\n","\n","        # Train + add\n","        index.train(X_np)\n","\n","        # tune probe count across CPU/GPU bindings\n","        if hasattr(index, \"nprobe\"):\n","            index.nprobe = max(1, FAISS_NPROBE)\n","        elif hasattr(index, \"setNumProbes\"):\n","            index.setNumProbes(max(1, FAISS_NPROBE))\n","\n","        index.add(X_np)\n","        self._faiss = index\n","        self._trained = True\n","\n","    def search(self, q_emb: torch.Tensor, topk: int):\n","        n = self.size\n","        if n == 0:\n","            return (\n","                torch.empty(q_emb.shape[0], 0, dtype=torch.int64, device=q_emb.device),\n","                torch.empty(q_emb.shape[0], 0, dtype=torch.float32, device=q_emb.device),\n","            )\n","\n","        X, _ = self._cat()\n","        q = q_emb.detach().to(\"cpu\", dtype=torch.float32).contiguous().numpy()\n","\n","        # Use FAISS if trained; otherwise brute force cosine sim\n","        if self.use_faiss and self._faiss is not None and self._trained:\n","            D, I = self._faiss.search(q, topk)\n","            idx = torch.from_numpy(I.astype(np.int64)).to(q_emb.device)\n","            dist = torch.from_numpy(D.astype(np.float32)).to(q_emb.device)  # L2 distance\n","            return idx, dist\n","        else:\n","            xb = X.to(q_emb.device)  # [N, d]\n","            qn = F.normalize(q_emb, dim=-1)\n","            xn = F.normalize(xb, dim=-1)\n","            sim = torch.matmul(qn, xn.T)  # [B, N]\n","            dist, idx = torch.topk(sim, k=min(topk, xb.shape[0]), dim=-1, largest=True)\n","            # convert to \"distance-like\" (lower is better) for weighting\n","            dist = 1.0 - dist.clamp(-1, 1)  # in [0, 2]\n","            return idx, dist\n","\n","# -------- Losses ----------\n","def neighbor_mse(emb, mem_vecs, idx, weights=None):\n","    \"\"\"\n","    emb: [B, d], mem_vecs: [N, d] (same device), idx: [B, K], weights: [B, K] or None\n","    \"\"\"\n","    if idx.numel() == 0:\n","        return emb.new_tensor(0.0)\n","    knn = mem_vecs[idx]                       # [B, K, d]\n","    if weights is not None:\n","        w = (weights + 1e-8)\n","        w = w / w.sum(dim=1, keepdim=True)    # [B, K]\n","        centroid = (knn * w.unsqueeze(-1)).sum(dim=1)  # [B, d]\n","    else:\n","        centroid = knn.mean(dim=1)            # [B, d]\n","    return F.mse_loss(emb, centroid)\n","\n","def info_nce(emb, mem_vecs, idx, temp=TEMP):\n","    if idx.numel() == 0:\n","        return emb.new_tensor(0.0)\n","    B, K = idx.shape\n","    knn = mem_vecs[idx]  # [B, K, d]\n","    q = F.normalize(emb, dim=-1).unsqueeze(1)           # [B, 1, d]\n","    k_all = F.normalize(knn, dim=-1)                    # [B, K, d]\n","    pos = (q * k_all[:, :1]).sum(-1) / temp             # [B, 1]\n","    neg = (q * k_all[:, 1:]).sum(-1) / temp             # [B, K-1]\n","    logits = torch.cat([pos, neg], dim=1)\n","    labels = torch.zeros(B, dtype=torch.long, device=emb.device)\n","    return F.cross_entropy(logits, labels)\n","\n","# -------- Training ----------\n","class Runner:\n","    def __init__(self):\n","        self.model = Encoder(D_IN, D_HID, D_OUT).to(device)\n","        self._compiled = False\n","        if USE_COMPILE and hasattr(torch, \"compile\"):\n","            try:\n","                self.model = torch.compile(self.model, mode=COMPILE_MODE, fullgraph=False)\n","                self._compiled = True\n","            except Exception:\n","                self._compiled = False\n","\n","        # Optimizer: try 8-bit AdamW if available, else PyTorch AdamW(foreach=True)\n","        self._use_bnb = False\n","        if USE_BNB_8BIT and bnb is not None:\n","            try:\n","                self.opt = bnb.optim.AdamW8bit(self.model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","                self._use_bnb = True\n","            except Exception:\n","                pass\n","        if not self._use_bnb:\n","            self.opt = torch.optim.AdamW(\n","                self.model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY,\n","                eps=1e-8, betas=(0.9, 0.95), foreach=True, fused=False\n","            )\n","\n","        # AMP scaler: only needed for fp16; bf16 runs without scaler\n","        self.scaler = torch.cuda.amp.GradScaler(\n","            enabled=(use_amp and amp_dtype == torch.float16)\n","        )\n","\n","        self.mem = MemoryBank(D_OUT, use_faiss=True, ivfpq=FAISS_USE_IVFPQ)\n","        self.ckpt_dir = os.path.join(SAVE_ROOT, \"graph_world_runs\", RUN_NAME)\n","        os.makedirs(self.ckpt_dir, exist_ok=True)\n","\n","        # cache of CPU memory vectors as a single tensor for fast gather\n","        self._cpu_cat = None\n","\n","    def _refresh_cpu_cat(self):\n","        X, _ = self.mem._cat()\n","        if X is None:\n","            self._cpu_cat = None\n","        else:\n","            self._cpu_cat = X.to(device)\n","\n","    def train(self):\n","        t0 = time.time()\n","        print(f\"Device: {device} ({cuda_name}) | TF32={getattr(torch.backends.cuda.matmul, 'allow_tf32', False)} \"\n","              f\"| AMP_dtype={'bf16' if amp_dtype==torch.bfloat16 else 'fp16'} \"\n","              f\"| compile={'on' if self._compiled else 'off'} | bnb8bit={'on' if self._use_bnb else 'off'}\")\n","        print(f\"FAISS available: {bool(faiss)} | IVF-PQ={'on' if (bool(faiss) and FAISS_USE_IVFPQ) else 'off'}\")\n","\n","        global_steps = 0\n","        for ep in range(1, EPISODES+1):\n","            ep_t0 = time.time()\n","            loss_accum = 0.0\n","            step_times = []\n","\n","            for step in range(1, STEPS_PER_EP+1):\n","                st = time.time()\n","                x, y = get_batch(BATCH)  # [B, D_IN], labels (synthetic)\n","\n","                # Forward with AMP\n","                with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n","                    emb = self.model(x)  # [B, D_OUT]\n","\n","                # --- warm start: seed memory before the first real update ---\n","                if self.mem.size == 0:\n","                    self.mem.add(emb.detach(), labels=y.detach().clone())\n","                    global_steps += 1\n","                    if step == 1 and ep == 1:\n","                        print(\"  - warm start: seeded memory with first batch\")\n","                    step_times.append(time.time() - st)\n","                    continue\n","\n","                # Retrieval against current memory\n","                if (self._cpu_cat is None) or (self._cpu_cat.shape[0] != self.mem.size):\n","                    self._refresh_cpu_cat()\n","\n","                idx, dist = self.mem.search(emb, TOPK)\n","\n","                # distance-weighted centroid (nearer neighbors pull harder)\n","                weights = None\n","                if dist.numel() > 0:\n","                    weights = 1.0 / (dist + 1e-6)\n","\n","                mse = neighbor_mse(emb, self._cpu_cat, idx, weights=weights)\n","                nce = info_nce(emb, self._cpu_cat, idx, temp=TEMP)\n","                loss = LAMBDA_MSE * mse + LAMBDA_CONTRAST * nce\n","\n","                # step\n","                self.opt.zero_grad(set_to_none=True)\n","                if self.scaler.is_enabled():\n","                    self.scaler.scale(loss).backward()\n","                    self.scaler.step(self.opt)\n","                    self.scaler.update()\n","                else:\n","                    loss.backward()\n","                    self.opt.step()\n","\n","                # add to memory after update\n","                self.mem.add(emb.detach(), labels=y.detach().clone())\n","                global_steps += 1\n","\n","                # opportunistic reindex\n","                self.mem.maybe_reindex(force=False)\n","\n","                # logging\n","                loss_accum += float(loss.detach().cpu())\n","                step_times.append(time.time() - st)\n","                if step % 10 == 0:\n","                    avg_step = sum(step_times[-10:]) / min(10, len(step_times))\n","                    print(f\"  - step {step:02d}/{STEPS_PER_EP} | loss={loss_accum/step:.6f} \"\n","                          f\"| mem={self.mem.size} | avg_step={avg_step:.3f}s\")\n","\n","            # episode end: stronger index rebuild for freshness\n","            self.mem.maybe_reindex(force=True)\n","            if (self._cpu_cat is None) or (self._cpu_cat.shape[0] != self.mem.size):\n","                self._refresh_cpu_cat()\n","\n","            mean_loss = loss_accum / STEPS_PER_EP if STEPS_PER_EP > 0 else float('nan')\n","            nodes = self.mem.size\n","            print(f\"Episode {ep:02d} done: mean_loss={mean_loss:.6f} | nodes={nodes}\")\n","\n","            if CHECKPOINT_EVERY_EP:\n","                base_model = self.model._orig_mod if getattr(self, \"_compiled\", False) else self.model\n","                ckpt = {\n","                    \"model\": base_model.state_dict(),\n","                    \"opt\": self.opt.state_dict(),\n","                    \"cfg\": dict(\n","                        D_IN=D_IN, D_HID=D_HID, D_OUT=D_OUT,\n","                        LR=LR, WD=WEIGHT_DECAY, PRESET=PRESET, TOPK=TOPK\n","                    ),\n","                    \"mem_size\": self.mem.size\n","                }\n","                path = os.path.join(self.ckpt_dir, f\"ckpt_ep{ep:02d}.pt\")\n","                torch.save(ckpt, path)\n","\n","            # modest sleep yields more stable wallclock reporting on shared VMs\n","            time.sleep(0.01)\n","\n","        elapsed = time.time() - t0\n","        print(f\"Done. Elapsed: {elapsed:.1f}s | preset={PRESET}\")\n","        print(f\"Artifacts in: {self.ckpt_dir}\")\n","\n","# ---- Run ----\n","runner = Runner()\n","runner.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":15173,"status":"ok","timestamp":1758219556041,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"},"user_tz":-180},"id":"BCkWf2WuCCN6","outputId":"0f0a3160-7355-4fcf-9e3b-fae900c6d579"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda (NVIDIA A100-SXM4-40GB) | TF32=True | AMP_dtype=bf16 | compile=on | bnb8bit=off\n","FAISS available: False | IVF-PQ=off\n","  - warm start: seeded memory with first batch\n","  - step 10/40 | loss=0.411585 | mem=2560 | avg_step=0.064s\n","  - step 20/40 | loss=0.451531 | mem=5120 | avg_step=0.007s\n","  - step 30/40 | loss=0.469928 | mem=7680 | avg_step=0.007s\n","  - step 40/40 | loss=0.480150 | mem=10240 | avg_step=0.008s\n","Episode 01 done: mean_loss=0.480150 | nodes=10240\n","  - step 10/40 | loss=0.512467 | mem=12800 | avg_step=0.008s\n","  - step 20/40 | loss=0.512739 | mem=15360 | avg_step=0.010s\n","  - step 30/40 | loss=0.512889 | mem=17920 | avg_step=0.011s\n","  - step 40/40 | loss=0.513038 | mem=20480 | avg_step=0.012s\n","Episode 02 done: mean_loss=0.513038 | nodes=20480\n","  - step 10/40 | loss=0.514119 | mem=23040 | avg_step=0.014s\n","  - step 20/40 | loss=0.514562 | mem=25600 | avg_step=0.014s\n","  - step 30/40 | loss=0.514909 | mem=28160 | avg_step=0.015s\n","  - step 40/40 | loss=0.515201 | mem=30720 | avg_step=0.017s\n","Episode 03 done: mean_loss=0.515201 | nodes=30720\n","  - step 10/40 | loss=0.516319 | mem=33280 | avg_step=0.017s\n","  - step 20/40 | loss=0.516429 | mem=35840 | avg_step=0.019s\n","  - step 30/40 | loss=0.516457 | mem=38400 | avg_step=0.022s\n","  - step 40/40 | loss=0.516318 | mem=40960 | avg_step=0.021s\n","Episode 04 done: mean_loss=0.516318 | nodes=40960\n","  - step 10/40 | loss=0.514309 | mem=43520 | avg_step=0.022s\n","  - step 20/40 | loss=0.513582 | mem=46080 | avg_step=0.024s\n","  - step 30/40 | loss=0.513517 | mem=48640 | avg_step=0.028s\n","  - step 40/40 | loss=0.513188 | mem=51200 | avg_step=0.029s\n","Episode 05 done: mean_loss=0.513188 | nodes=51200\n","  - step 10/40 | loss=0.511507 | mem=53760 | avg_step=0.030s\n","  - step 20/40 | loss=0.511842 | mem=56320 | avg_step=0.029s\n","  - step 30/40 | loss=0.511700 | mem=58880 | avg_step=0.028s\n","  - step 40/40 | loss=0.511561 | mem=61440 | avg_step=0.029s\n","Episode 06 done: mean_loss=0.511561 | nodes=61440\n","  - step 10/40 | loss=0.512944 | mem=64000 | avg_step=0.029s\n","  - step 20/40 | loss=0.511838 | mem=66560 | avg_step=0.038s\n","  - step 30/40 | loss=0.511259 | mem=69120 | avg_step=0.052s\n","  - step 40/40 | loss=0.511616 | mem=71680 | avg_step=0.056s\n","Episode 07 done: mean_loss=0.511616 | nodes=71680\n","  - step 10/40 | loss=0.511996 | mem=74240 | avg_step=0.056s\n","  - step 20/40 | loss=0.513663 | mem=76800 | avg_step=0.062s\n","  - step 30/40 | loss=0.514843 | mem=79360 | avg_step=0.064s\n","  - step 40/40 | loss=0.515820 | mem=81920 | avg_step=0.066s\n","Episode 08 done: mean_loss=0.515820 | nodes=81920\n","  - step 10/40 | loss=0.518811 | mem=84480 | avg_step=0.065s\n","  - step 20/40 | loss=0.518836 | mem=87040 | avg_step=0.068s\n","  - step 30/40 | loss=0.518855 | mem=89600 | avg_step=0.072s\n","  - step 40/40 | loss=0.518884 | mem=92160 | avg_step=0.075s\n","Episode 09 done: mean_loss=0.518884 | nodes=92160\n","  - step 10/40 | loss=0.519013 | mem=94720 | avg_step=0.074s\n","  - step 20/40 | loss=0.519011 | mem=97280 | avg_step=0.081s\n","  - step 30/40 | loss=0.518988 | mem=99840 | avg_step=0.083s\n","  - step 40/40 | loss=0.518959 | mem=102400 | avg_step=0.086s\n","Episode 10 done: mean_loss=0.518959 | nodes=102400\n","Done. Elapsed: 15.4s | preset=FAST_24CU\n","Artifacts in: /content/graph_world_runs/glrm_singlecell\n"]}],"source":["# @title\n","#  GLRM: single-cell Colab runner (updated AMP + optional FAISS auto-install)\n","# - New AMP API: torch.amp.autocast / torch.amp.GradScaler(\"cuda\")\n","# - Optional AUTO_INSTALL_FAISS for Colab GPU (faiss-gpu-cu12), falls back to faiss-cpu\n","# - torch.compile(..., mode=\"reduce-overhead\"), AdamW(foreach) or bnb 8-bit\n","# - In-memory vector store with FAISS IVF-PQ (GPU/CPU) when available\n","# - Distance-weighted neighbor centroid + InfoNCE-ish contrast\n","# - Clean logs, per-episode ckpts, GMM toy stream (swap get_batch for real data)\n","\n","# ========================== CONFIG ==========================\n","PRESET = \"FAST_24CU\"   # \"FAST_24CU\", \"BALANCED\", \"MAX_60CU\"\n","SEED = 123\n","MOUNT_DRIVE = False\n","RUN_NAME = \"glrm_singlecell\"\n","SAVE_ROOT = \"/content/drive/MyDrive\" if MOUNT_DRIVE else \"/content\"\n","CHECKPOINT_EVERY_EP = True\n","\n","# Model/optim\n","D_IN = 128            # input feature dim (synthetic stream default)\n","D_HID = 256\n","D_OUT = 128           # embedding dim\n","LR = 3e-3\n","WEIGHT_DECAY = 1e-2\n","USE_BNB_8BIT = True   # try bitsandbytes AdamW8bit if present\n","USE_COMPILE = True    # torch.compile for lower overhead\n","COMPILE_MODE = \"reduce-overhead\"  # good for many small steps\n","\n","# Training schedule by preset\n","PRESETS = {\n","    \"FAST_24CU\": dict(EPISODES=10, STEPS_PER_EP=40, BATCH=256, K=8),\n","    \"BALANCED\":  dict(EPISODES=20, STEPS_PER_EP=60, BATCH=384, K=16),\n","    \"MAX_60CU\":  dict(EPISODES=30, STEPS_PER_EP=80, BATCH=512, K=32),\n","}\n","S = PRESETS[PRESET]\n","EPISODES, STEPS_PER_EP, BATCH, K_NEI = S[\"EPISODES\"], S[\"STEPS_PER_EP\"], S[\"BATCH\"], S[\"K\"]\n","\n","# Memory / FAISS\n","AUTO_INSTALL_FAISS = False         # flip to True to auto-install faiss on Colab\n","REINDEX_EVERY_STEPS = 400          # rebuild IVF-PQ roughly every N adds (or at episode end)\n","FAISS_USE_IVFPQ = True             # IVF-PQ (fast & memory-light) when faiss is present\n","FAISS_NLIST = 1024                 # coarse centroids (tune with data scale)\n","FAISS_M = 16                       # PQ subquantizers\n","FAISS_NBITS = 8                    # bits per subvector\n","FAISS_NPROBE = 32                  # probes at search (latency/recall knob)\n","TOPK = K_NEI                       # neighbors to retrieve from memory bank\n","\n","# Loss\n","LAMBDA_MSE = 1.0                   # pull embedding to neighbor centroid\n","LAMBDA_CONTRAST = 0.25             # InfoNCE-ish term over retrieved + random negatives\n","TEMP = 0.07                        # temperature for contrastive\n","\n","# ============================================================\n","import os, sys, math, time, random, subprocess\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# -------- Helpers ----------\n","def _lazy_import(name):\n","    try:\n","        return __import__(name)\n","    except Exception:\n","        return None\n","\n","def _maybe_install_faiss():\n","    # Try GPU wheel first when CUDA 12.x likely on Colab; fallback to CPU wheel.\n","    try:\n","        print(\"Attempting to install FAISS (gpu->cpu fallback)...\")\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-gpu-cu12\"])\n","    except Exception:\n","        try:\n","            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-cpu\"])\n","        except Exception as e:\n","            print(f\"FAISS install failed: {e}\")\n","\n","faiss = _lazy_import(\"faiss\") or _lazy_import(\"faiss_gpu\") or _lazy_import(\"faiss_cpu\")\n","bnb = _lazy_import(\"bitsandbytes\")\n","\n","# -------- Colab / Drive ----------\n","IN_COLAB = \"google.colab\" in sys.modules\n","if MOUNT_DRIVE and IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","\n","# -------- Repro & device ----------\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cuda_name = torch.cuda.get_device_name(0) if device.type == \"cuda\" else \"CPU\"\n","bf16_ok = (device.type == \"cuda\") and torch.cuda.is_bf16_supported()\n","amp_dtype = torch.bfloat16 if bf16_ok else torch.float16\n","amp_device = \"cuda\" if device.type == \"cuda\" else \"cpu\"\n","use_amp = (device.type == \"cuda\")  # GPU AMP; keep CPU off by default for simplicity\n","\n","# Enable TF32 on Ampere+ where available (matrix-multiply fast path)\n","try:\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    torch.set_float32_matmul_precision(\"high\")\n","except Exception:\n","    pass\n","\n","# Speedy CuDNN heuristics for convs (safe here, no determinism requirement)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.benchmark = True\n","\n","# Optional: try to auto-install FAISS if requested and missing\n","if AUTO_INSTALL_FAISS and (faiss is None):\n","    _maybe_install_faiss()\n","    faiss = _lazy_import(\"faiss\") or _lazy_import(\"faiss_gpu\") or _lazy_import(\"faiss_cpu\")\n","\n","# -------- Model ----------\n","class Encoder(nn.Module):\n","    def __init__(self, d_in=D_IN, d_hid=D_HID, d_out=D_OUT):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d_in, d_hid),\n","            nn.GELU(),\n","            nn.Linear(d_hid, d_out),\n","        )\n","        with torch.no_grad():\n","            for m in self.modules():\n","                if isinstance(m, nn.Linear):\n","                    nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n","                    nn.init.zeros_(m.bias)\n","\n","    def forward(self, x):\n","        return F.normalize(self.net(x), dim=-1)\n","\n","# -------- Synthetic stream (GMM) ----------\n","class GMMStream:\n","    def __init__(self, d=D_IN, k=32, std=0.3):\n","        self.d, self.k, self.std = d, k, std\n","        rng = np.random.default_rng(SEED)\n","        self.means = rng.normal(size=(k, d)).astype(np.float32)\n","        self.assign = rng\n","\n","    def sample(self, n):\n","        idx = self.assign.integers(0, self.k, size=(n,))\n","        base = self.means[idx]\n","        noise = self.assign.normal(scale=self.std, size=(n, self.d)).astype(np.float32)\n","        x = base + noise\n","        return torch.from_numpy(x), torch.from_numpy(idx.astype(np.int64))\n","\n","gmm = GMMStream(d=D_IN, k=32, std=0.3)\n","\n","def get_batch(bs=BATCH):\n","    x, y = gmm.sample(bs)\n","    if device.type == \"cuda\":\n","        x = x.pin_memory()\n","        y = y.pin_memory()\n","    return x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n","\n","# -------- Memory bank with optional FAISS index ----------\n","class MemoryBank:\n","    def __init__(self, d, use_faiss=True, ivfpq=True):\n","        self.d = d\n","        self.use_faiss = bool(faiss) and use_faiss\n","        self.ivfpq = ivfpq\n","        self._vecs = []   # list of CPU tensors (fp32)\n","        self._labels = [] # optional labels\n","        self._faiss = None\n","        self._trained = False\n","        self._added = 0\n","\n","    @property\n","    def size(self):\n","        return sum(v.shape[0] for v in self._vecs)\n","\n","    def add(self, vecs: torch.Tensor, labels: torch.Tensor = None):\n","        vc = vecs.detach().to(\"cpu\", dtype=torch.float32).contiguous()\n","        self._vecs.append(vc)\n","        if labels is not None:\n","            self._labels.append(labels.detach().to(\"cpu\"))\n","        self._added += vc.shape[0]\n","\n","    def _cat(self):\n","        if not self._vecs:\n","            return None, None\n","        X = torch.cat(self._vecs, dim=0)\n","        y = torch.cat(self._labels, dim=0) if self._labels else None\n","        return X, y\n","\n","    def maybe_reindex(self, force=False):\n","        if not self.use_faiss:\n","            return\n","        if not force and self._added < REINDEX_EVERY_STEPS:\n","            return\n","        self._added = 0\n","        X, _ = self._cat()\n","        if X is None or X.shape[0] < max(FAISS_NLIST, 1024):\n","            self._faiss = None\n","            self._trained = False\n","            return\n","\n","        X_np = X.numpy()\n","        d = X_np.shape[1]\n","\n","        if self.ivfpq:\n","            quantizer = faiss.IndexFlatL2(d)\n","            index = faiss.IndexIVFPQ(quantizer, d, FAISS_NLIST, FAISS_M, FAISS_NBITS)\n","        else:\n","            quantizer = faiss.IndexFlatL2(d)\n","            index = faiss.IndexIVFFlat(quantizer, d, FAISS_NLIST, faiss.METRIC_L2)\n","\n","        if hasattr(faiss, \"StandardGpuResources\") and torch.cuda.is_available():\n","            try:\n","                res = faiss.StandardGpuResources()\n","                index = faiss.index_cpu_to_gpu(res, 0, index)\n","            except Exception:\n","                pass\n","\n","        index.train(X_np)\n","        # robust probe setting across CPU/GPU bindings\n","        if hasattr(index, \"nprobe\"):\n","            index.nprobe = max(1, FAISS_NPROBE)\n","        elif hasattr(index, \"setNumProbes\"):\n","            index.setNumProbes(max(1, FAISS_NPROBE))\n","\n","        index.add(X_np)\n","        self._faiss = index\n","        self._trained = True\n","\n","    def search(self, q_emb: torch.Tensor, topk: int):\n","        n = self.size\n","        if n == 0:\n","            return (\n","                torch.empty(q_emb.shape[0], 0, dtype=torch.int64, device=q_emb.device),\n","                torch.empty(q_emb.shape[0], 0, dtype=torch.float32, device=q_emb.device),\n","            )\n","\n","        X, _ = self._cat()\n","        q = q_emb.detach().to(\"cpu\", dtype=torch.float32).contiguous().numpy()\n","\n","        if self.use_faiss and self._faiss is not None and self._trained:\n","            D, I = self._faiss.search(q, topk)\n","            idx = torch.from_numpy(I.astype(np.int64)).to(q_emb.device)\n","            dist = torch.from_numpy(D.astype(np.float32)).to(q_emb.device)  # L2 distance\n","            return idx, dist\n","        else:\n","            xb = X.to(q_emb.device)  # [N, d]\n","            qn = F.normalize(q_emb, dim=-1)\n","            xn = F.normalize(xb, dim=-1)\n","            sim = torch.matmul(qn, xn.T)  # [B, N]\n","            dist, idx = torch.topk(sim, k=min(topk, xb.shape[0]), dim=-1, largest=True)\n","            dist = 1.0 - dist.clamp(-1, 1)  # cosine -> distance-like\n","            return idx, dist\n","\n","# -------- Losses ----------\n","def neighbor_mse(emb, mem_vecs, idx, weights=None):\n","    if idx.numel() == 0:\n","        return emb.new_tensor(0.0)\n","    knn = mem_vecs[idx]                       # [B, K, d]\n","    if weights is not None:\n","        w = (weights + 1e-8)\n","        w = w / w.sum(dim=1, keepdim=True)    # [B, K]\n","        centroid = (knn * w.unsqueeze(-1)).sum(dim=1)\n","    else:\n","        centroid = knn.mean(dim=1)\n","    return F.mse_loss(emb, centroid)\n","\n","def info_nce(emb, mem_vecs, idx, temp=TEMP):\n","    if idx.numel() == 0:\n","        return emb.new_tensor(0.0)\n","    B, K = idx.shape\n","    knn = mem_vecs[idx]                                # [B, K, d]\n","    q = F.normalize(emb, dim=-1).unsqueeze(1)          # [B, 1, d]\n","    k_all = F.normalize(knn, dim=-1)                   # [B, K, d]\n","    pos = (q * k_all[:, :1]).sum(-1) / temp            # [B, 1]\n","    neg = (q * k_all[:, 1:]).sum(-1) / temp            # [B, K-1]\n","    logits = torch.cat([pos, neg], dim=1)\n","    labels = torch.zeros(B, dtype=torch.long, device=emb.device)\n","    return F.cross_entropy(logits, labels)\n","\n","# -------- Training ----------\n","class Runner:\n","    def __init__(self):\n","        self.model = Encoder(D_IN, D_HID, D_OUT).to(device)\n","        self._compiled = False\n","        if USE_COMPILE and hasattr(torch, \"compile\"):\n","            try:\n","                self.model = torch.compile(self.model, mode=COMPILE_MODE, fullgraph=False)\n","                self._compiled = True\n","            except Exception:\n","                self._compiled = False\n","\n","        # Optimizer: try 8-bit AdamW if available, else PyTorch AdamW(foreach=True)\n","        self._use_bnb = False\n","        if USE_BNB_8BIT and bnb is not None:\n","            try:\n","                self.opt = bnb.optim.AdamW8bit(self.model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","                self._use_bnb = True\n","            except Exception:\n","                pass\n","        if not self._use_bnb:\n","            self.opt = torch.optim.AdamW(\n","                self.model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY,\n","                eps=1e-8, betas=(0.9, 0.95), foreach=True, fused=False\n","            )\n","\n","        # AMP scaler: only needed for fp16; bf16 runs without scaler\n","        self.scaler = torch.amp.GradScaler(\n","            amp_device,\n","            enabled=(use_amp and amp_dtype == torch.float16)\n","        )\n","\n","        self.mem = MemoryBank(D_OUT, use_faiss=True, ivfpq=FAISS_USE_IVFPQ)\n","        self.ckpt_dir = os.path.join(SAVE_ROOT, \"graph_world_runs\", RUN_NAME)\n","        os.makedirs(self.ckpt_dir, exist_ok=True)\n","\n","        # cache of memory vectors on device for fast gather\n","        self._dev_mem = None\n","\n","    def _refresh_dev_mem(self):\n","        X, _ = self.mem._cat()\n","        self._dev_mem = None if X is None else X.to(device)\n","\n","    def train(self):\n","        t0 = time.time()\n","        print(f\"Device: {device} ({cuda_name}) | TF32={getattr(torch.backends.cuda.matmul, 'allow_tf32', False)} \"\n","              f\"| AMP_dtype={'bf16' if amp_dtype==torch.bfloat16 else 'fp16'} \"\n","              f\"| compile={'on' if self._compiled else 'off'} | bnb8bit={'on' if self._use_bnb else 'off'}\")\n","        print(f\"FAISS available: {bool(faiss)} | IVF-PQ={'on' if (bool(faiss) and FAISS_USE_IVFPQ) else 'off'}\")\n","\n","        global_steps = 0\n","        for ep in range(1, EPISODES+1):\n","            ep_t0 = time.time()\n","            loss_accum = 0.0\n","            step_times = []\n","\n","            for step in range(1, STEPS_PER_EP+1):\n","                st = time.time()\n","                x, y = get_batch(BATCH)\n","\n","                # Forward with AMP (new API)\n","                with torch.amp.autocast(amp_device, dtype=amp_dtype, enabled=use_amp):\n","                    emb = self.model(x)\n","\n","                # Warm start: seed memory before the first update\n","                if self.mem.size == 0:\n","                    self.mem.add(emb.detach(), labels=y.detach().clone())\n","                    global_steps += 1\n","                    if step == 1 and ep == 1:\n","                        print(\"  - warm start: seeded memory with first batch\")\n","                    step_times.append(time.time() - st)\n","                    continue\n","\n","                # Retrieval against current memory\n","                if (self._dev_mem is None) or (self._dev_mem.shape[0] != self.mem.size):\n","                    self._refresh_dev_mem()\n","\n","                idx, dist = self.mem.search(emb, TOPK)\n","\n","                # distance-weighted centroid\n","                weights = None\n","                if dist.numel() > 0:\n","                    weights = 1.0 / (dist + 1e-6)\n","\n","                mse = neighbor_mse(emb, self._dev_mem, idx, weights=weights)\n","                nce = info_nce(emb, self._dev_mem, idx, temp=TEMP)\n","                loss = LAMBDA_MSE * mse + LAMBDA_CONTRAST * nce\n","\n","                # step\n","                self.opt.zero_grad(set_to_none=True)\n","                if self.scaler.is_enabled():\n","                    self.scaler.scale(loss).backward()\n","                    self.scaler.step(self.opt)\n","                    self.scaler.update()\n","                else:\n","                    loss.backward()\n","                    self.opt.step()\n","\n","                # Add to memory and opportunistically reindex\n","                self.mem.add(emb.detach(), labels=y.detach().clone())\n","                global_steps += 1\n","                self.mem.maybe_reindex(force=False)\n","\n","                # logging\n","                loss_accum += float(loss.detach().cpu())\n","                step_times.append(time.time() - st)\n","                if step % 10 == 0:\n","                    avg_step = sum(step_times[-10:]) / min(10, len(step_times))\n","                    print(f\"  - step {step:02d}/{STEPS_PER_EP} | loss={loss_accum/step:.6f} \"\n","                          f\"| mem={self.mem.size} | avg_step={avg_step:.3f}s\")\n","\n","            # episode end: stronger index rebuild for freshness\n","            self.mem.maybe_reindex(force=True)\n","            if (self._dev_mem is None) or (self._dev_mem.shape[0] != self.mem.size):\n","                self._refresh_dev_mem()\n","\n","            mean_loss = loss_accum / STEPS_PER_EP if STEPS_PER_EP > 0 else float('nan')\n","            nodes = self.mem.size\n","            print(f\"Episode {ep:02d} done: mean_loss={mean_loss:.6f} | nodes={nodes}\")\n","\n","            if CHECKPOINT_EVERY_EP:\n","                base_model = self.model._orig_mod if self._compiled else self.model\n","                ckpt = {\n","                    \"model\": base_model.state_dict(),\n","                    \"opt\": self.opt.state_dict(),\n","                    \"cfg\": dict(\n","                        D_IN=D_IN, D_HID=D_HID, D_OUT=D_OUT,\n","                        LR=LR, WD=WEIGHT_DECAY, PRESET=PRESET, TOPK=TOPK\n","                    ),\n","                    \"mem_size\": self.mem.size\n","                }\n","                path = os.path.join(self.ckpt_dir, f\"ckpt_ep{ep:02d}.pt\")\n","                torch.save(ckpt, path)\n","\n","            time.sleep(0.01)\n","\n","        elapsed = time.time() - t0\n","        print(f\"Done. Elapsed: {elapsed:.1f}s | preset={PRESET}\")\n","        print(f\"Artifacts in: {self.ckpt_dir}\")\n","\n","# ---- Run ----\n","runner = Runner()\n","runner.train()"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdhuz7IrIQjX","cellView":"form","executionInfo":{"status":"ok","timestamp":1758260307413,"user_tz":-180,"elapsed":2548249,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"}},"outputId":"d5c6e6e0-e65f-477a-ba82-443de8014e76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda (NVIDIA A100-SXM4-40GB) | TF32=True | AMP_dtype=bf16 | compile=on | bnb8bit=off\n","FAISS available: False | IVF-PQ=off\n","Preset=LONG_RUN | BATCH=512 | ACCUM_STEPS=2 | effective_batch=1024\n","  - up      50 | ep 001/200 step 101/200 | loss=0.668172 | mem=51712 | lr=3.04e-04 | avg_step=0.027s\n","Episode 001 done | nodes=102400\n","  - up     100 | ep 002/200 step 002/200 | loss=0.670986 | mem=103424 | lr=3.17e-04 | avg_step=0.060s\n","  - up     150 | ep 002/200 step 102/200 | loss=0.672354 | mem=154624 | lr=3.37e-04 | avg_step=0.094s\n","Episode 002 done | nodes=200000\n","  - up     200 | ep 003/200 step 002/200 | loss=1.585197 | mem=200000 | lr=3.66e-04 | avg_step=0.065s\n","  - up     250 | ep 003/200 step 102/200 | loss=1.501303 | mem=200000 | lr=4.03e-04 | avg_step=0.064s\n","Episode 003 done | nodes=200000\n","  - up     300 | ep 004/200 step 002/200 | loss=1.558031 | mem=200000 | lr=4.47e-04 | avg_step=0.060s\n","  - up     350 | ep 004/200 step 102/200 | loss=1.555750 | mem=200000 | lr=4.99e-04 | avg_step=0.066s\n","Episode 004 done | nodes=200000\n","  - up     400 | ep 005/200 step 002/200 | loss=1.586027 | mem=200000 | lr=5.58e-04 | avg_step=0.060s\n","  - up     450 | ep 005/200 step 102/200 | loss=1.540369 | mem=200000 | lr=6.24e-04 | avg_step=0.067s\n","Episode 005 done | nodes=200000\n","  - up     500 | ep 006/200 step 002/200 | loss=1.527557 | mem=200000 | lr=6.96e-04 | avg_step=0.060s\n","  - up     550 | ep 006/200 step 102/200 | loss=1.553351 | mem=200000 | lr=7.74e-04 | avg_step=0.061s\n","Episode 006 done | nodes=200000\n","  - up     600 | ep 007/200 step 002/200 | loss=1.554356 | mem=200000 | lr=8.57e-04 | avg_step=0.062s\n","  - up     650 | ep 007/200 step 102/200 | loss=1.565250 | mem=200000 | lr=9.45e-04 | avg_step=0.062s\n","Episode 007 done | nodes=200000\n","  - up     700 | ep 008/200 step 002/200 | loss=1.591394 | mem=200000 | lr=1.04e-03 | avg_step=0.061s\n","  - up     750 | ep 008/200 step 102/200 | loss=1.581581 | mem=200000 | lr=1.13e-03 | avg_step=0.060s\n","Episode 008 done | nodes=200000\n","  - up     800 | ep 009/200 step 002/200 | loss=1.509099 | mem=200000 | lr=1.23e-03 | avg_step=0.061s\n","  - up     850 | ep 009/200 step 102/200 | loss=1.562982 | mem=200000 | lr=1.34e-03 | avg_step=0.061s\n","Episode 009 done | nodes=200000\n","  - up     900 | ep 010/200 step 002/200 | loss=1.579912 | mem=200000 | lr=1.44e-03 | avg_step=0.061s\n","  - up     950 | ep 010/200 step 102/200 | loss=1.557344 | mem=200000 | lr=1.55e-03 | avg_step=0.061s\n","Episode 010 done | nodes=200000\n","  - up    1000 | ep 011/200 step 002/200 | loss=1.539688 | mem=200000 | lr=1.65e-03 | avg_step=0.064s\n","  - up    1050 | ep 011/200 step 102/200 | loss=1.535484 | mem=200000 | lr=1.76e-03 | avg_step=0.061s\n","Episode 011 done | nodes=200000\n","  - up    1100 | ep 012/200 step 002/200 | loss=1.495150 | mem=200000 | lr=1.86e-03 | avg_step=0.068s\n","  - up    1150 | ep 012/200 step 102/200 | loss=1.508318 | mem=200000 | lr=1.97e-03 | avg_step=0.061s\n","Episode 012 done | nodes=200000\n","  - up    1200 | ep 013/200 step 002/200 | loss=1.529956 | mem=200000 | lr=2.07e-03 | avg_step=0.064s\n","  - up    1250 | ep 013/200 step 102/200 | loss=1.500521 | mem=200000 | lr=2.17e-03 | avg_step=0.060s\n","Episode 013 done | nodes=200000\n","  - up    1300 | ep 014/200 step 002/200 | loss=1.502732 | mem=200000 | lr=2.26e-03 | avg_step=0.061s\n","  - up    1350 | ep 014/200 step 102/200 | loss=1.548680 | mem=200000 | lr=2.36e-03 | avg_step=0.061s\n","Episode 014 done | nodes=200000\n","  - up    1400 | ep 015/200 step 002/200 | loss=1.522223 | mem=200000 | lr=2.44e-03 | avg_step=0.060s\n","  - up    1450 | ep 015/200 step 102/200 | loss=1.536690 | mem=200000 | lr=2.53e-03 | avg_step=0.061s\n","Episode 015 done | nodes=200000\n","  - up    1500 | ep 016/200 step 002/200 | loss=1.533679 | mem=200000 | lr=2.61e-03 | avg_step=0.061s\n","  - up    1550 | ep 016/200 step 102/200 | loss=1.533366 | mem=200000 | lr=2.68e-03 | avg_step=0.061s\n","Episode 016 done | nodes=200000\n","  - up    1600 | ep 017/200 step 002/200 | loss=1.543681 | mem=200000 | lr=2.74e-03 | avg_step=0.062s\n","  - up    1650 | ep 017/200 step 102/200 | loss=1.558690 | mem=200000 | lr=2.80e-03 | avg_step=0.064s\n","Episode 017 done | nodes=200000\n","  - up    1700 | ep 018/200 step 002/200 | loss=1.536900 | mem=200000 | lr=2.85e-03 | avg_step=0.061s\n","  - up    1750 | ep 018/200 step 102/200 | loss=1.560329 | mem=200000 | lr=2.90e-03 | avg_step=0.068s\n","Episode 018 done | nodes=200000\n","  - up    1800 | ep 019/200 step 002/200 | loss=1.522180 | mem=200000 | lr=2.93e-03 | avg_step=0.061s\n","  - up    1850 | ep 019/200 step 102/200 | loss=1.548152 | mem=200000 | lr=2.96e-03 | avg_step=0.063s\n","Episode 019 done | nodes=200000\n","  - up    1900 | ep 020/200 step 002/200 | loss=1.555667 | mem=200000 | lr=2.98e-03 | avg_step=0.063s\n","  - up    1950 | ep 020/200 step 102/200 | loss=1.560672 | mem=200000 | lr=3.00e-03 | avg_step=0.063s\n","Episode 020 done | nodes=200000\n","  - up    2000 | ep 021/200 step 002/200 | loss=1.542981 | mem=200000 | lr=3.00e-03 | avg_step=0.061s\n","  - up    2050 | ep 021/200 step 102/200 | loss=1.587573 | mem=200000 | lr=3.00e-03 | avg_step=0.062s\n","Episode 021 done | nodes=200000\n","  - up    2100 | ep 022/200 step 002/200 | loss=1.527673 | mem=200000 | lr=3.00e-03 | avg_step=0.061s\n","  - up    2150 | ep 022/200 step 102/200 | loss=1.542441 | mem=200000 | lr=3.00e-03 | avg_step=0.063s\n","Episode 022 done | nodes=200000\n","  - up    2200 | ep 023/200 step 002/200 | loss=1.534935 | mem=200000 | lr=3.00e-03 | avg_step=0.061s\n","  - up    2250 | ep 023/200 step 102/200 | loss=1.556364 | mem=200000 | lr=3.00e-03 | avg_step=0.062s\n","Episode 023 done | nodes=200000\n","  - up    2300 | ep 024/200 step 002/200 | loss=1.583807 | mem=200000 | lr=3.00e-03 | avg_step=0.066s\n","  - up    2350 | ep 024/200 step 102/200 | loss=1.484307 | mem=200000 | lr=3.00e-03 | avg_step=0.061s\n","Episode 024 done | nodes=200000\n","  - up    2400 | ep 025/200 step 002/200 | loss=1.562199 | mem=200000 | lr=3.00e-03 | avg_step=0.069s\n","  - up    2450 | ep 025/200 step 102/200 | loss=1.542901 | mem=200000 | lr=3.00e-03 | avg_step=0.065s\n","Episode 025 done | nodes=200000\n","  - up    2500 | ep 026/200 step 002/200 | loss=1.525323 | mem=200000 | lr=2.99e-03 | avg_step=0.062s\n","  - up    2550 | ep 026/200 step 102/200 | loss=1.553778 | mem=200000 | lr=2.99e-03 | avg_step=0.061s\n","Episode 026 done | nodes=200000\n","  - up    2600 | ep 027/200 step 002/200 | loss=1.515344 | mem=200000 | lr=2.99e-03 | avg_step=0.061s\n","  - up    2650 | ep 027/200 step 102/200 | loss=1.553275 | mem=200000 | lr=2.99e-03 | avg_step=0.063s\n","Episode 027 done | nodes=200000\n","  - up    2700 | ep 028/200 step 002/200 | loss=1.538813 | mem=200000 | lr=2.99e-03 | avg_step=0.062s\n","  - up    2750 | ep 028/200 step 102/200 | loss=1.537964 | mem=200000 | lr=2.99e-03 | avg_step=0.061s\n","Episode 028 done | nodes=200000\n","  - up    2800 | ep 029/200 step 002/200 | loss=1.544580 | mem=200000 | lr=2.99e-03 | avg_step=0.061s\n","  - up    2850 | ep 029/200 step 102/200 | loss=1.533896 | mem=200000 | lr=2.98e-03 | avg_step=0.060s\n","Episode 029 done | nodes=200000\n","  - up    2900 | ep 030/200 step 002/200 | loss=1.518420 | mem=200000 | lr=2.98e-03 | avg_step=0.061s\n","  - up    2950 | ep 030/200 step 102/200 | loss=1.519013 | mem=200000 | lr=2.98e-03 | avg_step=0.065s\n","Episode 030 done | nodes=200000\n","  - up    3000 | ep 031/200 step 002/200 | loss=1.513772 | mem=200000 | lr=2.98e-03 | avg_step=0.062s\n","  - up    3050 | ep 031/200 step 102/200 | loss=1.551033 | mem=200000 | lr=2.97e-03 | avg_step=0.066s\n","Episode 031 done | nodes=200000\n","  - up    3100 | ep 032/200 step 002/200 | loss=1.530024 | mem=200000 | lr=2.97e-03 | avg_step=0.060s\n","  - up    3150 | ep 032/200 step 102/200 | loss=1.534454 | mem=200000 | lr=2.97e-03 | avg_step=0.069s\n","Episode 032 done | nodes=200000\n","  - up    3200 | ep 033/200 step 002/200 | loss=1.524148 | mem=200000 | lr=2.97e-03 | avg_step=0.061s\n","  - up    3250 | ep 033/200 step 102/200 | loss=1.551499 | mem=200000 | lr=2.96e-03 | avg_step=0.062s\n","Episode 033 done | nodes=200000\n","  - up    3300 | ep 034/200 step 002/200 | loss=1.520905 | mem=200000 | lr=2.96e-03 | avg_step=0.061s\n","  - up    3350 | ep 034/200 step 102/200 | loss=1.552272 | mem=200000 | lr=2.96e-03 | avg_step=0.062s\n","Episode 034 done | nodes=200000\n","  - up    3400 | ep 035/200 step 002/200 | loss=1.471120 | mem=200000 | lr=2.96e-03 | avg_step=0.061s\n","  - up    3450 | ep 035/200 step 102/200 | loss=1.521554 | mem=200000 | lr=2.95e-03 | avg_step=0.061s\n","Episode 035 done | nodes=200000\n","  - up    3500 | ep 036/200 step 002/200 | loss=1.538703 | mem=200000 | lr=2.95e-03 | avg_step=0.063s\n","  - up    3550 | ep 036/200 step 102/200 | loss=1.571341 | mem=200000 | lr=2.95e-03 | avg_step=0.061s\n","Episode 036 done | nodes=200000\n","  - up    3600 | ep 037/200 step 002/200 | loss=1.530007 | mem=200000 | lr=2.94e-03 | avg_step=0.066s\n","  - up    3650 | ep 037/200 step 102/200 | loss=1.494146 | mem=200000 | lr=2.94e-03 | avg_step=0.064s\n","Episode 037 done | nodes=200000\n","  - up    3700 | ep 038/200 step 002/200 | loss=1.515419 | mem=200000 | lr=2.93e-03 | avg_step=0.066s\n","  - up    3750 | ep 038/200 step 102/200 | loss=1.552210 | mem=200000 | lr=2.93e-03 | avg_step=0.063s\n","Episode 038 done | nodes=200000\n","  - up    3800 | ep 039/200 step 002/200 | loss=1.500722 | mem=200000 | lr=2.93e-03 | avg_step=0.061s\n","  - up    3850 | ep 039/200 step 102/200 | loss=1.495946 | mem=200000 | lr=2.92e-03 | avg_step=0.061s\n","Episode 039 done | nodes=200000\n","  - up    3900 | ep 040/200 step 002/200 | loss=1.571606 | mem=200000 | lr=2.92e-03 | avg_step=0.061s\n","  - up    3950 | ep 040/200 step 102/200 | loss=1.505180 | mem=200000 | lr=2.91e-03 | avg_step=0.062s\n","Episode 040 done | nodes=200000\n","  - up    4000 | ep 041/200 step 002/200 | loss=1.526734 | mem=200000 | lr=2.91e-03 | avg_step=0.062s\n","  - up    4050 | ep 041/200 step 102/200 | loss=1.517113 | mem=200000 | lr=2.91e-03 | avg_step=0.062s\n","Episode 041 done | nodes=200000\n","  - up    4100 | ep 042/200 step 002/200 | loss=1.573067 | mem=200000 | lr=2.90e-03 | avg_step=0.061s\n","  - up    4150 | ep 042/200 step 102/200 | loss=1.555570 | mem=200000 | lr=2.90e-03 | avg_step=0.062s\n","Episode 042 done | nodes=200000\n","  - up    4200 | ep 043/200 step 002/200 | loss=1.567952 | mem=200000 | lr=2.89e-03 | avg_step=0.062s\n","  - up    4250 | ep 043/200 step 102/200 | loss=1.530781 | mem=200000 | lr=2.89e-03 | avg_step=0.068s\n","Episode 043 done | nodes=200000\n","  - up    4300 | ep 044/200 step 002/200 | loss=1.599542 | mem=200000 | lr=2.88e-03 | avg_step=0.061s\n","  - up    4350 | ep 044/200 step 102/200 | loss=1.591148 | mem=200000 | lr=2.88e-03 | avg_step=0.069s\n","Episode 044 done | nodes=200000\n","  - up    4400 | ep 045/200 step 002/200 | loss=1.526475 | mem=200000 | lr=2.87e-03 | avg_step=0.062s\n","  - up    4450 | ep 045/200 step 102/200 | loss=1.562292 | mem=200000 | lr=2.86e-03 | avg_step=0.063s\n","Episode 045 done | nodes=200000\n","  - up    4500 | ep 046/200 step 002/200 | loss=1.523028 | mem=200000 | lr=2.86e-03 | avg_step=0.062s\n","  - up    4550 | ep 046/200 step 102/200 | loss=1.537497 | mem=200000 | lr=2.85e-03 | avg_step=0.062s\n","Episode 046 done | nodes=200000\n","  - up    4600 | ep 047/200 step 002/200 | loss=1.507129 | mem=200000 | lr=2.85e-03 | avg_step=0.062s\n","  - up    4650 | ep 047/200 step 102/200 | loss=1.510193 | mem=200000 | lr=2.84e-03 | avg_step=0.062s\n","Episode 047 done | nodes=200000\n","  - up    4700 | ep 048/200 step 002/200 | loss=1.511665 | mem=200000 | lr=2.84e-03 | avg_step=0.062s\n","  - up    4750 | ep 048/200 step 102/200 | loss=1.532409 | mem=200000 | lr=2.83e-03 | avg_step=0.062s\n","Episode 048 done | nodes=200000\n","  - up    4800 | ep 049/200 step 002/200 | loss=1.528242 | mem=200000 | lr=2.82e-03 | avg_step=0.062s\n","  - up    4850 | ep 049/200 step 102/200 | loss=1.579914 | mem=200000 | lr=2.82e-03 | avg_step=0.062s\n","Episode 049 done | nodes=200000\n","  - up    4900 | ep 050/200 step 002/200 | loss=1.546818 | mem=200000 | lr=2.81e-03 | avg_step=0.068s\n","  - up    4950 | ep 050/200 step 102/200 | loss=1.589317 | mem=200000 | lr=2.81e-03 | avg_step=0.062s\n","Episode 050 done | nodes=200000\n","  - up    5000 | ep 051/200 step 002/200 | loss=1.547636 | mem=200000 | lr=2.80e-03 | avg_step=0.069s\n","  - up    5050 | ep 051/200 step 102/200 | loss=1.525229 | mem=200000 | lr=2.79e-03 | avg_step=0.062s\n","Episode 051 done | nodes=200000\n","  - up    5100 | ep 052/200 step 002/200 | loss=1.561929 | mem=200000 | lr=2.79e-03 | avg_step=0.061s\n","  - up    5150 | ep 052/200 step 102/200 | loss=1.516663 | mem=200000 | lr=2.78e-03 | avg_step=0.062s\n","Episode 052 done | nodes=200000\n","  - up    5200 | ep 053/200 step 002/200 | loss=1.518674 | mem=200000 | lr=2.77e-03 | avg_step=0.061s\n","  - up    5250 | ep 053/200 step 102/200 | loss=1.496634 | mem=200000 | lr=2.77e-03 | avg_step=0.064s\n","Episode 053 done | nodes=200000\n","  - up    5300 | ep 054/200 step 002/200 | loss=1.575830 | mem=200000 | lr=2.76e-03 | avg_step=0.061s\n","  - up    5350 | ep 054/200 step 102/200 | loss=1.474328 | mem=200000 | lr=2.75e-03 | avg_step=0.061s\n","Episode 054 done | nodes=200000\n","  - up    5400 | ep 055/200 step 002/200 | loss=1.505219 | mem=200000 | lr=2.74e-03 | avg_step=0.061s\n","  - up    5450 | ep 055/200 step 102/200 | loss=1.513208 | mem=200000 | lr=2.74e-03 | avg_step=0.061s\n","Episode 055 done | nodes=200000\n","  - up    5500 | ep 056/200 step 002/200 | loss=1.591425 | mem=200000 | lr=2.73e-03 | avg_step=0.062s\n","  - up    5550 | ep 056/200 step 102/200 | loss=1.539793 | mem=200000 | lr=2.72e-03 | avg_step=0.065s\n","Episode 056 done | nodes=200000\n","  - up    5600 | ep 057/200 step 002/200 | loss=1.557611 | mem=200000 | lr=2.71e-03 | avg_step=0.062s\n","  - up    5650 | ep 057/200 step 102/200 | loss=1.519022 | mem=200000 | lr=2.71e-03 | avg_step=0.070s\n","Episode 057 done | nodes=200000\n","  - up    5700 | ep 058/200 step 002/200 | loss=1.528532 | mem=200000 | lr=2.70e-03 | avg_step=0.061s\n","  - up    5750 | ep 058/200 step 102/200 | loss=1.566575 | mem=200000 | lr=2.69e-03 | avg_step=0.068s\n","Episode 058 done | nodes=200000\n","  - up    5800 | ep 059/200 step 002/200 | loss=1.523389 | mem=200000 | lr=2.68e-03 | avg_step=0.063s\n","  - up    5850 | ep 059/200 step 102/200 | loss=1.571108 | mem=200000 | lr=2.67e-03 | avg_step=0.062s\n","Episode 059 done | nodes=200000\n","  - up    5900 | ep 060/200 step 002/200 | loss=1.591257 | mem=200000 | lr=2.67e-03 | avg_step=0.061s\n","  - up    5950 | ep 060/200 step 102/200 | loss=1.496314 | mem=200000 | lr=2.66e-03 | avg_step=0.061s\n","Episode 060 done | nodes=200000\n","  - up    6000 | ep 061/200 step 002/200 | loss=1.548830 | mem=200000 | lr=2.65e-03 | avg_step=0.062s\n","  - up    6050 | ep 061/200 step 102/200 | loss=1.524920 | mem=200000 | lr=2.64e-03 | avg_step=0.062s\n","Episode 061 done | nodes=200000\n","  - up    6100 | ep 062/200 step 002/200 | loss=1.558573 | mem=200000 | lr=2.63e-03 | avg_step=0.061s\n","  - up    6150 | ep 062/200 step 102/200 | loss=1.589190 | mem=200000 | lr=2.62e-03 | avg_step=0.063s\n","Episode 062 done | nodes=200000\n","  - up    6200 | ep 063/200 step 002/200 | loss=1.524144 | mem=200000 | lr=2.61e-03 | avg_step=0.068s\n","  - up    6250 | ep 063/200 step 102/200 | loss=1.562700 | mem=200000 | lr=2.61e-03 | avg_step=0.062s\n","Episode 063 done | nodes=200000\n","  - up    6300 | ep 064/200 step 002/200 | loss=1.553602 | mem=200000 | lr=2.60e-03 | avg_step=0.069s\n","  - up    6350 | ep 064/200 step 102/200 | loss=1.523065 | mem=200000 | lr=2.59e-03 | avg_step=0.064s\n","Episode 064 done | nodes=200000\n","  - up    6400 | ep 065/200 step 002/200 | loss=1.537930 | mem=200000 | lr=2.58e-03 | avg_step=0.063s\n","  - up    6450 | ep 065/200 step 102/200 | loss=1.515754 | mem=200000 | lr=2.57e-03 | avg_step=0.062s\n","Episode 065 done | nodes=200000\n","  - up    6500 | ep 066/200 step 002/200 | loss=1.553215 | mem=200000 | lr=2.56e-03 | avg_step=0.062s\n","  - up    6550 | ep 066/200 step 102/200 | loss=1.604438 | mem=200000 | lr=2.55e-03 | avg_step=0.062s\n","Episode 066 done | nodes=200000\n","  - up    6600 | ep 067/200 step 002/200 | loss=1.551131 | mem=200000 | lr=2.54e-03 | avg_step=0.061s\n","  - up    6650 | ep 067/200 step 102/200 | loss=1.488600 | mem=200000 | lr=2.53e-03 | avg_step=0.062s\n","Episode 067 done | nodes=200000\n","  - up    6700 | ep 068/200 step 002/200 | loss=1.543850 | mem=200000 | lr=2.52e-03 | avg_step=0.061s\n","  - up    6750 | ep 068/200 step 102/200 | loss=1.562944 | mem=200000 | lr=2.51e-03 | avg_step=0.065s\n","Episode 068 done | nodes=200000\n","  - up    6800 | ep 069/200 step 002/200 | loss=1.496687 | mem=200000 | lr=2.50e-03 | avg_step=0.062s\n","  - up    6850 | ep 069/200 step 102/200 | loss=1.528980 | mem=200000 | lr=2.49e-03 | avg_step=0.070s\n","Episode 069 done | nodes=200000\n","  - up    6900 | ep 070/200 step 002/200 | loss=1.522939 | mem=200000 | lr=2.48e-03 | avg_step=0.061s\n","  - up    6950 | ep 070/200 step 102/200 | loss=1.514940 | mem=200000 | lr=2.47e-03 | avg_step=0.063s\n","Episode 070 done | nodes=200000\n","  - up    7000 | ep 071/200 step 002/200 | loss=1.518359 | mem=200000 | lr=2.46e-03 | avg_step=0.062s\n","  - up    7050 | ep 071/200 step 102/200 | loss=1.526387 | mem=200000 | lr=2.45e-03 | avg_step=0.065s\n","Episode 071 done | nodes=200000\n","  - up    7100 | ep 072/200 step 002/200 | loss=1.534774 | mem=200000 | lr=2.44e-03 | avg_step=0.063s\n","  - up    7150 | ep 072/200 step 102/200 | loss=1.571620 | mem=200000 | lr=2.43e-03 | avg_step=0.062s\n","Episode 072 done | nodes=200000\n","  - up    7200 | ep 073/200 step 002/200 | loss=1.535453 | mem=200000 | lr=2.42e-03 | avg_step=0.062s\n","  - up    7250 | ep 073/200 step 102/200 | loss=1.519746 | mem=200000 | lr=2.41e-03 | avg_step=0.062s\n","Episode 073 done | nodes=200000\n","  - up    7300 | ep 074/200 step 002/200 | loss=1.510244 | mem=200000 | lr=2.40e-03 | avg_step=0.062s\n","  - up    7350 | ep 074/200 step 102/200 | loss=1.538165 | mem=200000 | lr=2.39e-03 | avg_step=0.062s\n","Episode 074 done | nodes=200000\n","  - up    7400 | ep 075/200 step 002/200 | loss=1.506752 | mem=200000 | lr=2.38e-03 | avg_step=0.069s\n","  - up    7450 | ep 075/200 step 102/200 | loss=1.498035 | mem=200000 | lr=2.37e-03 | avg_step=0.062s\n","Episode 075 done | nodes=200000\n","  - up    7500 | ep 076/200 step 002/200 | loss=1.529152 | mem=200000 | lr=2.36e-03 | avg_step=0.069s\n","  - up    7550 | ep 076/200 step 102/200 | loss=1.517139 | mem=200000 | lr=2.35e-03 | avg_step=0.062s\n","Episode 076 done | nodes=200000\n","  - up    7600 | ep 077/200 step 002/200 | loss=1.532346 | mem=200000 | lr=2.34e-03 | avg_step=0.062s\n","  - up    7650 | ep 077/200 step 102/200 | loss=1.533246 | mem=200000 | lr=2.33e-03 | avg_step=0.061s\n","Episode 077 done | nodes=200000\n","  - up    7700 | ep 078/200 step 002/200 | loss=1.569124 | mem=200000 | lr=2.32e-03 | avg_step=0.062s\n","  - up    7750 | ep 078/200 step 102/200 | loss=1.524509 | mem=200000 | lr=2.31e-03 | avg_step=0.062s\n","Episode 078 done | nodes=200000\n","  - up    7800 | ep 079/200 step 002/200 | loss=1.557229 | mem=200000 | lr=2.30e-03 | avg_step=0.061s\n","  - up    7850 | ep 079/200 step 102/200 | loss=1.524938 | mem=200000 | lr=2.28e-03 | avg_step=0.062s\n","Episode 079 done | nodes=200000\n","  - up    7900 | ep 080/200 step 002/200 | loss=1.515293 | mem=200000 | lr=2.27e-03 | avg_step=0.062s\n","  - up    7950 | ep 080/200 step 102/200 | loss=1.578189 | mem=200000 | lr=2.26e-03 | avg_step=0.062s\n","Episode 080 done | nodes=200000\n","  - up    8000 | ep 081/200 step 002/200 | loss=1.572048 | mem=200000 | lr=2.25e-03 | avg_step=0.067s\n","  - up    8050 | ep 081/200 step 102/200 | loss=1.555965 | mem=200000 | lr=2.24e-03 | avg_step=0.067s\n","Episode 081 done | nodes=200000\n","  - up    8100 | ep 082/200 step 002/200 | loss=1.548850 | mem=200000 | lr=2.23e-03 | avg_step=0.063s\n","  - up    8150 | ep 082/200 step 102/200 | loss=1.517440 | mem=200000 | lr=2.22e-03 | avg_step=0.063s\n","Episode 082 done | nodes=200000\n","  - up    8200 | ep 083/200 step 002/200 | loss=1.536724 | mem=200000 | lr=2.20e-03 | avg_step=0.062s\n","  - up    8250 | ep 083/200 step 102/200 | loss=1.509150 | mem=200000 | lr=2.19e-03 | avg_step=0.062s\n","Episode 083 done | nodes=200000\n","  - up    8300 | ep 084/200 step 002/200 | loss=1.551035 | mem=200000 | lr=2.18e-03 | avg_step=0.061s\n","  - up    8350 | ep 084/200 step 102/200 | loss=1.496685 | mem=200000 | lr=2.17e-03 | avg_step=0.062s\n","Episode 084 done | nodes=200000\n","  - up    8400 | ep 085/200 step 002/200 | loss=1.502906 | mem=200000 | lr=2.16e-03 | avg_step=0.062s\n","  - up    8450 | ep 085/200 step 102/200 | loss=1.568784 | mem=200000 | lr=2.15e-03 | avg_step=0.063s\n","Episode 085 done | nodes=200000\n","  - up    8500 | ep 086/200 step 002/200 | loss=1.524314 | mem=200000 | lr=2.13e-03 | avg_step=0.062s\n","  - up    8550 | ep 086/200 step 102/200 | loss=1.507034 | mem=200000 | lr=2.12e-03 | avg_step=0.063s\n","Episode 086 done | nodes=200000\n","  - up    8600 | ep 087/200 step 002/200 | loss=1.536428 | mem=200000 | lr=2.11e-03 | avg_step=0.067s\n","  - up    8650 | ep 087/200 step 102/200 | loss=1.561635 | mem=200000 | lr=2.10e-03 | avg_step=0.067s\n","Episode 087 done | nodes=200000\n","  - up    8700 | ep 088/200 step 002/200 | loss=1.529504 | mem=200000 | lr=2.09e-03 | avg_step=0.069s\n","  - up    8750 | ep 088/200 step 102/200 | loss=1.469252 | mem=200000 | lr=2.07e-03 | avg_step=0.062s\n","Episode 088 done | nodes=200000\n","  - up    8800 | ep 089/200 step 002/200 | loss=1.533402 | mem=200000 | lr=2.06e-03 | avg_step=0.063s\n","  - up    8850 | ep 089/200 step 102/200 | loss=1.547415 | mem=200000 | lr=2.05e-03 | avg_step=0.064s\n","Episode 089 done | nodes=200000\n","  - up    8900 | ep 090/200 step 002/200 | loss=1.493530 | mem=200000 | lr=2.04e-03 | avg_step=0.061s\n","  - up    8950 | ep 090/200 step 102/200 | loss=1.505950 | mem=200000 | lr=2.03e-03 | avg_step=0.062s\n","Episode 090 done | nodes=200000\n","  - up    9000 | ep 091/200 step 002/200 | loss=1.523552 | mem=200000 | lr=2.01e-03 | avg_step=0.063s\n","  - up    9050 | ep 091/200 step 102/200 | loss=1.552167 | mem=200000 | lr=2.00e-03 | avg_step=0.068s\n","Episode 091 done | nodes=200000\n","  - up    9100 | ep 092/200 step 002/200 | loss=1.544901 | mem=200000 | lr=1.99e-03 | avg_step=0.062s\n","  - up    9150 | ep 092/200 step 102/200 | loss=1.509281 | mem=200000 | lr=1.98e-03 | avg_step=0.062s\n","Episode 092 done | nodes=200000\n","  - up    9200 | ep 093/200 step 002/200 | loss=1.513593 | mem=200000 | lr=1.96e-03 | avg_step=0.063s\n","  - up    9250 | ep 093/200 step 102/200 | loss=1.515210 | mem=200000 | lr=1.95e-03 | avg_step=0.069s\n","Episode 093 done | nodes=200000\n","  - up    9300 | ep 094/200 step 002/200 | loss=1.542821 | mem=200000 | lr=1.94e-03 | avg_step=0.062s\n","  - up    9350 | ep 094/200 step 102/200 | loss=1.548941 | mem=200000 | lr=1.93e-03 | avg_step=0.066s\n","Episode 094 done | nodes=200000\n","  - up    9400 | ep 095/200 step 002/200 | loss=1.488395 | mem=200000 | lr=1.91e-03 | avg_step=0.062s\n","  - up    9450 | ep 095/200 step 102/200 | loss=1.533938 | mem=200000 | lr=1.90e-03 | avg_step=0.062s\n","Episode 095 done | nodes=200000\n","  - up    9500 | ep 096/200 step 002/200 | loss=1.521880 | mem=200000 | lr=1.89e-03 | avg_step=0.062s\n","  - up    9550 | ep 096/200 step 102/200 | loss=1.580698 | mem=200000 | lr=1.88e-03 | avg_step=0.062s\n","Episode 096 done | nodes=200000\n","  - up    9600 | ep 097/200 step 002/200 | loss=1.578324 | mem=200000 | lr=1.86e-03 | avg_step=0.061s\n","  - up    9650 | ep 097/200 step 102/200 | loss=1.555654 | mem=200000 | lr=1.85e-03 | avg_step=0.064s\n","Episode 097 done | nodes=200000\n","  - up    9700 | ep 098/200 step 002/200 | loss=1.584557 | mem=200000 | lr=1.84e-03 | avg_step=0.063s\n","  - up    9750 | ep 098/200 step 102/200 | loss=1.546542 | mem=200000 | lr=1.83e-03 | avg_step=0.061s\n","Episode 098 done | nodes=200000\n","  - up    9800 | ep 099/200 step 002/200 | loss=1.478401 | mem=200000 | lr=1.81e-03 | avg_step=0.065s\n","  - up    9850 | ep 099/200 step 102/200 | loss=1.529103 | mem=200000 | lr=1.80e-03 | avg_step=0.061s\n","Episode 099 done | nodes=200000\n","  - up    9900 | ep 100/200 step 002/200 | loss=1.566970 | mem=200000 | lr=1.79e-03 | avg_step=0.067s\n","  - up    9950 | ep 100/200 step 102/200 | loss=1.486969 | mem=200000 | lr=1.77e-03 | avg_step=0.062s\n","Episode 100 done | nodes=200000\n","  - up   10000 | ep 101/200 step 002/200 | loss=1.553536 | mem=200000 | lr=1.76e-03 | avg_step=0.063s\n","  - up   10050 | ep 101/200 step 102/200 | loss=1.539648 | mem=200000 | lr=1.75e-03 | avg_step=0.063s\n","Episode 101 done | nodes=200000\n","  - up   10100 | ep 102/200 step 002/200 | loss=1.489624 | mem=200000 | lr=1.74e-03 | avg_step=0.065s\n","  - up   10150 | ep 102/200 step 102/200 | loss=1.534828 | mem=200000 | lr=1.72e-03 | avg_step=0.064s\n","Episode 102 done | nodes=200000\n","  - up   10200 | ep 103/200 step 002/200 | loss=1.563512 | mem=200000 | lr=1.71e-03 | avg_step=0.062s\n","  - up   10250 | ep 103/200 step 102/200 | loss=1.535553 | mem=200000 | lr=1.70e-03 | avg_step=0.062s\n","Episode 103 done | nodes=200000\n","  - up   10300 | ep 104/200 step 002/200 | loss=1.494355 | mem=200000 | lr=1.68e-03 | avg_step=0.063s\n","  - up   10350 | ep 104/200 step 102/200 | loss=1.553838 | mem=200000 | lr=1.67e-03 | avg_step=0.062s\n","Episode 104 done | nodes=200000\n","  - up   10400 | ep 105/200 step 002/200 | loss=1.540753 | mem=200000 | lr=1.66e-03 | avg_step=0.062s\n","  - up   10450 | ep 105/200 step 102/200 | loss=1.564416 | mem=200000 | lr=1.64e-03 | avg_step=0.069s\n","Episode 105 done | nodes=200000\n","  - up   10500 | ep 106/200 step 002/200 | loss=1.569767 | mem=200000 | lr=1.63e-03 | avg_step=0.061s\n","  - up   10550 | ep 106/200 step 102/200 | loss=1.558910 | mem=200000 | lr=1.62e-03 | avg_step=0.067s\n","Episode 106 done | nodes=200000\n","  - up   10600 | ep 107/200 step 002/200 | loss=1.476898 | mem=200000 | lr=1.61e-03 | avg_step=0.065s\n","  - up   10650 | ep 107/200 step 102/200 | loss=1.522729 | mem=200000 | lr=1.59e-03 | avg_step=0.062s\n","Episode 107 done | nodes=200000\n","  - up   10700 | ep 108/200 step 002/200 | loss=1.560929 | mem=200000 | lr=1.58e-03 | avg_step=0.063s\n","  - up   10750 | ep 108/200 step 102/200 | loss=1.577617 | mem=200000 | lr=1.57e-03 | avg_step=0.062s\n","Episode 108 done | nodes=200000\n","  - up   10800 | ep 109/200 step 002/200 | loss=1.496990 | mem=200000 | lr=1.55e-03 | avg_step=0.061s\n","  - up   10850 | ep 109/200 step 102/200 | loss=1.516469 | mem=200000 | lr=1.54e-03 | avg_step=0.062s\n","Episode 109 done | nodes=200000\n","  - up   10900 | ep 110/200 step 002/200 | loss=1.479237 | mem=200000 | lr=1.53e-03 | avg_step=0.062s\n","  - up   10950 | ep 110/200 step 102/200 | loss=1.547328 | mem=200000 | lr=1.51e-03 | avg_step=0.064s\n","Episode 110 done | nodes=200000\n","  - up   11000 | ep 111/200 step 002/200 | loss=1.515458 | mem=200000 | lr=1.50e-03 | avg_step=0.069s\n","  - up   11050 | ep 111/200 step 102/200 | loss=1.544271 | mem=200000 | lr=1.49e-03 | avg_step=0.062s\n","Episode 111 done | nodes=200000\n","  - up   11100 | ep 112/200 step 002/200 | loss=1.519820 | mem=200000 | lr=1.48e-03 | avg_step=0.068s\n","  - up   11150 | ep 112/200 step 102/200 | loss=1.526388 | mem=200000 | lr=1.46e-03 | avg_step=0.062s\n","Episode 112 done | nodes=200000\n","  - up   11200 | ep 113/200 step 002/200 | loss=1.542239 | mem=200000 | lr=1.45e-03 | avg_step=0.063s\n","  - up   11250 | ep 113/200 step 102/200 | loss=1.511724 | mem=200000 | lr=1.44e-03 | avg_step=0.063s\n","Episode 113 done | nodes=200000\n","  - up   11300 | ep 114/200 step 002/200 | loss=1.516011 | mem=200000 | lr=1.42e-03 | avg_step=0.065s\n","  - up   11350 | ep 114/200 step 102/200 | loss=1.540521 | mem=200000 | lr=1.41e-03 | avg_step=0.062s\n","Episode 114 done | nodes=200000\n","  - up   11400 | ep 115/200 step 002/200 | loss=1.529172 | mem=200000 | lr=1.40e-03 | avg_step=0.063s\n","  - up   11450 | ep 115/200 step 102/200 | loss=1.551726 | mem=200000 | lr=1.38e-03 | avg_step=0.062s\n","Episode 115 done | nodes=200000\n","  - up   11500 | ep 116/200 step 002/200 | loss=1.530663 | mem=200000 | lr=1.37e-03 | avg_step=0.063s\n","  - up   11550 | ep 116/200 step 102/200 | loss=1.611004 | mem=200000 | lr=1.36e-03 | avg_step=0.065s\n","Episode 116 done | nodes=200000\n","  - up   11600 | ep 117/200 step 002/200 | loss=1.535171 | mem=200000 | lr=1.34e-03 | avg_step=0.061s\n","  - up   11650 | ep 117/200 step 102/200 | loss=1.492258 | mem=200000 | lr=1.33e-03 | avg_step=0.069s\n","Episode 117 done | nodes=200000\n","  - up   11700 | ep 118/200 step 002/200 | loss=1.501824 | mem=200000 | lr=1.32e-03 | avg_step=0.062s\n","  - up   11750 | ep 118/200 step 102/200 | loss=1.538532 | mem=200000 | lr=1.31e-03 | avg_step=0.068s\n","Episode 118 done | nodes=200000\n","  - up   11800 | ep 119/200 step 002/200 | loss=1.538074 | mem=200000 | lr=1.29e-03 | avg_step=0.062s\n","  - up   11850 | ep 119/200 step 102/200 | loss=1.538309 | mem=200000 | lr=1.28e-03 | avg_step=0.062s\n","Episode 119 done | nodes=200000\n","  - up   11900 | ep 120/200 step 002/200 | loss=1.549816 | mem=200000 | lr=1.27e-03 | avg_step=0.063s\n","  - up   11950 | ep 120/200 step 102/200 | loss=1.554608 | mem=200000 | lr=1.25e-03 | avg_step=0.063s\n","Episode 120 done | nodes=200000\n","  - up   12000 | ep 121/200 step 002/200 | loss=1.522600 | mem=200000 | lr=1.24e-03 | avg_step=0.061s\n","  - up   12050 | ep 121/200 step 102/200 | loss=1.499618 | mem=200000 | lr=1.23e-03 | avg_step=0.062s\n","Episode 121 done | nodes=200000\n","  - up   12100 | ep 122/200 step 002/200 | loss=1.552500 | mem=200000 | lr=1.22e-03 | avg_step=0.062s\n","  - up   12150 | ep 122/200 step 102/200 | loss=1.508741 | mem=200000 | lr=1.20e-03 | avg_step=0.062s\n","Episode 122 done | nodes=200000\n","  - up   12200 | ep 123/200 step 002/200 | loss=1.570757 | mem=200000 | lr=1.19e-03 | avg_step=0.065s\n","  - up   12250 | ep 123/200 step 102/200 | loss=1.512615 | mem=200000 | lr=1.18e-03 | avg_step=0.062s\n","Episode 123 done | nodes=200000\n","  - up   12300 | ep 124/200 step 002/200 | loss=1.529613 | mem=200000 | lr=1.16e-03 | avg_step=0.069s\n","  - up   12350 | ep 124/200 step 102/200 | loss=1.525515 | mem=200000 | lr=1.15e-03 | avg_step=0.061s\n","Episode 124 done | nodes=200000\n","  - up   12400 | ep 125/200 step 002/200 | loss=1.533629 | mem=200000 | lr=1.14e-03 | avg_step=0.062s\n","  - up   12450 | ep 125/200 step 102/200 | loss=1.491771 | mem=200000 | lr=1.13e-03 | avg_step=0.061s\n","Episode 125 done | nodes=200000\n","  - up   12500 | ep 126/200 step 002/200 | loss=1.555991 | mem=200000 | lr=1.11e-03 | avg_step=0.063s\n","  - up   12550 | ep 126/200 step 102/200 | loss=1.618227 | mem=200000 | lr=1.10e-03 | avg_step=0.063s\n","Episode 126 done | nodes=200000\n","  - up   12600 | ep 127/200 step 002/200 | loss=1.537101 | mem=200000 | lr=1.09e-03 | avg_step=0.063s\n","  - up   12650 | ep 127/200 step 102/200 | loss=1.567786 | mem=200000 | lr=1.08e-03 | avg_step=0.062s\n","Episode 127 done | nodes=200000\n","  - up   12700 | ep 128/200 step 002/200 | loss=1.508377 | mem=200000 | lr=1.06e-03 | avg_step=0.062s\n","  - up   12750 | ep 128/200 step 102/200 | loss=1.492073 | mem=200000 | lr=1.05e-03 | avg_step=0.068s\n","Episode 128 done | nodes=200000\n","  - up   12800 | ep 129/200 step 002/200 | loss=1.515352 | mem=200000 | lr=1.04e-03 | avg_step=0.063s\n","  - up   12850 | ep 129/200 step 102/200 | loss=1.578619 | mem=200000 | lr=1.03e-03 | avg_step=0.068s\n","Episode 129 done | nodes=200000\n","  - up   12900 | ep 130/200 step 002/200 | loss=1.539157 | mem=200000 | lr=1.01e-03 | avg_step=0.062s\n","  - up   12950 | ep 130/200 step 102/200 | loss=1.551495 | mem=200000 | lr=1.00e-03 | avg_step=0.062s\n","Episode 130 done | nodes=200000\n","  - up   13000 | ep 131/200 step 002/200 | loss=1.502584 | mem=200000 | lr=9.89e-04 | avg_step=0.061s\n","  - up   13050 | ep 131/200 step 102/200 | loss=1.530575 | mem=200000 | lr=9.76e-04 | avg_step=0.062s\n","Episode 131 done | nodes=200000\n","  - up   13100 | ep 132/200 step 002/200 | loss=1.610977 | mem=200000 | lr=9.64e-04 | avg_step=0.061s\n","  - up   13150 | ep 132/200 step 102/200 | loss=1.503997 | mem=200000 | lr=9.52e-04 | avg_step=0.063s\n","Episode 132 done | nodes=200000\n","  - up   13200 | ep 133/200 step 002/200 | loss=1.525429 | mem=200000 | lr=9.40e-04 | avg_step=0.061s\n","  - up   13250 | ep 133/200 step 102/200 | loss=1.525802 | mem=200000 | lr=9.28e-04 | avg_step=0.063s\n","Episode 133 done | nodes=200000\n","  - up   13300 | ep 134/200 step 002/200 | loss=1.552809 | mem=200000 | lr=9.16e-04 | avg_step=0.066s\n","  - up   13350 | ep 134/200 step 102/200 | loss=1.509460 | mem=200000 | lr=9.04e-04 | avg_step=0.063s\n","Episode 134 done | nodes=200000\n","  - up   13400 | ep 135/200 step 002/200 | loss=1.563703 | mem=200000 | lr=8.92e-04 | avg_step=0.070s\n","  - up   13450 | ep 135/200 step 102/200 | loss=1.545468 | mem=200000 | lr=8.80e-04 | avg_step=0.062s\n","Episode 135 done | nodes=200000\n","  - up   13500 | ep 136/200 step 002/200 | loss=1.528492 | mem=200000 | lr=8.68e-04 | avg_step=0.063s\n","  - up   13550 | ep 136/200 step 102/200 | loss=1.503082 | mem=200000 | lr=8.56e-04 | avg_step=0.063s\n","Episode 136 done | nodes=200000\n","  - up   13600 | ep 137/200 step 002/200 | loss=1.539155 | mem=200000 | lr=8.44e-04 | avg_step=0.062s\n","  - up   13650 | ep 137/200 step 102/200 | loss=1.452420 | mem=200000 | lr=8.33e-04 | avg_step=0.062s\n","Episode 137 done | nodes=200000\n","  - up   13700 | ep 138/200 step 002/200 | loss=1.514637 | mem=200000 | lr=8.21e-04 | avg_step=0.063s\n","  - up   13750 | ep 138/200 step 102/200 | loss=1.489304 | mem=200000 | lr=8.09e-04 | avg_step=0.063s\n","Episode 138 done | nodes=200000\n","  - up   13800 | ep 139/200 step 002/200 | loss=1.515165 | mem=200000 | lr=7.98e-04 | avg_step=0.063s\n","  - up   13850 | ep 139/200 step 102/200 | loss=1.541928 | mem=200000 | lr=7.86e-04 | avg_step=0.067s\n","Episode 139 done | nodes=200000\n","  - up   13900 | ep 140/200 step 002/200 | loss=1.544105 | mem=200000 | lr=7.75e-04 | avg_step=0.062s\n","  - up   13950 | ep 140/200 step 102/200 | loss=1.536999 | mem=200000 | lr=7.63e-04 | avg_step=0.068s\n","Episode 140 done | nodes=200000\n","  - up   14000 | ep 141/200 step 002/200 | loss=1.575408 | mem=200000 | lr=7.52e-04 | avg_step=0.062s\n","  - up   14050 | ep 141/200 step 102/200 | loss=1.597229 | mem=200000 | lr=7.41e-04 | avg_step=0.063s\n","Episode 141 done | nodes=200000\n","  - up   14100 | ep 142/200 step 002/200 | loss=1.522115 | mem=200000 | lr=7.29e-04 | avg_step=0.063s\n","  - up   14150 | ep 142/200 step 102/200 | loss=1.525679 | mem=200000 | lr=7.18e-04 | avg_step=0.062s\n","Episode 142 done | nodes=200000\n","  - up   14200 | ep 143/200 step 002/200 | loss=1.585686 | mem=200000 | lr=7.07e-04 | avg_step=0.062s\n","  - up   14250 | ep 143/200 step 102/200 | loss=1.544739 | mem=200000 | lr=6.96e-04 | avg_step=0.062s\n","Episode 143 done | nodes=200000\n","  - up   14300 | ep 144/200 step 002/200 | loss=1.525783 | mem=200000 | lr=6.85e-04 | avg_step=0.064s\n","  - up   14350 | ep 144/200 step 102/200 | loss=1.522126 | mem=200000 | lr=6.74e-04 | avg_step=0.063s\n","Episode 144 done | nodes=200000\n","  - up   14400 | ep 145/200 step 002/200 | loss=1.534729 | mem=200000 | lr=6.63e-04 | avg_step=0.067s\n","  - up   14450 | ep 145/200 step 102/200 | loss=1.551140 | mem=200000 | lr=6.53e-04 | avg_step=0.065s\n","Episode 145 done | nodes=200000\n","  - up   14500 | ep 146/200 step 002/200 | loss=1.510033 | mem=200000 | lr=6.42e-04 | avg_step=0.069s\n","  - up   14550 | ep 146/200 step 102/200 | loss=1.546670 | mem=200000 | lr=6.31e-04 | avg_step=0.063s\n","Episode 146 done | nodes=200000\n","  - up   14600 | ep 147/200 step 002/200 | loss=1.559313 | mem=200000 | lr=6.20e-04 | avg_step=0.062s\n","  - up   14650 | ep 147/200 step 102/200 | loss=1.536656 | mem=200000 | lr=6.10e-04 | avg_step=0.065s\n","Episode 147 done | nodes=200000\n","  - up   14700 | ep 148/200 step 002/200 | loss=1.576331 | mem=200000 | lr=5.99e-04 | avg_step=0.062s\n","  - up   14750 | ep 148/200 step 102/200 | loss=1.526925 | mem=200000 | lr=5.89e-04 | avg_step=0.063s\n","Episode 148 done | nodes=200000\n","  - up   14800 | ep 149/200 step 002/200 | loss=1.501466 | mem=200000 | lr=5.79e-04 | avg_step=0.062s\n","  - up   14850 | ep 149/200 step 102/200 | loss=1.566032 | mem=200000 | lr=5.68e-04 | avg_step=0.063s\n","Episode 149 done | nodes=200000\n","  - up   14900 | ep 150/200 step 002/200 | loss=1.547057 | mem=200000 | lr=5.58e-04 | avg_step=0.062s\n","  - up   14950 | ep 150/200 step 102/200 | loss=1.518374 | mem=200000 | lr=5.48e-04 | avg_step=0.070s\n","Episode 150 done | nodes=200000\n","  - up   15000 | ep 151/200 step 002/200 | loss=1.562990 | mem=200000 | lr=5.38e-04 | avg_step=0.063s\n","  - up   15050 | ep 151/200 step 102/200 | loss=1.559220 | mem=200000 | lr=5.28e-04 | avg_step=0.064s\n","Episode 151 done | nodes=200000\n","  - up   15100 | ep 152/200 step 002/200 | loss=1.554664 | mem=200000 | lr=5.18e-04 | avg_step=0.062s\n","  - up   15150 | ep 152/200 step 102/200 | loss=1.474423 | mem=200000 | lr=5.08e-04 | avg_step=0.062s\n","Episode 152 done | nodes=200000\n","  - up   15200 | ep 153/200 step 002/200 | loss=1.508272 | mem=200000 | lr=4.99e-04 | avg_step=0.062s\n","  - up   15250 | ep 153/200 step 102/200 | loss=1.538893 | mem=200000 | lr=4.89e-04 | avg_step=0.062s\n","Episode 153 done | nodes=200000\n","  - up   15300 | ep 154/200 step 002/200 | loss=1.525628 | mem=200000 | lr=4.79e-04 | avg_step=0.063s\n","  - up   15350 | ep 154/200 step 102/200 | loss=1.547778 | mem=200000 | lr=4.70e-04 | avg_step=0.064s\n","Episode 154 done | nodes=200000\n","  - up   15400 | ep 155/200 step 002/200 | loss=1.510639 | mem=200000 | lr=4.60e-04 | avg_step=0.064s\n","  - up   15450 | ep 155/200 step 102/200 | loss=1.502542 | mem=200000 | lr=4.51e-04 | avg_step=0.062s\n","Episode 155 done | nodes=200000\n","  - up   15500 | ep 156/200 step 002/200 | loss=1.561671 | mem=200000 | lr=4.42e-04 | avg_step=0.068s\n","  - up   15550 | ep 156/200 step 102/200 | loss=1.507819 | mem=200000 | lr=4.33e-04 | avg_step=0.062s\n","Episode 156 done | nodes=200000\n","  - up   15600 | ep 157/200 step 002/200 | loss=1.571002 | mem=200000 | lr=4.23e-04 | avg_step=0.065s\n","  - up   15650 | ep 157/200 step 102/200 | loss=1.527138 | mem=200000 | lr=4.14e-04 | avg_step=0.065s\n","Episode 157 done | nodes=200000\n","  - up   15700 | ep 158/200 step 002/200 | loss=1.513382 | mem=200000 | lr=4.05e-04 | avg_step=0.062s\n","  - up   15750 | ep 158/200 step 102/200 | loss=1.568905 | mem=200000 | lr=3.97e-04 | avg_step=0.062s\n","Episode 158 done | nodes=200000\n","  - up   15800 | ep 159/200 step 002/200 | loss=1.517029 | mem=200000 | lr=3.88e-04 | avg_step=0.062s\n","  - up   15850 | ep 159/200 step 102/200 | loss=1.535073 | mem=200000 | lr=3.79e-04 | avg_step=0.062s\n","Episode 159 done | nodes=200000\n","  - up   15900 | ep 160/200 step 002/200 | loss=1.547394 | mem=200000 | lr=3.70e-04 | avg_step=0.062s\n","  - up   15950 | ep 160/200 step 102/200 | loss=1.518074 | mem=200000 | lr=3.62e-04 | avg_step=0.064s\n","Episode 160 done | nodes=200000\n","  - up   16000 | ep 161/200 step 002/200 | loss=1.535576 | mem=200000 | lr=3.53e-04 | avg_step=0.064s\n","  - up   16050 | ep 161/200 step 102/200 | loss=1.526808 | mem=200000 | lr=3.45e-04 | avg_step=0.070s\n","Episode 161 done | nodes=200000\n","  - up   16100 | ep 162/200 step 002/200 | loss=1.509148 | mem=200000 | lr=3.37e-04 | avg_step=0.063s\n","  - up   16150 | ep 162/200 step 102/200 | loss=1.509686 | mem=200000 | lr=3.29e-04 | avg_step=0.062s\n","Episode 162 done | nodes=200000\n","  - up   16200 | ep 163/200 step 002/200 | loss=1.566299 | mem=200000 | lr=3.21e-04 | avg_step=0.063s\n","  - up   16250 | ep 163/200 step 102/200 | loss=1.543813 | mem=200000 | lr=3.13e-04 | avg_step=0.064s\n","Episode 163 done | nodes=200000\n","  - up   16300 | ep 164/200 step 002/200 | loss=1.523886 | mem=200000 | lr=3.05e-04 | avg_step=0.066s\n","  - up   16350 | ep 164/200 step 102/200 | loss=1.530539 | mem=200000 | lr=2.97e-04 | avg_step=0.064s\n","Episode 164 done | nodes=200000\n","  - up   16400 | ep 165/200 step 002/200 | loss=1.518865 | mem=200000 | lr=2.89e-04 | avg_step=0.064s\n","  - up   16450 | ep 165/200 step 102/200 | loss=1.545969 | mem=200000 | lr=2.81e-04 | avg_step=0.064s\n","Episode 165 done | nodes=200000\n","  - up   16500 | ep 166/200 step 002/200 | loss=1.499368 | mem=200000 | lr=2.74e-04 | avg_step=0.071s\n","  - up   16550 | ep 166/200 step 102/200 | loss=1.532050 | mem=200000 | lr=2.66e-04 | avg_step=0.064s\n","Episode 166 done | nodes=200000\n","  - up   16600 | ep 167/200 step 002/200 | loss=1.561176 | mem=200000 | lr=2.59e-04 | avg_step=0.064s\n","  - up   16650 | ep 167/200 step 102/200 | loss=1.553241 | mem=200000 | lr=2.52e-04 | avg_step=0.064s\n","Episode 167 done | nodes=200000\n","  - up   16700 | ep 168/200 step 002/200 | loss=1.578513 | mem=200000 | lr=2.45e-04 | avg_step=0.065s\n","  - up   16750 | ep 168/200 step 102/200 | loss=1.537198 | mem=200000 | lr=2.38e-04 | avg_step=0.064s\n","Episode 168 done | nodes=200000\n","  - up   16800 | ep 169/200 step 002/200 | loss=1.531851 | mem=200000 | lr=2.31e-04 | avg_step=0.064s\n","  - up   16850 | ep 169/200 step 102/200 | loss=1.562270 | mem=200000 | lr=2.24e-04 | avg_step=0.068s\n","Episode 169 done | nodes=200000\n","  - up   16900 | ep 170/200 step 002/200 | loss=1.635524 | mem=200000 | lr=2.17e-04 | avg_step=0.064s\n","  - up   16950 | ep 170/200 step 102/200 | loss=1.479536 | mem=200000 | lr=2.10e-04 | avg_step=0.070s\n","Episode 170 done | nodes=200000\n","  - up   17000 | ep 171/200 step 002/200 | loss=1.566906 | mem=200000 | lr=2.04e-04 | avg_step=0.066s\n","  - up   17050 | ep 171/200 step 102/200 | loss=1.560594 | mem=200000 | lr=1.97e-04 | avg_step=0.065s\n","Episode 171 done | nodes=200000\n","  - up   17100 | ep 172/200 step 002/200 | loss=1.569008 | mem=200000 | lr=1.91e-04 | avg_step=0.062s\n","  - up   17150 | ep 172/200 step 102/200 | loss=1.568490 | mem=200000 | lr=1.84e-04 | avg_step=0.063s\n","Episode 172 done | nodes=200000\n","  - up   17200 | ep 173/200 step 002/200 | loss=1.545039 | mem=200000 | lr=1.78e-04 | avg_step=0.062s\n","  - up   17250 | ep 173/200 step 102/200 | loss=1.517238 | mem=200000 | lr=1.72e-04 | avg_step=0.065s\n","Episode 173 done | nodes=200000\n","  - up   17300 | ep 174/200 step 002/200 | loss=1.547570 | mem=200000 | lr=1.66e-04 | avg_step=0.065s\n","  - up   17350 | ep 174/200 step 102/200 | loss=1.573895 | mem=200000 | lr=1.60e-04 | avg_step=0.063s\n","Episode 174 done | nodes=200000\n","  - up   17400 | ep 175/200 step 002/200 | loss=1.534361 | mem=200000 | lr=1.55e-04 | avg_step=0.069s\n","  - up   17450 | ep 175/200 step 102/200 | loss=1.512771 | mem=200000 | lr=1.49e-04 | avg_step=0.064s\n","Episode 175 done | nodes=200000\n","  - up   17500 | ep 176/200 step 002/200 | loss=1.467125 | mem=200000 | lr=1.43e-04 | avg_step=0.063s\n","  - up   17550 | ep 176/200 step 102/200 | loss=1.476644 | mem=200000 | lr=1.38e-04 | avg_step=0.063s\n","Episode 176 done | nodes=200000\n","  - up   17600 | ep 177/200 step 002/200 | loss=1.552081 | mem=200000 | lr=1.32e-04 | avg_step=0.064s\n","  - up   17650 | ep 177/200 step 102/200 | loss=1.614251 | mem=200000 | lr=1.27e-04 | avg_step=0.063s\n","Episode 177 done | nodes=200000\n","  - up   17700 | ep 178/200 step 002/200 | loss=1.561192 | mem=200000 | lr=1.22e-04 | avg_step=0.064s\n","  - up   17750 | ep 178/200 step 102/200 | loss=1.507058 | mem=200000 | lr=1.17e-04 | avg_step=0.062s\n","Episode 178 done | nodes=200000\n","  - up   17800 | ep 179/200 step 002/200 | loss=1.528344 | mem=200000 | lr=1.12e-04 | avg_step=0.062s\n","  - up   17850 | ep 179/200 step 102/200 | loss=1.538545 | mem=200000 | lr=1.07e-04 | avg_step=0.070s\n","Episode 179 done | nodes=200000\n","  - up   17900 | ep 180/200 step 002/200 | loss=1.578461 | mem=200000 | lr=1.02e-04 | avg_step=0.062s\n","  - up   17950 | ep 180/200 step 102/200 | loss=1.534247 | mem=200000 | lr=9.78e-05 | avg_step=0.066s\n","Episode 180 done | nodes=200000\n","  - up   18000 | ep 181/200 step 002/200 | loss=1.553907 | mem=200000 | lr=9.33e-05 | avg_step=0.064s\n","  - up   18050 | ep 181/200 step 102/200 | loss=1.543384 | mem=200000 | lr=8.89e-05 | avg_step=0.062s\n","Episode 181 done | nodes=200000\n","  - up   18100 | ep 182/200 step 002/200 | loss=1.521670 | mem=200000 | lr=8.46e-05 | avg_step=0.063s\n","  - up   18150 | ep 182/200 step 102/200 | loss=1.546308 | mem=200000 | lr=8.04e-05 | avg_step=0.063s\n","Episode 182 done | nodes=200000\n","  - up   18200 | ep 183/200 step 002/200 | loss=1.482783 | mem=200000 | lr=7.63e-05 | avg_step=0.063s\n","  - up   18250 | ep 183/200 step 102/200 | loss=1.531370 | mem=200000 | lr=7.23e-05 | avg_step=0.062s\n","Episode 183 done | nodes=200000\n","  - up   18300 | ep 184/200 step 002/200 | loss=1.529290 | mem=200000 | lr=6.84e-05 | avg_step=0.062s\n","  - up   18350 | ep 184/200 step 102/200 | loss=1.534202 | mem=200000 | lr=6.46e-05 | avg_step=0.063s\n","Episode 184 done | nodes=200000\n","  - up   18400 | ep 185/200 step 002/200 | loss=1.446536 | mem=200000 | lr=6.10e-05 | avg_step=0.069s\n","  - up   18450 | ep 185/200 step 102/200 | loss=1.541032 | mem=200000 | lr=5.74e-05 | avg_step=0.065s\n","Episode 185 done | nodes=200000\n","  - up   18500 | ep 186/200 step 002/200 | loss=1.561663 | mem=200000 | lr=5.40e-05 | avg_step=0.064s\n","  - up   18550 | ep 186/200 step 102/200 | loss=1.479203 | mem=200000 | lr=5.07e-05 | avg_step=0.063s\n","Episode 186 done | nodes=200000\n","  - up   18600 | ep 187/200 step 002/200 | loss=1.556980 | mem=200000 | lr=4.74e-05 | avg_step=0.063s\n","  - up   18650 | ep 187/200 step 102/200 | loss=1.599765 | mem=200000 | lr=4.43e-05 | avg_step=0.063s\n","Episode 187 done | nodes=200000\n","  - up   18700 | ep 188/200 step 002/200 | loss=1.493351 | mem=200000 | lr=4.13e-05 | avg_step=0.063s\n","  - up   18750 | ep 188/200 step 102/200 | loss=1.525728 | mem=200000 | lr=3.85e-05 | avg_step=0.062s\n","Episode 188 done | nodes=200000\n","  - up   18800 | ep 189/200 step 002/200 | loss=1.555800 | mem=200000 | lr=3.57e-05 | avg_step=0.066s\n","  - up   18850 | ep 189/200 step 102/200 | loss=1.574851 | mem=200000 | lr=3.30e-05 | avg_step=0.064s\n","Episode 189 done | nodes=200000\n","  - up   18900 | ep 190/200 step 002/200 | loss=1.576846 | mem=200000 | lr=3.05e-05 | avg_step=0.062s\n","  - up   18950 | ep 190/200 step 102/200 | loss=1.542562 | mem=200000 | lr=2.80e-05 | avg_step=0.067s\n","Episode 190 done | nodes=200000\n","  - up   19000 | ep 191/200 step 002/200 | loss=1.499249 | mem=200000 | lr=2.57e-05 | avg_step=0.064s\n","  - up   19050 | ep 191/200 step 102/200 | loss=1.530210 | mem=200000 | lr=2.35e-05 | avg_step=0.064s\n","Episode 191 done | nodes=200000\n","  - up   19100 | ep 192/200 step 002/200 | loss=1.501877 | mem=200000 | lr=2.14e-05 | avg_step=0.062s\n","  - up   19150 | ep 192/200 step 102/200 | loss=1.554476 | mem=200000 | lr=1.94e-05 | avg_step=0.062s\n","Episode 192 done | nodes=200000\n","  - up   19200 | ep 193/200 step 002/200 | loss=1.531277 | mem=200000 | lr=1.75e-05 | avg_step=0.063s\n","  - up   19250 | ep 193/200 step 102/200 | loss=1.492155 | mem=200000 | lr=1.58e-05 | avg_step=0.062s\n","Episode 193 done | nodes=200000\n","  - up   19300 | ep 194/200 step 002/200 | loss=1.537765 | mem=200000 | lr=1.41e-05 | avg_step=0.063s\n","  - up   19350 | ep 194/200 step 102/200 | loss=1.582860 | mem=200000 | lr=1.26e-05 | avg_step=0.063s\n","Episode 194 done | nodes=200000\n","  - up   19400 | ep 195/200 step 002/200 | loss=1.624993 | mem=200000 | lr=1.12e-05 | avg_step=0.063s\n","  - up   19450 | ep 195/200 step 102/200 | loss=1.548254 | mem=200000 | lr=9.87e-06 | avg_step=0.063s\n","Episode 195 done | nodes=200000\n","  - up   19500 | ep 196/200 step 002/200 | loss=1.509034 | mem=200000 | lr=8.68e-06 | avg_step=0.069s\n","  - up   19550 | ep 196/200 step 102/200 | loss=1.560412 | mem=200000 | lr=7.60e-06 | avg_step=0.062s\n","Episode 196 done | nodes=200000\n","  - up   19600 | ep 197/200 step 002/200 | loss=1.513793 | mem=200000 | lr=6.63e-06 | avg_step=0.064s\n","  - up   19650 | ep 197/200 step 102/200 | loss=1.525762 | mem=200000 | lr=5.78e-06 | avg_step=0.064s\n","Episode 197 done | nodes=200000\n","  - up   19700 | ep 198/200 step 002/200 | loss=1.534815 | mem=200000 | lr=5.04e-06 | avg_step=0.064s\n","  - up   19750 | ep 198/200 step 102/200 | loss=1.534456 | mem=200000 | lr=4.41e-06 | avg_step=0.062s\n","Episode 198 done | nodes=200000\n","  - up   19800 | ep 199/200 step 002/200 | loss=1.540834 | mem=200000 | lr=3.90e-06 | avg_step=0.063s\n","  - up   19850 | ep 199/200 step 102/200 | loss=1.522765 | mem=200000 | lr=3.51e-06 | avg_step=0.064s\n","Episode 199 done | nodes=200000\n","  - up   19900 | ep 200/200 step 002/200 | loss=1.486034 | mem=200000 | lr=3.22e-06 | avg_step=0.062s\n","  - up   19950 | ep 200/200 step 102/200 | loss=1.524802 | mem=200000 | lr=3.05e-06 | avg_step=0.063s\n","Episode 200 done | nodes=200000\n","Done. Elapsed: 42.3 min | preset=LONG_RUN | steps=19999\n","Artifacts: /content/graph_world_runs/glrm_longrun\n"]}],"source":["# @title\n","#  GLRM: single-cell Colab (LONG-RUN, CUDA Graphs-safe)\n","# - Fix for torch.compile+CUDAGraphs overwrite: cudagraph_mark_step_begin() per iteration\n","# - Optional emb.clone() outside compiled region to fully decouple buffers\n","# - Long-run presets + wallclock cap, grad accumulation, OneCycleLR\n","# - AMP (bf16 preferred, fp16+GradScaler fallback), torch.compile(\"reduce-overhead\")\n","# - AdamW(foreach) or bitsandbytes AdamW8bit, FAISS IVF-PQ (optional), ring-buffer memory\n","\n","# ========================== CONFIG ==========================\n","PRESET = \"LONG_RUN\"    # \"FAST_24CU\",\"BALANCED\",\"MAX_60CU\",\"LONG_RUN\",\"MARATHON\"\n","SEED = 123\n","MOUNT_DRIVE = False\n","RUN_NAME = \"glrm_longrun\"\n","SAVE_ROOT = \"/content/drive/MyDrive\" if MOUNT_DRIVE else \"/content\"\n","\n","# Train-longer knobs\n","MAX_WALLCLOCK_MIN = 0      # 0 disables wallclock stop\n","ACCUM_STEPS = 2            # effective batch = BATCH * ACCUM_STEPS\n","SAVE_EVERY_STEPS = 500     # step checkpoint interval (optimizer steps)\n","LOG_EVERY_STEPS = 50       # log interval (optimizer steps)\n","\n","# Model/optim\n","D_IN = 128; D_HID = 256; D_OUT = 128\n","LR = 3e-3; WEIGHT_DECAY = 1e-2\n","USE_BNB_8BIT = True\n","USE_COMPILE = True\n","COMPILE_MODE = \"reduce-overhead\"\n","\n","# Presets\n","PRESETS = {\n","    \"FAST_24CU\": dict(EPISODES=10,   STEPS_PER_EP=40,  BATCH=256, K=8),\n","    \"BALANCED\":  dict(EPISODES=20,   STEPS_PER_EP=60,  BATCH=384, K=16),\n","    \"MAX_60CU\":  dict(EPISODES=30,   STEPS_PER_EP=80,  BATCH=512, K=32),\n","    \"LONG_RUN\":  dict(EPISODES=200,  STEPS_PER_EP=200, BATCH=512, K=16),\n","    \"MARATHON\":  dict(EPISODES=1000, STEPS_PER_EP=400, BATCH=512, K=32),\n","}\n","S = PRESETS[PRESET]\n","EPISODES, STEPS_PER_EP, BATCH, K_NEI = S[\"EPISODES\"], S[\"STEPS_PER_EP\"], S[\"BATCH\"], S[\"K\"]\n","\n","# Memory / FAISS\n","AUTO_INSTALL_FAISS = False\n","REINDEX_EVERY_STEPS = 1000\n","FAISS_USE_IVFPQ = True\n","FAISS_NLIST = 2048; FAISS_M = 16; FAISS_NBITS = 8; FAISS_NPROBE = 64\n","TOPK = K_NEI\n","MEM_CAP = 200_000  # ring buffer cap\n","\n","# Loss\n","LAMBDA_MSE = 1.0\n","LAMBDA_CONTRAST = 0.25\n","TEMP = 0.07\n","\n","# ============================================================\n","import os, sys, math, time, random, subprocess\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# -------- Helpers ----------\n","def _lazy_import(name):\n","    try:\n","        return __import__(name)\n","    except Exception:\n","        return None\n","\n","def _maybe_install_faiss():\n","    try:\n","        print(\"Attempting FAISS install (gpu->cpu fallback)...\")\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-gpu-cu12\"])\n","    except Exception:\n","        try:\n","            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-cpu\"])\n","        except Exception as e:\n","            print(f\"FAISS install failed: {e}\")\n","\n","faiss = _lazy_import(\"faiss\") or _lazy_import(\"faiss_gpu\") or _lazy_import(\"faiss_cpu\")\n","bnb = _lazy_import(\"bitsandbytes\")\n","\n","# -------- Colab / Drive ----------\n","IN_COLAB = \"google.colab\" in sys.modules\n","if MOUNT_DRIVE and IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","\n","# -------- Repro & device ----------\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cuda_name = torch.cuda.get_device_name(0) if device.type == \"cuda\" else \"CPU\"\n","bf16_ok = (device.type == \"cuda\") and torch.cuda.is_bf16_supported()\n","amp_dtype = torch.bfloat16 if bf16_ok else torch.float16\n","amp_device = \"cuda\" if device.type == \"cuda\" else \"cpu\"\n","use_amp = (device.type == \"cuda\")\n","\n","# Enable TF32\n","try:\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    torch.set_float32_matmul_precision(\"high\")\n","except Exception:\n","    pass\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.benchmark = True\n","\n","# Optional FAISS auto-install\n","if AUTO_INSTALL_FAISS and (faiss is None):\n","    _maybe_install_faiss()\n","    faiss = _lazy_import(\"faiss\") or _lazy_import(\"faiss_gpu\") or _lazy_import(\"faiss_cpu\")\n","\n","# -------- Model ----------\n","class Encoder(nn.Module):\n","    def __init__(self, d_in=D_IN, d_hid=D_HID, d_out=D_OUT):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d_in, d_hid),\n","            nn.GELU(),\n","            nn.Linear(d_hid, d_out),\n","        )\n","        with torch.no_grad():\n","            for m in self.modules():\n","                if isinstance(m, nn.Linear):\n","                    nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n","                    nn.init.zeros_(m.bias)\n","    def forward(self, x):\n","        return F.normalize(self.net(x), dim=-1)\n","\n","# -------- Synthetic stream (GMM) ----------\n","class GMMStream:\n","    def __init__(self, d=D_IN, k=32, std=0.3):\n","        self.d, self.k, self.std = d, k, std\n","        rng = np.random.default_rng(SEED)\n","        self.means = rng.normal(size=(k, d)).astype(np.float32)\n","        self.assign = rng\n","    def sample(self, n):\n","        idx = self.assign.integers(0, self.k, size=(n,))\n","        base = self.means[idx]\n","        noise = self.assign.normal(scale=self.std, size=(n, self.d)).astype(np.float32)\n","        x = base + noise\n","        return torch.from_numpy(x), torch.from_numpy(idx.astype(np.int64))\n","\n","gmm = GMMStream(d=D_IN, k=32, std=0.3)\n","def get_batch(bs=BATCH):\n","    x, y = gmm.sample(bs)\n","    if device.type == \"cuda\":\n","        x = x.pin_memory(); y = y.pin_memory()\n","    return x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n","\n","# -------- Memory bank with optional FAISS index ----------\n","class MemoryBank:\n","    def __init__(self, d, use_faiss=True, ivfpq=True):\n","        self.d = d\n","        self.use_faiss = bool(faiss) and use_faiss\n","        self.ivfpq = ivfpq\n","        self._vecs = []; self._labels = []\n","        self._faiss = None; self._trained = False; self._added = 0\n","    @property\n","    def size(self): return sum(v.shape[0] for v in self._vecs)\n","    def _enforce_cap(self):\n","        if MEM_CAP and self.size > MEM_CAP:\n","            X = torch.cat(self._vecs, 0)\n","            y = torch.cat(self._labels, 0) if self._labels else None\n","            X = X[-MEM_CAP:].contiguous()\n","            self._vecs = [X]\n","            self._labels = [y[-MEM_CAP:].contiguous()] if y is not None else []\n","            self._faiss = None; self._trained = False; self._added = X.shape[0]\n","    def add(self, vecs, labels=None):\n","        vc = vecs.detach().to(\"cpu\", dtype=torch.float32).contiguous()\n","        self._vecs.append(vc)\n","        if labels is not None: self._labels.append(labels.detach().to(\"cpu\"))\n","        self._added += vc.shape[0]; self._enforce_cap()\n","    def _cat(self):\n","        if not self._vecs: return None, None\n","        X = torch.cat(self._vecs, 0)\n","        y = torch.cat(self._labels, 0) if self._labels else None\n","        return X, y\n","    def maybe_reindex(self, force=False):\n","        if not self.use_faiss: return\n","        if not force and self._added < REINDEX_EVERY_STEPS: return\n","        self._added = 0\n","        X, _ = self._cat()\n","        if X is None or X.shape[0] < max(FAISS_NLIST, 1024):\n","            self._faiss = None; self._trained = False; return\n","        X_np = X.numpy(); d = X_np.shape[1]\n","        if self.ivfpq:\n","            quantizer = faiss.IndexFlatL2(d)\n","            index = faiss.IndexIVFPQ(quantizer, d, FAISS_NLIST, FAISS_M, FAISS_NBITS)\n","        else:\n","            quantizer = faiss.IndexFlatL2(d)\n","            index = faiss.IndexIVFFlat(quantizer, d, FAISS_NLIST, faiss.METRIC_L2)\n","        if hasattr(faiss, \"StandardGpuResources\") and torch.cuda.is_available():\n","            try:\n","                res = faiss.StandardGpuResources()\n","                index = faiss.index_cpu_to_gpu(res, 0, index)\n","            except Exception:\n","                pass\n","        index.train(X_np)\n","        if hasattr(index, \"nprobe\"):\n","            index.nprobe = max(1, FAISS_NPROBE)\n","        elif hasattr(index, \"setNumProbes\"):\n","            index.setNumProbes(max(1, FAISS_NPROBE))\n","        index.add(X_np)\n","        self._faiss = index; self._trained = True\n","    def search(self, q_emb, topk):\n","        if self.size == 0:\n","            return (torch.empty(q_emb.shape[0], 0, dtype=torch.int64, device=q_emb.device),\n","                    torch.empty(q_emb.shape[0], 0, dtype=torch.float32, device=q_emb.device))\n","        X, _ = self._cat(); q = q_emb.detach().to(\"cpu\", dtype=torch.float32).contiguous().numpy()\n","        if self.use_faiss and self._faiss is not None and self._trained:\n","            D, I = self._faiss.search(q, topk)\n","            idx = torch.from_numpy(I.astype(np.int64)).to(q_emb.device)\n","            dist = torch.from_numpy(D.astype(np.float32)).to(q_emb.device)\n","            return idx, dist\n","        xb = X.to(q_emb.device)\n","        qn = F.normalize(q_emb, dim=-1); xn = F.normalize(xb, dim=-1)\n","        sim = torch.matmul(qn, xn.T)\n","        dist, idx = torch.topk(sim, k=min(topk, xb.shape[0]), dim=-1, largest=True)\n","        dist = 1.0 - dist.clamp(-1, 1)\n","        return idx, dist\n","\n","# -------- Losses ----------\n","def neighbor_mse(emb, mem_vecs, idx, weights=None):\n","    if idx.numel() == 0: return emb.new_tensor(0.0)\n","    knn = mem_vecs[idx]\n","    if weights is not None:\n","        w = (weights + 1e-8); w = w / w.sum(dim=1, keepdim=True)\n","        centroid = (knn * w.unsqueeze(-1)).sum(dim=1)\n","    else:\n","        centroid = knn.mean(dim=1)\n","    return F.mse_loss(emb, centroid)\n","\n","def info_nce(emb, mem_vecs, idx, temp=TEMP):\n","    if idx.numel() == 0: return emb.new_tensor(0.0)\n","    B, K = idx.shape\n","    knn = mem_vecs[idx]\n","    q = F.normalize(emb, dim=-1).unsqueeze(1)\n","    k_all = F.normalize(knn, dim=-1)\n","    pos = (q * k_all[:, :1]).sum(-1) / temp\n","    neg = (q * k_all[:, 1:]).sum(-1) / temp\n","    logits = torch.cat([pos, neg], dim=1)\n","    labels = torch.zeros(B, dtype=torch.long, device=emb.device)\n","    return F.cross_entropy(logits, labels)\n","\n","# -------- Training ----------\n","class Runner:\n","    def __init__(self):\n","        self.model = Encoder(D_IN, D_HID, D_OUT).to(device)\n","        self._compiled = False\n","        if USE_COMPILE and hasattr(torch, \"compile\"):\n","            try:\n","                self.model = torch.compile(self.model, mode=COMPILE_MODE, fullgraph=False)\n","                self._compiled = True\n","            except Exception:\n","                self._compiled = False\n","        # Optimizer\n","        self._use_bnb = False\n","        if USE_BNB_8BIT and bnb is not None:\n","            try:\n","                self.opt = bnb.optim.AdamW8bit(self.model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","                self._use_bnb = True\n","            except Exception:\n","                pass\n","        if not self._use_bnb:\n","            self.opt = torch.optim.AdamW(\n","                self.model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY,\n","                eps=1e-8, betas=(0.9,0.95), foreach=True, fused=False\n","            )\n","        # AMP scaler (fp16 only)\n","        self.scaler = torch.amp.GradScaler(\"cuda\", enabled=(use_amp and amp_dtype==torch.float16))\n","        # OneCycleLR over optimizer UPDATES (post-accum)\n","        total_updates = (EPISODES * STEPS_PER_EP + ACCUM_STEPS - 1) // ACCUM_STEPS\n","        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n","            self.opt, max_lr=LR, total_steps=total_updates,\n","            pct_start=0.1, anneal_strategy=\"cos\", div_factor=10.0, final_div_factor=100.0\n","        )\n","        self.mem = MemoryBank(D_OUT, use_faiss=True, ivfpq=FAISS_USE_IVFPQ)\n","        self.ckpt_dir = os.path.join(SAVE_ROOT, \"graph_world_runs\", RUN_NAME)\n","        os.makedirs(self.ckpt_dir, exist_ok=True)\n","        self._dev_mem = None\n","        self.global_steps = 0; self.global_tokens = 0\n","        self.start_time = time.time()\n","        self._next_log_at = LOG_EVERY_STEPS; self._next_save_at = SAVE_EVERY_STEPS\n","\n","    def _refresh_dev_mem(self):\n","        X, _ = self.mem._cat()\n","        self._dev_mem = None if X is None else X.to(device)\n","\n","    def _time_exceeded(self):\n","        return (MAX_WALLCLOCK_MIN and (time.time()-self.start_time) > MAX_WALLCLOCK_MIN*60.0)\n","\n","    def _maybe_log(self, ep, step, loss_running, step_times):\n","        if self.global_steps >= self._next_log_at:\n","            avg_step = sum(step_times[-min(10,len(step_times)):]) / max(1, min(10,len(step_times)))\n","            lr = self.opt.param_groups[0][\"lr\"]\n","            print(f\"  - up {self.global_steps:7d} | ep {ep:03d}/{EPISODES} step {step:03d}/{STEPS_PER_EP} \"\n","                  f\"| loss={loss_running:.6f} | mem={self.mem.size} | lr={lr:.2e} | avg_step={avg_step:.3f}s\")\n","            self._next_log_at += LOG_EVERY_STEPS\n","\n","    def _maybe_save_step_ckpt(self):\n","        if self.global_steps >= self._next_save_at:\n","            base_model = self.model._orig_mod if self._compiled else self.model\n","            ckpt = {\n","                \"model\": base_model.state_dict(),\n","                \"opt\": self.opt.state_dict(),\n","                \"sched\": self.sched.state_dict(),\n","                \"meta\": dict(global_steps=self.global_steps, seed=SEED),\n","            }\n","            path = os.path.join(self.ckpt_dir, f\"ckpt_step{self.global_steps:08d}.pt\")\n","            torch.save(ckpt, path)\n","            self._next_save_at += SAVE_EVERY_STEPS\n","\n","    def train(self):\n","        print(f\"Device: {device} ({cuda_name}) | TF32={getattr(torch.backends.cuda.matmul, 'allow_tf32', False)} \"\n","              f\"| AMP_dtype={'bf16' if amp_dtype==torch.bfloat16 else 'fp16'} \"\n","              f\"| compile={'on' if self._compiled else 'off'} | bnb8bit={'on' if self._use_bnb else 'off'}\")\n","        print(f\"FAISS available: {bool(faiss)} | IVF-PQ={'on' if (bool(faiss) and FAISS_USE_IVFPQ) else 'off'}\")\n","        print(f\"Preset={PRESET} | BATCH={BATCH} | ACCUM_STEPS={ACCUM_STEPS} | effective_batch={BATCH*ACCUM_STEPS}\")\n","\n","        step_times=[]; done=False\n","        for ep in range(1, EPISODES+1):\n","            if done: break\n","            micro_loss_running = 0.0; micro_count = 0\n","            for step in range(1, STEPS_PER_EP+1):\n","                if done: break\n","                st = time.time()\n","\n","                # ---- CUDA Graphs step boundary (CRITICAL: before any forward pass) ----\n","                if self._compiled and hasattr(torch, \"compiler\") and hasattr(torch.compiler, \"cudagraph_mark_step_begin\"):\n","                    torch.compiler.cudagraph_mark_step_begin()\n","\n","                x, y = get_batch(BATCH)\n","\n","                # Forward with AMP\n","                with torch.amp.autocast(amp_device, dtype=amp_dtype, enabled=use_amp):\n","                    emb = self.model(x)\n","\n","                # CRITICAL: Clone embeddings IMMEDIATELY after forward pass to decouple from CUDA graph\n","                if self._compiled:\n","                    emb = emb.clone().detach().requires_grad_(True)\n","\n","                # Warm start: seed memory before first update\n","                if self.mem.size == 0:\n","                    self.mem.add(emb.detach(), labels=y.detach().clone())\n","                    step_times.append(time.time() - st)\n","                    continue\n","\n","                if (self._dev_mem is None) or (self._dev_mem.shape[0] != self.mem.size):\n","                    self._refresh_dev_mem()\n","\n","                # Memory search and loss computation\n","                idx, dist = self.mem.search(emb, TOPK)\n","                weights = (1.0 / (dist + 1e-6)) if dist.numel() > 0 else None\n","                mse = neighbor_mse(emb, self._dev_mem, idx, weights=weights)\n","                nce = info_nce(emb, self._dev_mem, idx, temp=TEMP)\n","                loss = LAMBDA_MSE * mse + LAMBDA_CONTRAST * nce\n","\n","                # Accumulate\n","                loss = loss / ACCUM_STEPS\n","                if self.scaler.is_enabled():\n","                    self.scaler.scale(loss).backward()\n","                else:\n","                    loss.backward()\n","\n","                micro_loss_running += float(loss.detach().cpu()); micro_count += 1\n","\n","                # Add to memory & opportunistic reindex (use detached emb for memory)\n","                self.mem.add(emb.detach(), labels=y.detach().clone())\n","                self.mem.maybe_reindex(force=False)\n","\n","                # Step optimizer every ACCUM_STEPS\n","                if micro_count % ACCUM_STEPS == 0:\n","                    if self.scaler.is_enabled():\n","                        self.scaler.step(self.opt); self.scaler.update()\n","                    else:\n","                        self.opt.step()\n","                    self.opt.zero_grad(set_to_none=True)\n","                    self.sched.step()\n","                    self.global_steps += 1\n","                    self._maybe_save_step_ckpt()\n","                    self._maybe_log(ep, step, micro_loss_running, step_times)\n","                    micro_loss_running = 0.0\n","\n","                self.global_tokens += x.shape[0]\n","                step_times.append(time.time() - st)\n","                if self._time_exceeded():\n","                    print(f\"[Wallclock reached ~{MAX_WALLCLOCK_MIN} min] Stopping gracefully.\")\n","                    done = True; break\n","\n","            # episode end\n","            self.mem.maybe_reindex(force=True)\n","            if (self._dev_mem is None) or (self._dev_mem.shape[0] != self.mem.size):\n","                self._refresh_dev_mem()\n","            print(f\"Episode {ep:03d} done | nodes={self.mem.size}\")\n","\n","            # epoch ckpt\n","            base_model = self.model._orig_mod if self._compiled else self.model\n","            ckpt = {\n","                \"model\": base_model.state_dict(),\n","                \"opt\": self.opt.state_dict(),\n","                \"sched\": self.sched.state_dict(),\n","                \"cfg\": dict(D_IN=D_IN, D_HID=D_HID, D_OUT=D_OUT, LR=LR, WD=WEIGHT_DECAY,\n","                            PRESET=PRESET, TOPK=TOPK, ACCUM_STEPS=ACCUM_STEPS),\n","                \"mem_size\": self.mem.size,\n","                \"global_steps\": self.global_steps,\n","            }\n","            path = os.path.join(self.ckpt_dir, f\"ckpt_ep{ep:03d}.pt\")\n","            torch.save(ckpt, path); time.sleep(0.01)\n","\n","        elapsed = time.time() - self.start_time\n","        print(f\"Done. Elapsed: {elapsed/60:.1f} min | preset={PRESET} | steps={self.global_steps}\")\n","        print(f\"Artifacts: {self.ckpt_dir}\")\n","\n","# ---- Run ----\n","runner = Runner()\n","runner.train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"authorship_tag":"ABX9TyPJ7rMz5h6jxd3JKrxwVUef"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}