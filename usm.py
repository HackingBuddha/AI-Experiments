# -*- coding: utf-8 -*-
"""USM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5h3CLeqYgqlmUszKtahGR7i18_X-Dea
"""

# @title
# -*- coding: utf-8 -*-
"""
USM v1.5 (clean): Hard Binding CTM core + perm-invariant + binding loss + slot probes
and an optional tiny GridWorld + hypergraph + Active Inference stub.

Paste this as a single Colab cell and run.
"""

import math
import random
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# ---------------------------------------------------------------------
# Config + device
# ---------------------------------------------------------------------

@dataclass
class USMConfig:
    # which "world" to run
    world_type: str = "hard_binding"       # "hard_binding" or "gridworld"
    seed: int = 0
    device: str = "cuda"

    # ----- Hard-binding task -----
    hb_train_samples: int = 8000
    hb_eval_samples: int = 4000
    hb_epochs: int = 100
    hb_batch_size: int = 256
    hb_lr: float = 1e-3

    hb_pred_loss_weight: float = 0.2
    hb_bind_loss_weight: float = 1.0
    hb_probe_loss_weight: float = 0.1
    hb_bind_warmup_epochs: int = 20

    # data augmentation / invariances
    hb_permute_objects: bool = True
    hb_jitter_std_train: float = 0.05         # jitter coords in train only
    hb_jitter_std_eval: float = 0.0           # usually 0

    # pattern-based generalisation:
    # "iid" = random queries; "heldout_pairs" = hold out entire (tgt_color, rel, ref_color) patterns from train
    hb_split_mode: str = "iid"                # or "heldout_pairs"
    hb_num_heldout_patterns: int = 16         # number of held-out patterns if split_mode == heldout_pairs

    # ----- GridWorld (toy) -----
    gw_size: int = 5
    gw_n_episodes: int = 200
    gw_max_steps: int = 40
    gw_lr: float = 1e-3
    gw_gamma: float = 0.99
    gw_latent_dim: int = 64


def resolve_device(cfg: USMConfig) -> torch.device:
    if cfg.device == "cuda" and torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")


# global device handle (updated in main/run_X)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ---------------------------------------------------------------------
# HARD BINDING DATASET
# ---------------------------------------------------------------------

class HardBindingDataset(Dataset):
    """
    Scene with 4 objects, each has:
      - color ∈ {red, green, blue, yellow}
      - shape ∈ {ball, cube, cone, star}
      - (x, y) ∈ {0,0.2,0.4,0.6,0.8}²

    Query:
      "What shape is <target_color> <relation> of <ref_color>?"

    Relations:
      - left_of, right_of, above, below

    Label:
      - shape index of one candidate object satisfying the relation.
      - Some scenes are ambiguous (multiple candidates) → marked separately.

    Extra (for binding probes, no gradient):
      - cand_mask: [4] bool, which object indices satisfy the relation.

    Encoding:
      - 4 objects × (color one-hot 4 + shape one-hot 4 + x + y) = 4 × 10 = 40
      - query = target_color (4) + relation (4) + ref_color (4) = 12
      - total input dim = 52
    """

    COLORS = ["red", "green", "blue", "yellow"]
    SHAPES = ["ball", "cube", "cone", "star"]
    RELS   = ["left_of", "right_of", "above", "below"]

    def __init__(
        self,
        n_samples: int = 8000,
        seed: int = 0,
        allowed_patterns: Optional[List[Tuple[int, int, int]]] = None,
        permute_objects: bool = True,
        coord_jitter_std: float = 0.0,
    ):
        """
        allowed_patterns: optional list of (tgt_color_idx, rel_idx, ref_color_idx)
          - if None: any pattern allowed
          - if not None: only generate queries with pattern in this list
        """
        super().__init__()
        self.n_samples = n_samples
        self.rng = random.Random(seed)
        self.permute_objects = permute_objects
        self.coord_jitter_std = coord_jitter_std
        self.allowed_patterns = None
        if allowed_patterns is not None:
            # normalize to set of tuples of ints
            self.allowed_patterns = {tuple(map(int, p)) for p in allowed_patterns}

        # each sample is a dict:
        # {
        #   "objects": [ {color:str, shape:str, x:float, y:float} × 4 ],
        #   "target_color": str,
        #   "rel": str,
        #   "ref_color": str,
        #   "y": int (shape index),
        #   "amb": bool,
        #   "cand_mask": BoolTensor[4],
        #   "pattern": (tgt_idx, rel_idx, ref_idx)
        # }
        self.samples: List[Dict[str, Any]] = []
        self._generate()

    def _sample_coord(self) -> float:
        # 5×5 grid → coordinates in {0,0.2,0.4,0.6,0.8}
        return self.rng.choice([0.0, 0.2, 0.4, 0.6, 0.8])

    def _relation_holds(self, obj, ref, rel: str) -> bool:
        x, y = obj["x"], obj["y"]
        rx, ry = ref["x"], ref["y"]
        if rel == "left_of":
            return x < rx and abs(y - ry) <= 0.4
        if rel == "right_of":
            return x > rx and abs(y - ry) <= 0.4
        if rel == "above":
            return y > ry and abs(x - rx) <= 0.4
        if rel == "below":
            return y < ry and abs(x - rx) <= 0.4
        return False

    def _generate_one(self) -> Dict[str, Any]:
        colors = self.COLORS
        shapes = self.SHAPES
        rels = self.RELS

        # sample until pattern is allowed (if restricted)
        while True:
            # 4 random objects
            objects = []
            for _ in range(4):
                obj = {
                    "color": self.rng.choice(colors),
                    "shape": self.rng.choice(shapes),
                    "x": self._sample_coord(),
                    "y": self._sample_coord(),
                }
                objects.append(obj)

            # choose query
            ref_idx = self.rng.randrange(4)
            ref = objects[ref_idx]
            ref_color = ref["color"]

            target_color = self.rng.choice(colors)
            rel = self.rng.choice(rels)

            tgt_idx = colors.index(target_color)
            rel_idx = rels.index(rel)
            ref_idx_color = colors.index(ref_color)
            pattern = (tgt_idx, rel_idx, ref_idx_color)

            if (self.allowed_patterns is not None) and (pattern not in self.allowed_patterns):
                # resample
                continue

            # Candidate targets: objects of target_color in relation to ANY ref of ref_color
            candidates = []
            for i, obj in enumerate(objects):
                if obj["color"] != target_color:
                    continue
                holds_any = False
                for j, ref2 in enumerate(objects):
                    if ref2["color"] == ref_color and i != j:
                        if self._relation_holds(obj, ref2, rel):
                            holds_any = True
                            break
                if holds_any:
                    candidates.append(i)

            if not candidates:
                # try again, but don't get stuck forever
                # (if impossible, relax and create a "fake" candidate)
                # In practice this rarely triggers.
                if self.rng.random() < 0.05:
                    candidates = [self.rng.randrange(4)]
                else:
                    continue

            ambiguous = len(candidates) > 1

            chosen_idx = self.rng.choice(candidates)
            answer_shape = objects[chosen_idx]["shape"]
            y_idx = shapes.index(answer_shape)

            cand_mask = torch.zeros(4, dtype=torch.bool)
            for idx in candidates:
                cand_mask[idx] = True

            sample = {
                "objects": objects,
                "target_color": target_color,
                "rel": rel,
                "ref_color": ref_color,
                "y": y_idx,
                "amb": ambiguous,
                "cand_mask": cand_mask,
                "pattern": pattern,
            }
            return sample

    def _generate(self):
        self.samples = [self._generate_one() for _ in range(self.n_samples)]

    def __len__(self):
        return len(self.samples)

    def sample_humans(self, k: int = 3):
        out = []
        for i in range(min(k, len(self.samples))):
            s = self.samples[i]
            objects = s["objects"]
            q = (s["target_color"], s["rel"], s["ref_color"])
            answer = self.SHAPES[s["y"]]
            amb = s["amb"]
            out.append((objects, q, answer, amb))
        return out

    def __getitem__(self, idx: int):
        """
        Returns:
          x: [52] float32
          y: int
          amb: bool
          cand_mask: [4] bool
          color_idx: [4] long
          shape_idx: [4] long
          pos_xy: [4,2] float32
        """
        s = self.samples[idx]

        # work on local copy of objects so we can permute/jitter without affecting stored sample
        objects = [dict(o) for o in s["objects"]]
        cand_mask = s["cand_mask"].clone()
        colors = self.COLORS
        shapes = self.SHAPES
        rels = self.RELS

        # permutation invariance over object order
        if self.permute_objects:
            perm = torch.randperm(4)
            objects = [objects[int(i)] for i in perm]
            cand_mask = cand_mask[perm]

        # coordinate jitter for train set
        if self.coord_jitter_std > 0.0:
            sigma = self.coord_jitter_std
            for obj in objects:
                obj["x"] = max(0.0, min(1.0, obj["x"] + self.rng.gauss(0.0, sigma)))
                obj["y"] = max(0.0, min(1.0, obj["y"] + self.rng.gauss(0.0, sigma)))

        # encode features
        feat: List[float] = []
        for obj in objects:
            color_oh = [1.0 if obj["color"] == c else 0.0 for c in colors]
            shape_oh = [1.0 if obj["shape"] == s else 0.0 for s in shapes]
            feat.extend(color_oh + shape_oh + [obj["x"], obj["y"]])

        tgt_color = s["target_color"]
        rel = s["rel"]
        ref_color = s["ref_color"]

        tgt_oh = [1.0 if tgt_color == c else 0.0 for c in colors]
        rel_oh = [1.0 if rel == r else 0.0 for r in rels]
        ref_oh = [1.0 if ref_color == c else 0.0 for c in colors]
        feat.extend(tgt_oh + rel_oh + ref_oh)

        x = torch.tensor(feat, dtype=torch.float32)
        y = int(s["y"])
        amb = bool(s["amb"])
        color_idx = torch.tensor([colors.index(o["color"]) for o in objects], dtype=torch.long)
        shape_idx = torch.tensor([shapes.index(o["shape"]) for o in objects], dtype=torch.long)
        pos_xy = torch.tensor([[o["x"], o["y"]] for o in objects], dtype=torch.float32)
        return x, y, amb, cand_mask, color_idx, shape_idx, pos_xy


def ambig_fraction(ds: HardBindingDataset) -> Tuple[float, int]:
    ambigs = sum(1 for s in ds.samples if s["amb"])
    return ambigs / len(ds.samples), ambigs


# ---------------------------------------------------------------------
# MLP BASELINE
# ---------------------------------------------------------------------

class MLPBaseline(nn.Module):
    def __init__(self, input_dim=52, hidden=64, n_classes=4):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, n_classes),
        )

    def forward(self, x):
        return self.net(x)


# ---------------------------------------------------------------------
# CTM BLOCK (with LTC-ish dynamics + adaptive p + latent prediction loss)
# ---------------------------------------------------------------------

class CTMBlock(nn.Module):
    def __init__(
        self,
        dim: int,
        n_slots: int,
        n_ticks: int = 3,
        dt: float = 1.0,
        p_min: float = 2.0,
        p_max: float = 4.0,
    ):
        super().__init__()
        self.dim = dim
        self.n_slots = n_slots
        self.n_ticks = n_ticks
        self.dt = dt
        self.p_min = p_min
        self.p_max = p_max

        self.in_proj = nn.Linear(dim, dim)
        self.pre_ln = nn.LayerNorm(dim)

        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        self.out_proj = nn.Linear(dim, dim)

        # LTC per-dimension timescales
        self.log_tau = nn.Parameter(torch.zeros(dim))

        # Latent prediction network: predict z_{t+1} from z_t
        self.pred_net = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.Linear(dim, dim),
        )

    def forward(self, z: torch.Tensor, return_attn: bool = False):
        """
        z: [B, T, d]

        Returns:
          z_new: [B, T, d]
          pred_loss: scalar
          eff_p_mean: scalar (detached, for logging)
          last_attn: [B, T, T] or None (final tick attention)
        """
        B, T, d = z.shape
        tau = torch.exp(self.log_tau).view(1, 1, d)  # [1,1,d]

        pred_losses = []
        eff_ps = []
        last_attn = None

        for _ in range(self.n_ticks):
            h = self.pre_ln(self.in_proj(z))  # [B,T,d]

            Q = self.q_proj(h)
            K = self.k_proj(h)
            V = self.v_proj(h)

            sim = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d)  # [B,T,T]
            attn = torch.softmax(sim, dim=-1)                           # [B,T,T]

            # Attention peakiness → sync confidence
            row_max = attn.max(dim=-1).values  # [B,T]
            uniform = 1.0 / T
            conf = (row_max.mean(dim=-1) - uniform) / (1.0 - uniform + 1e-6)  # [B]
            conf = conf.clamp(0.0, 1.0)

            eff_p = self.p_min + (self.p_max - self.p_min) * conf  # [B]
            eff_ps.append(eff_p.detach())

            eff_p_exp = eff_p.view(B, 1, 1)
            sim_sharp = sim * eff_p_exp
            attn_sharp = torch.softmax(sim_sharp, dim=-1)  # [B,T,T]
            last_attn = attn_sharp

            context = torch.matmul(attn_sharp, V)  # [B,T,d]
            target = self.out_proj(context)        # [B,T,d]

            # latent prediction
            z_pred = self.pred_net(z)             # [B,T,d]

            # LTC update: dz/dt = (target - z) / tau
            dz = (target - z) / (tau + 1e-6)
            z_next = z + self.dt * dz

            pred_loss = F.mse_loss(z_pred, z_next.detach())
            pred_losses.append(pred_loss)

            z = z_next

        pred_loss_mean = torch.stack(pred_losses).mean()
        eff_p_mean = torch.cat(eff_ps).mean()
        if return_attn:
            return z, pred_loss_mean, eff_p_mean.detach(), last_attn
        else:
            return z, pred_loss_mean, eff_p_mean.detach(), None


# ---------------------------------------------------------------------
# HIERARCHICAL CTM MODEL: 2 LAYERS (FAST + SLOW) + PROBES
# ---------------------------------------------------------------------

class CTMHierModel(nn.Module):
    """
    2-layer CTM-ish model for hard binding:

    - Encodes 4 objects into slots.
    - Encodes query into a separate slot.
    - Total slots: 5 (4 objects + 1 query).
    - Layer 1: fast CTM (few ticks).
    - Layer 2: slow CTM (more ticks).
    - Reads out from query slot after slow layer (pointer to object slots).
    - Adds probe heads to decode slot → (color, shape, position).
    """

    def __init__(
        self,
        input_dim: int = 52,
        slot_dim: int = 32,
        n_slots: int = 4,
        n_classes: int = 4,
        n_ticks_fast: int = 2,
        n_ticks_slow: int = 3,
        n_colors: int = 4,
        n_shapes: int = 4,
    ):
        super().__init__()
        self.input_dim = input_dim
        self.slot_dim = slot_dim
        self.n_slots = n_slots
        self.n_classes = n_classes
        self.n_colors = n_colors
        self.n_shapes = n_shapes

        # Each object chunk: color(4) + shape(4) + x + y = 10 dims
        self.obj_dim = 10
        assert 4 * self.obj_dim + 12 == input_dim, "input_dim mismatch"

        # Object encoder: 10 -> slot_dim
        self.slot_enc = nn.Sequential(
            nn.Linear(self.obj_dim, slot_dim),
            nn.ReLU(),
            nn.Linear(slot_dim, slot_dim),
        )

        # Query encoder: 12 -> slot_dim
        self.query_enc = nn.Sequential(
            nn.Linear(12, slot_dim),
            nn.ReLU(),
            nn.Linear(slot_dim, slot_dim),
        )

        # Hierarchical CTM blocks
        self.ctm_fast = CTMBlock(slot_dim, n_slots + 1, n_ticks=n_ticks_fast)
        self.ctm_slow = CTMBlock(slot_dim, n_slots + 1, n_ticks=n_ticks_slow)

        # Per-slot shape head (object-wise logits)
        self.slot_shape_head = nn.Linear(slot_dim, n_classes)

        # Probe heads for slot interpretability
        self.probe_color_head = nn.Linear(slot_dim, n_colors)
        self.probe_shape_head = nn.Linear(slot_dim, n_shapes)
        self.probe_pos_head   = nn.Linear(slot_dim, 2)

    def encode_slots(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [B, 52]
        Returns:
          z0: [B, 5, slot_dim]
          slots 0..3: objects
          slot 4: query
        """
        B = x.shape[0]
        obj_feats = x[:, :4 * self.obj_dim].view(B, 4, self.obj_dim)   # [B,4,10]
        query_feats = x[:, 4 * self.obj_dim:]                           # [B,12]

        obj_emb = self.slot_enc(obj_feats)                              # [B,4,D]
        query_emb = self.query_enc(query_feats).unsqueeze(1)            # [B,1,D]

        z0 = torch.cat([obj_emb, query_emb], dim=1)                     # [B,5,D]
        return z0

    def forward(
        self,
        x: torch.Tensor,
        return_slots: bool = False,
        return_attn: bool = False,
    ):
        """
        x: [B, 52]
        Returns:
          logits: [B, n_classes]
          pred_loss: scalar
          eff_p_mean: scalar
          attn_q2s: [B, n_slots] (query→object attention)
          slots: [B, n_slots, D] or None
        """
        z = self.encode_slots(x)
        z, pred_loss_fast, eff_p_fast, _ = self.ctm_fast(z, return_attn=False)
        z, pred_loss_slow, eff_p_slow, attn_last = self.ctm_slow(z, return_attn=True)

        pred_loss = pred_loss_fast + pred_loss_slow
        eff_p_mean = 0.5 * (eff_p_fast + eff_p_slow)

        slots = z[:, :self.n_slots, :]   # [B, n_slots, D]

        # Query→slot attention (from last slot row to the object slots)
        q_idx = attn_last.shape[1] - 1
        obj_attn = attn_last[:, q_idx, :self.n_slots]  # [B, n_slots]
        attn_q2s = obj_attn / (obj_attn.sum(dim=-1, keepdim=True) + 1e-8)

        # Per-slot logits over shapes
        slot_logits = self.slot_shape_head(slots)  # [B, n_slots, n_classes]

        # Pointer-style aggregation (log-space mix)
        log_p_slots = F.log_softmax(slot_logits, dim=-1)
        log_attn = torch.log(attn_q2s.clamp_min(1e-8)).unsqueeze(-1)   # [B,n_slots,1]
        joint_log = log_p_slots + log_attn
        final_logits = torch.logsumexp(joint_log, dim=1)               # [B,n_classes]

        if not return_slots:
            slots_out = None
        else:
            slots_out = slots
        attn_out = attn_last if return_attn else None

        return final_logits, pred_loss, eff_p_mean, attn_q2s, slots_out if return_slots else None


# ---------------------------------------------------------------------
# EVALUATION HELPERS
# ---------------------------------------------------------------------

def evaluate_model(model: nn.Module, loader, device: torch.device):
    model.eval()
    correct = total = 0
    correct_simple = total_simple = 0
    correct_amb = total_amb = 0

    with torch.no_grad():
        for batch in loader:
            x, y, amb, *_ = batch
            x = x.to(device)
            y = y.to(device)

            if isinstance(model, CTMHierModel):
                logits, _, _, _, _ = model(x)
            else:
                logits = model(x)

            preds = logits.argmax(dim=-1)

            correct += (preds == y).sum().item()
            total += y.numel()

            amb = amb.to(torch.bool)
            simple_mask = ~amb
            amb_mask = amb

            simple_idx = simple_mask.nonzero(as_tuple=False).squeeze(-1)
            amb_idx = amb_mask.nonzero(as_tuple=False).squeeze(-1)

            if simple_idx.numel() > 0:
                correct_simple += (preds[simple_idx] == y[simple_idx]).sum().item()
                total_simple += simple_idx.numel()

            if amb_idx.numel() > 0:
                correct_amb += (preds[amb_idx] == y[amb_idx]).sum().item()
                total_amb += amb_idx.numel()

    overall = correct / total if total else 0.0
    simple = correct_simple / total_simple if total_simple else 0.0
    amb = correct_amb / total_amb if total_amb else 0.0
    return overall, simple, amb


def evaluate_binding(
    ctm: CTMHierModel,
    loader,
    device: torch.device,
    n_obj_slots: int = 4,
):
    """
    Probes whether the query slot's attention actually binds to the correct objects.

    For each sample:
      - Take attention from query slot row to the first n_obj_slots.
      - Binding-argmax is index of max attention among object slots.
      - If that index is in cand_mask (one of the valid candidates), count as success.

    Returns:
      (overall_acc, simple_acc, amb_acc, overall_mass_on_correct)
    """
    ctm.eval()
    correct = total = 0
    correct_simple = total_simple = 0
    correct_amb = total_amb = 0
    mass_sum = 0.0
    mass_count = 0

    with torch.no_grad():
        for batch in loader:
            x, _, amb, cand_mask, *_ = batch
            x = x.to(device)
            amb = amb.to(torch.bool)
            cand_mask = cand_mask.to(device)  # [B,4]

            logits, _, _, attn_q2s, _ = ctm(x, return_attn=True, return_slots=False)

            obj_attn = attn_q2s[:, :n_obj_slots]  # [B,4]
            B = obj_attn.shape[0]

            # argmax binding
            max_idx = obj_attn.argmax(dim=-1)  # [B]
            batch_indices = torch.arange(B, device=device)
            correct_flags = cand_mask[batch_indices, max_idx]  # bool

            correct += correct_flags.sum().item()
            total += B

            # attention mass on ANY correct candidate(s)
            mass_on_correct = (obj_attn * cand_mask.float()).sum(dim=-1)  # [B]
            mass_sum += mass_on_correct.sum().item()
            mass_count += B

            # simple vs amb splits
            simple_mask = ~amb
            amb_mask = amb

            if simple_mask.any():
                sm_idx = simple_mask.nonzero(as_tuple=False).squeeze(-1)
                correct_simple += correct_flags[sm_idx].sum().item()
                total_simple += sm_idx.numel()

            if amb_mask.any():
                am_idx = amb_mask.nonzero(as_tuple=False).squeeze(-1)
                correct_amb += correct_flags[am_idx].sum().item()
                total_amb += am_idx.numel()

    overall = correct / total if total else 0.0
    simple = correct_simple / total_simple if total_simple else 0.0
    amb = correct_amb / total_amb if total_amb else 0.0
    mean_mass = mass_sum / mass_count if mass_count else 0.0
    return overall, simple, amb, mean_mass


def decode_query_pattern(x_batch: torch.Tensor) -> torch.Tensor:
    """
    Decode (tgt_color_idx, rel_idx, ref_color_idx) triple from the query part of x.

    x_batch: [B, 52] with last 12 dims = tgt_color (4) + rel (4) + ref_color (4)

    Returns:
      patterns: [B, 3] long tensor
    """
    B = x_batch.size(0)
    obj_dim = 10
    query = x_batch[:, 4 * obj_dim:]  # [B, 12]

    tgt_oh = query[:, 0:4]
    rel_oh = query[:, 4:8]
    ref_oh = query[:, 8:12]

    tgt_idx = tgt_oh.argmax(dim=-1)
    rel_idx = rel_oh.argmax(dim=-1)
    ref_idx = ref_oh.argmax(dim=-1)

    patterns = torch.stack([tgt_idx, rel_idx, ref_idx], dim=-1).long()
    return patterns


def evaluate_model_with_patterns(
    model: nn.Module,
    loader,
    device: torch.device,
    heldout_patterns: Optional[List[Tuple[int, int, int]]] = None,
):
    """
    Evaluate accuracy separately on:
      - heldout query patterns (never seen in train) if provided
      - "train" patterns (everything else)

    heldout_patterns: list of (tgt_idx, rel_idx, ref_idx) triplets.
    """
    model.eval()
    if heldout_patterns is None:
        return {}

    heldout_set = {tuple(map(int, p)) for p in heldout_patterns}

    stats = {
        "heldout_total": 0,
        "heldout_correct": 0,
        "train_total": 0,
        "train_correct": 0,
    }

    with torch.no_grad():
        for batch in loader:
            x, y, amb, *_ = batch
            x = x.to(device)
            y = y.to(device)

            if isinstance(model, CTMHierModel):
                logits, _, _, _, _ = model(x)
            else:
                logits = model(x)

            preds = logits.argmax(dim=-1)
            patterns = decode_query_pattern(x)   # [B,3]

            for i in range(y.size(0)):
                patt = tuple(patterns[i].tolist())
                is_held = patt in heldout_set
                if is_held:
                    stats["heldout_total"] += 1
                    if preds[i] == y[i]:
                        stats["heldout_correct"] += 1
                else:
                    stats["train_total"] += 1
                    if preds[i] == y[i]:
                        stats["train_correct"] += 1

    def safe_div(a, b):
        return a / b if b > 0 else 0.0

    result = {
        "heldout_acc": safe_div(stats["heldout_correct"], stats["heldout_total"]),
        "train_acc": safe_div(stats["train_correct"], stats["train_total"]),
        "heldout_count": stats["heldout_total"],
        "train_count": stats["train_total"],
    }
    return result


# ---------------------------------------------------------------------
# LOSSES: binding + probe + schedules
# ---------------------------------------------------------------------

def compute_binding_loss(
    attn_q2s: torch.Tensor,
    cand_mask: torch.Tensor,
    amb: torch.Tensor,
    eps: float = 1e-8,
) -> torch.Tensor:
    """
    Supervised binding loss for query→object attention.

    Handles simple (single candidate), ambiguous (multiple candidates),
    and degenerate (no candidate) cases robustly.
    """
    assert attn_q2s is not None, "attn_q2s must not be None"
    assert attn_q2s.dim() == 2, "attn_q2s should be [B, n_slots]"
    assert cand_mask.shape == attn_q2s.shape, "cand_mask shape mismatch"

    attn_q2s = attn_q2s.to(dtype=torch.float32)
    cand_mask = cand_mask.to(attn_q2s.device)
    amb = amb.to(attn_q2s.device).bool()

    simple_mask = ~amb
    losses = []

    if simple_mask.any():
        idx = simple_mask.nonzero(as_tuple=False).squeeze(-1)
        attn_simple = attn_q2s[idx]
        cand_simple = cand_mask[idx].float()
        target_idx = cand_simple.argmax(dim=-1)
        ce_simple = F.nll_loss((attn_simple + eps).log(), target_idx, reduction="mean")
        losses.append(ce_simple)

    if amb.any():
        idx = amb.nonzero(as_tuple=False).squeeze(-1)
        attn_amb = attn_q2s[idx]
        cand_amb = cand_mask[idx].float()
        cand_sum = cand_amb.sum(dim=-1, keepdim=True)
        valid = (cand_sum.squeeze(-1) > 0).nonzero(as_tuple=False).squeeze(-1)
        if valid.numel() > 0:
            attn_sel = attn_amb[valid]
            cand_sel = cand_amb[valid]
            cand_soft = cand_sel / (cand_sel.sum(dim=-1, keepdim=True) + eps)
            ce_amb = -(cand_soft * (attn_sel + eps).log()).sum(dim=-1).mean()
            losses.append(ce_amb)

    if not losses:
        return torch.tensor(0.0, device=attn_q2s.device)
    return torch.stack(losses).mean()


def attention_entropy(attn_q2s: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    """Average entropy of the query→slot attention."""
    p = attn_q2s.clamp_min(eps)
    return -(p * p.log()).sum(dim=-1).mean()


def compute_probe_loss(
    ctm: CTMHierModel,
    slots: torch.Tensor,
    color_idx: torch.Tensor,
    shape_idx: torch.Tensor,
    pos_xy: torch.Tensor,
) -> torch.Tensor:
    """Probe slots for object attributes (color, shape, position)."""
    if slots is None:
        return torch.tensor(0.0, device=color_idx.device)

    color_logits = ctm.probe_color_head(slots)  # [B,4,n_colors]
    shape_logits = ctm.probe_shape_head(slots)  # [B,4,n_shapes]
    pos_pred = ctm.probe_pos_head(slots)        # [B,4,2]

    color_loss = F.cross_entropy(
        color_logits.view(-1, color_logits.size(-1)),
        color_idx.view(-1),
    )
    shape_loss = F.cross_entropy(
        shape_logits.view(-1, shape_logits.size(-1)),
        shape_idx.view(-1),
    )
    pos_loss = F.mse_loss(pos_pred, pos_xy)

    loss = color_loss + shape_loss + 2.0 * pos_loss
    return loss


# ---------------------------------------------------------------------
# HARD BINDING EXPERIMENT WRAPPER
# ---------------------------------------------------------------------

def run_hard_binding_experiment(cfg: USMConfig):
    global device
    device = resolve_device(cfg)
    torch.manual_seed(cfg.seed)
    random.seed(cfg.seed)

    print("=" * 60)
    print("USM v1.5: HARD BINDING + 2-LAYER CTM + ADAPTIVE p + LATENT PRED")
    print("         + BINDING LOSS (WARMUP) + SLOT PROBES + PERM-INVARIANT")
    print("=" * 60)
    print("Device:", device)

    # set up pattern split (for generalization)
    heldout_patterns = None
    train_patterns = None

    if cfg.hb_split_mode == "heldout_pairs":
        colors = list(range(len(HardBindingDataset.COLORS)))
        rels = list(range(len(HardBindingDataset.RELS)))
        all_patterns = [(c_t, r, c_ref) for c_t in colors for r in rels for c_ref in colors]
        random.shuffle(all_patterns)
        k = min(cfg.hb_num_heldout_patterns, len(all_patterns))
        heldout_patterns = all_patterns[:k]
        train_patterns = all_patterns[k:]
        print(f"[Split] Using HELD-OUT patterns for generalization ({k} patterns)")
    elif cfg.hb_split_mode == "iid":
        print("[Split] Using IID query patterns (no held-out patterns)")
    else:
        raise ValueError(f"Unknown hb_split_mode: {cfg.hb_split_mode}")

    # datasets
    if cfg.hb_split_mode == "iid":
        train_ds = HardBindingDataset(
            n_samples=cfg.hb_train_samples,
            seed=cfg.seed,
            allowed_patterns=None,
            permute_objects=cfg.hb_permute_objects,
            coord_jitter_std=cfg.hb_jitter_std_train,
        )
    else:
        train_ds = HardBindingDataset(
            n_samples=cfg.hb_train_samples,
            seed=cfg.seed,
            allowed_patterns=train_patterns,
            permute_objects=cfg.hb_permute_objects,
            coord_jitter_std=cfg.hb_jitter_std_train,
        )

    eval_ds = HardBindingDataset(
        n_samples=cfg.hb_eval_samples,
        seed=cfg.seed + 1,
        allowed_patterns=None,  # full pattern support in eval
        permute_objects=cfg.hb_permute_objects,
        coord_jitter_std=cfg.hb_jitter_std_eval,
    )

    amb_train, amb_train_n = ambig_fraction(train_ds)
    amb_eval, amb_eval_n = ambig_fraction(eval_ds)

    print("\nHard Binding Task:")
    print(f"  Objects: 4")
    print(f"  Colors can REPEAT: True")
    print(f"  Input dim: 52")
    print(f"  Output dim: 4 (shapes)")
    print(f"  Ambiguous fraction (train): {amb_train*100:.1f}% ({amb_train_n} cases)")
    print(f"  Ambiguous fraction (eval):  {amb_eval*100:.1f}% ({amb_eval_n} cases)")

    # show a few examples
    print("\n--- Examples ---\n")
    for objects, query, answer, amb in train_ds.sample_humans(3):
        qs = f"What shape is {query[0]} {query[1]} of {query[2]}?"
        print(f"Objects: {[(o['color'], o['shape'], o['x'], o['y']) for o in objects]}")
        print(f"Query: {qs}")
        print(f"Answer: {answer}")
        print(f"Has ambiguity: {amb}\n")

    train_loader = DataLoader(train_ds, batch_size=cfg.hb_batch_size, shuffle=True)
    eval_loader = DataLoader(eval_ds, batch_size=cfg.hb_batch_size, shuffle=False)

    # ---- Models ----
    mlp = MLPBaseline(input_dim=52, hidden=64, n_classes=4).to(device)
    ctm = CTMHierModel(
        input_dim=52,
        slot_dim=32,
        n_slots=4,
        n_classes=4,
        n_ticks_fast=2,
        n_ticks_slow=3,
        n_colors=len(HardBindingDataset.COLORS),
        n_shapes=len(HardBindingDataset.SHAPES),
    ).to(device)

    n_params_ctm = sum(p.numel() for p in ctm.parameters())
    n_params_mlp = sum(p.numel() for p in mlp.parameters())
    print("----------------------------------------")
    print("MODELS")
    print("----------------------------------------")
    print(f"CTM: {n_params_ctm:,} params")
    print(f"MLP: {n_params_mlp:,} params")
    print(f"Ratio: {n_params_ctm / max(1, n_params_mlp):.2f}x")

    # ---- Train MLP baseline ----
    print("\n----------------------------------------")
    print(f"TRAINING MLP ({cfg.hb_epochs} epochs)")
    print("----------------------------------------")
    ce = nn.CrossEntropyLoss()
    opt_mlp = torch.optim.AdamW(mlp.parameters(), lr=cfg.hb_lr)

    for epoch in range(1, cfg.hb_epochs + 1):
        mlp.train()
        for x, y, amb, cand_mask, color_idx, shape_idx, pos_xy in train_loader:
            x = x.to(device)
            y = y.to(device)
            opt_mlp.zero_grad()
            logits = mlp(x)
            loss = ce(logits, y)
            loss.backward()
            opt_mlp.step()

        if epoch % 20 == 0 or epoch == cfg.hb_epochs:
            acc, acc_simple, acc_amb = evaluate_model(mlp, eval_loader, device)
            print(
                f"Epoch {epoch:3d} | MLP | "
                f"Eval: {acc*100:4.1f}% | "
                f"Simple: {acc_simple*100:4.1f}% | "
                f"Ambig: {acc_amb*100:4.1f}%"
            )

    # ---- Train CTM ----
    print("\n----------------------------------------")
    print(
        f"TRAINING CTM ({cfg.hb_epochs} epochs) + Adaptive p + Latent Pred Loss "
        f"+ Binding Loss (annealed) + Slot Probes"
    )
    print("----------------------------------------")

    opt_ctm = torch.optim.AdamW(ctm.parameters(), lr=cfg.hb_lr)
    pred_loss_weight = cfg.hb_pred_loss_weight
    probe_loss_weight = cfg.hb_probe_loss_weight

    for epoch in range(1, cfg.hb_epochs + 1):
        ctm.train()
        running_eff_p = 0.0
        running_batches = 0
        running_pred = 0.0
        running_bind = 0.0
        running_probe = 0.0
        running_attn_ent = 0.0
        attn_debug_stats = None

        if epoch <= cfg.hb_bind_warmup_epochs:
            bind_w = cfg.hb_bind_loss_weight * (epoch / cfg.hb_bind_warmup_epochs)
        else:
            bind_w = cfg.hb_bind_loss_weight

        for x, y, amb, cand_mask, color_idx, shape_idx, pos_xy in train_loader:
            x = x.to(device)
            y = y.to(device)
            amb = amb.to(device)
            cand_mask = cand_mask.to(device)
            color_idx = color_idx.to(device)
            shape_idx = shape_idx.to(device)
            pos_xy = pos_xy.to(device)

            opt_ctm.zero_grad()
            logits, pred_loss, eff_p_mean, attn_q2s, slots = ctm(
                x,
                return_attn=True,
                return_slots=True,
            )
            loss_cls = ce(logits, y)
            bind_loss = compute_binding_loss(attn_q2s, cand_mask, amb)
            probe_loss = compute_probe_loss(ctm, slots, color_idx, shape_idx, pos_xy)

            loss = (
                loss_cls
                + pred_loss_weight * pred_loss
                + bind_w * bind_loss
                + probe_loss_weight * probe_loss
            )

            loss.backward()
            opt_ctm.step()

            running_eff_p += float(eff_p_mean)
            running_pred += float(pred_loss.detach())
            running_bind += float(bind_loss.detach())
            running_probe += float(probe_loss.detach())
            running_attn_ent += float(attention_entropy(attn_q2s.detach()))
            running_batches += 1

            if attn_debug_stats is None:
                with torch.no_grad():
                    attn_sum = attn_q2s.sum(dim=-1)
                    attn_debug_stats = (
                        float(attn_q2s.min()),
                        float(attn_q2s.max()),
                        float(attn_sum.mean()),
                        bool(attn_q2s.requires_grad),
                    )

        if epoch % 20 == 0 or epoch == cfg.hb_epochs:
            acc, acc_simple, acc_amb = evaluate_model(ctm, eval_loader, device)
            avg_eff_p = running_eff_p / max(1, running_batches)
            avg_pred = running_pred / max(1, running_batches)
            avg_bind = running_bind / max(1, running_batches)
            avg_probe = running_probe / max(1, running_batches)
            avg_attn_ent = running_attn_ent / max(1, running_batches)
            if attn_debug_stats is not None:
                attn_min, attn_max, attn_sum_mean, attn_req_grad = attn_debug_stats
                print(
                    f"  [AttnDbg] min={attn_min:.4f} max={attn_max:.4f} "
                    f"mean_sum={attn_sum_mean:.4f} requires_grad={attn_req_grad}"
                )
            print(
                f"Epoch {epoch:3d} | CTM | "
                f"Eval: {acc*100:4.1f}% | "
                f"Simple: {acc_simple*100:4.1f}% | "
                f"Ambig: {acc_amb*100:4.1f}% | "
                f"PredLoss: {avg_pred:.4f} | "
                f"BindLoss: {avg_bind:.4f} | "
                f"ProbeLoss: {avg_probe:.4f} | "
                f"BindW: {bind_w:.3f} | "
                f"AttnEnt: {avg_attn_ent:.3f} | "
                f"eff_p: {avg_eff_p:.2f}"
            )

    # ---- Final classification comparison ----
    print("\n" + "=" * 60)
    print("DETAILED EVALUATION (CLASSIFICATION)")
    print("=" * 60)

    mlp_overall, mlp_simple, mlp_amb = evaluate_model(mlp, eval_loader, device)
    ctm_overall, ctm_simple, ctm_amb = evaluate_model(ctm, eval_loader, device)

    print("\nMLP Results:")
    print(f"  Overall: {mlp_overall*100:4.1f}%")
    print(f"  Simple (no ambiguity): {mlp_simple*100:4.1f}%")
    print(f"  AMBIGUOUS (hard): {mlp_amb*100:4.1f}%")

    print("\nCTM Results:")
    print(f"  Overall: {ctm_overall*100:4.1f}%")
    print(f"  Simple (no ambiguity): {ctm_simple*100:4.1f}%")
    print(f"  AMBIGUOUS (hard): {ctm_amb*100:4.1f}%")

    print("\n" + "=" * 60)
    print("CLASSIFICATION COMPARISON")
    print("=" * 60)
    print("\nOVERALL:")
    print(f"  MLP: {mlp_overall*100:4.1f}%")
    print(f"  CTM: {ctm_overall*100:4.1f}%")
    print(f"  Diff: {(ctm_overall-mlp_overall)*100:4.1f}%")

    print("\nAMBIGUOUS CASES (TRUE BINDING TEST, via answers only):")
    print(f"  MLP: {mlp_amb*100:4.1f}%")
    print(f"  CTM: {ctm_amb*100:4.1f}%")
    print(f"  Diff: {(ctm_amb-mlp_amb)*100:4.1f}%")

    # ---- Binding probes ----
    print("\n" + "=" * 60)
    print("BINDING PROBES (QUERY ATTENTION → OBJECT SLOTS)")
    print("=" * 60)

    bind_overall, bind_simple, bind_amb, mass_mean = evaluate_binding(
        ctm, eval_loader, device, n_obj_slots=4
    )
    print("\nBinding argmax accuracy (query attention to correct object):")
    print(f"  Overall: {bind_overall*100:4.1f}%")
    print(f"  Simple:  {bind_simple*100:4.1f}%")
    print(f"  Ambig:   {bind_amb*100:4.1f}%")
    print(f"\nMean attention mass on correct candidate(s): {mass_mean*100:4.1f}%")

    # ---- Slot probe metrics ----
    print("\nPROBE METRICS (Slots → object attributes):")
    ctm.eval()
    color_correct = color_total = 0
    shape_correct = shape_total = 0
    pos_se_sum = pos_count = 0.0

    with torch.no_grad():
        for x, _, _, _, color_idx, shape_idx, pos_xy in eval_loader:
            x = x.to(device)
            color_idx = color_idx.to(device)
            shape_idx = shape_idx.to(device)
            pos_xy = pos_xy.to(device)
            logits, _, _, _, slots = ctm(x, return_attn=False, return_slots=True)
            if slots is None:
                continue

            color_logits = ctm.probe_color_head(slots)  # [B,4,n_colors]
            shape_logits = ctm.probe_shape_head(slots)  # [B,4,n_shapes]
            pos_pred = ctm.probe_pos_head(slots)        # [B,4,2]

            color_pred = color_logits.argmax(dim=-1)
            shape_pred = shape_logits.argmax(dim=-1)

            color_correct += (color_pred == color_idx).sum().item()
            color_total += color_idx.numel()

            shape_correct += (shape_pred == shape_idx).sum().item()
            shape_total += shape_idx.numel()

            se = (pos_pred - pos_xy) ** 2
            pos_se_sum += se.sum().item()
            pos_count += se.numel()

    color_acc = color_correct / max(1, color_total)
    shape_acc = shape_correct / max(1, shape_total)
    pos_mse = pos_se_sum / max(1, pos_count)

    print(f"  Color accuracy: {color_acc*100:4.1f}%")
    print(f"  Shape accuracy: {shape_acc*100:4.1f}%")
    print(f"  Position MSE:   {pos_mse:.4f}")

    # ---- Held-out pattern generalisation (if configured) ----
    if heldout_patterns is not None:
        print("\n" + "=" * 60)
        print("PATTERN-LEVEL GENERALISATION (HELD-OUT QUERY PATTERNS)")
        print("=" * 60)

        mlp_gen = evaluate_model_with_patterns(mlp, eval_loader, device, heldout_patterns)
        ctm_gen = evaluate_model_with_patterns(ctm, eval_loader, device, heldout_patterns)

        print("\nMLP pattern split:")
        print(
            f"  Train patterns: {mlp_gen['train_acc']*100:4.1f}% "
            f"over {mlp_gen['train_count']} examples"
        )
        print(
            f"  Held-out patterns: {mlp_gen['heldout_acc']*100:4.1f}% "
            f"over {mlp_gen['heldout_count']} examples"
        )

        print("\nCTM pattern split:")
        print(
            f"  Train patterns: {ctm_gen['train_acc']*100:4.1f}% "
            f"over {ctm_gen['train_count']} examples"
        )
        print(
            f"  Held-out patterns: {ctm_gen['heldout_acc']*100:4.1f}% "
            f"over {ctm_gen['heldout_count']} examples"
        )

    print("\n" + "=" * 60)
    print("v1.5 HARD BINDING COMPLETE")
    print("=" * 60)


# ---------------------------------------------------------------------
# SIMPLE GRIDWORLD (optional toy)
# ---------------------------------------------------------------------

class GridWorld:
    """
    Simple 2D grid world with a single agent and a fixed goal.
    - Grid size: cfg.gw_size x cfg.gw_size
    - State: one-hot agent position + one-hot goal position.
    - Actions: 0=up, 1=down, 2=left, 3=right.
    - Reward:
        +1.0 on reaching goal,
        -0.01 per step otherwise.
    - Episode ends when goal reached or max_steps exceeded.
    """

    def __init__(self, size: int = 5):
        self.size = size
        self.n_actions = 4
        self.obs_dim = size * size * 2
        self.reset()

    def reset(self) -> torch.Tensor:
        self.agent_pos = [0, 0]
        self.goal_pos = [self.size - 1, self.size - 1]
        return self._get_obs()

    def _get_obs(self) -> torch.Tensor:
        agent_oh = torch.zeros(self.size * self.size)
        goal_oh = torch.zeros(self.size * self.size)
        agent_idx = self.agent_pos[0] * self.size + self.agent_pos[1]
        goal_idx = self.goal_pos[0] * self.size + self.goal_pos[1]
        agent_oh[agent_idx] = 1.0
        goal_oh[goal_idx] = 1.0
        return torch.cat([agent_oh, goal_oh], dim=0)

    def step(self, action: int) -> Tuple[torch.Tensor, float, bool]:
        # Move agent
        if action == 0:   # up
            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)
        elif action == 1:  # down
            self.agent_pos[0] = min(self.size - 1, self.agent_pos[0] + 1)
        elif action == 2:  # left
            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)
        elif action == 3:  # right
            self.agent_pos[1] = min(self.size - 1, self.agent_pos[1] + 1)

        reward = -0.01
        done = False
        if self.agent_pos == self.goal_pos:
            reward = 1.0
            done = True

        return self._get_obs(), reward, done


# minimal hypergraph + active inference agent, as in previous versions

class NodeType(Enum):
    EXPERIENCE = "experience"
    GOAL = "goal"


class HyperNode:
    def __init__(self, id: str, node_type: NodeType, embedding: torch.Tensor, data: Dict[str, Any], reward: float, created_at: int):
        self.id = id
        self.node_type = node_type
        self.embedding = embedding
        self.data = data
        self.reward = reward
        self.created_at = created_at


class EternalHypergraph:
    def __init__(self, embedding_dim: int, max_nodes: int = 5000):
        self.embedding_dim = embedding_dim
        self.max_nodes = max_nodes
        self.nodes: Dict[str, HyperNode] = {}
        self.experience_ids: List[str] = []
        self.step = 0

    def add_experience(self, z: torch.Tensor, action: int, z_next: torch.Tensor, reward: float) -> str:
        self.step += 1
        node_id = f"exp_{len(self.nodes)}_{self.step}"
        emb = z.detach().cpu().clone()
        node = HyperNode(
            id=node_id,
            node_type=NodeType.EXPERIENCE,
            embedding=emb,
            data={
                "z_next": z_next.detach().cpu().clone(),
                "action": int(action),
                "reward": float(reward),
            },
            reward=float(reward),
            created_at=self.step,
        )
        self.nodes[node_id] = node
        self.experience_ids.append(node_id)
        if len(self.experience_ids) > self.max_nodes:
            old_id = self.experience_ids.pop(0)
            self.nodes.pop(old_id, None)
        return node_id

    def get_action_values(self, z: torch.Tensor, n_actions: int) -> torch.Tensor:
        if not self.experience_ids:
            return torch.zeros(n_actions, device=z.device)
        ids = list(self.experience_ids)
        embs = torch.stack([self.nodes[i].embedding for i in ids], dim=0).to(z.device)
        z_norm = F.normalize(z.unsqueeze(0), dim=-1)
        embs_norm = F.normalize(embs, dim=-1)
        sims = torch.matmul(embs_norm, z_norm.transpose(0, 1)).squeeze(-1)  # [N]
        q_vals = torch.zeros(n_actions, device=z.device)
        counts = torch.zeros(n_actions, device=z.device) + 1e-6
        for sim, node in zip(sims, [self.nodes[i] for i in ids]):
            a = node.data.get("action", 0)
            if 0 <= a < n_actions:
                q_vals[a] += sim * node.data.get("reward", 0.0)
                counts[a] += sim.abs()
        return q_vals / counts


class ActiveInferenceAgent(nn.Module):
    def __init__(self, latent_dim: int, n_actions: int):
        super().__init__()
        self.latent_dim = latent_dim
        self.n_actions = n_actions
        self.transition = nn.Sequential(
            nn.Linear(latent_dim + n_actions, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim),
        )
        self.w_reward = nn.Parameter(torch.tensor(1.0))
        self.w_memory = nn.Parameter(torch.tensor(0.5))

    def forward_transition(self, z: torch.Tensor, action: int) -> torch.Tensor:
        a_onehot = torch.zeros(self.n_actions, device=z.device)
        a_onehot[action] = 1.0
        inp = torch.cat([z, a_onehot], dim=-1)
        return self.transition(inp)

    def select_action(
        self,
        z: torch.Tensor,
        hypergraph: EternalHypergraph,
        temperature: float = 1.0,
        deterministic: bool = False,
    ):
        memory_q = hypergraph.get_action_values(z.detach(), self.n_actions)
        scores = []
        for a in range(self.n_actions):
            score = self.w_reward * 0.0 + self.w_memory * memory_q[a]
            scores.append(score)
        scores_t = torch.stack(scores)
        if deterministic:
            action = int(scores_t.argmax().item())
            probs = torch.zeros_like(scores_t)
            probs[action] = 1.0
        else:
            probs = torch.softmax(scores_t / temperature, dim=-1)
            action = int(torch.multinomial(probs, 1).item())
        return action, {
            "scores": scores_t.detach().cpu(),
            "probs": probs.detach().cpu(),
            "q_memory": memory_q.detach().cpu(),
        }


class USMGridWorldAgent(nn.Module):
    def __init__(self, obs_dim: int, latent_dim: int, n_actions: int):
        super().__init__()
        self.obs_enc = nn.Sequential(
            nn.Linear(obs_dim, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim),
        )
        self.ctm = CTMBlock(dim=latent_dim, n_slots=1, n_ticks=1)
        self.active = ActiveInferenceAgent(latent_dim, n_actions)
        self.q_head = nn.Linear(latent_dim, n_actions)

    def encode_latent(self, obs: torch.Tensor) -> torch.Tensor:
        z0 = self.obs_enc(obs.unsqueeze(0))  # [1,D]
        z_seq = z0.unsqueeze(1)              # [1,1,D]
        z_out, _, _, _ = self.ctm(z_seq, return_attn=False)
        return z_out.squeeze(1).squeeze(0)

    def act(
        self,
        z: torch.Tensor,
        hypergraph: EternalHypergraph,
        temperature: float = 1.0,
        deterministic: bool = False,
    ):
        return self.active.select_action(z, hypergraph, temperature, deterministic)


def run_gridworld_experiment(cfg: USMConfig):
    global device
    device = resolve_device(cfg)
    torch.manual_seed(cfg.seed)
    random.seed(cfg.seed)

    print("=" * 60)
    print("USM v1.5: GRIDWORLD + HYPERGRAPH + ACTIVE INFERENCE (toy)")
    print("=" * 60)
    print("Device:", device)

    env = GridWorld(size=cfg.gw_size)
    agent = USMGridWorldAgent(env.obs_dim, cfg.gw_latent_dim, env.n_actions).to(device)
    hypergraph = EternalHypergraph(embedding_dim=cfg.gw_latent_dim)
    optimizer = torch.optim.Adam(agent.parameters(), lr=cfg.gw_lr)
    gamma = cfg.gw_gamma

    def tensorize(obs: torch.Tensor) -> torch.Tensor:
        return obs.to(device).float()

    reward_history = []
    success_history = []

    for ep in range(1, cfg.gw_n_episodes + 1):
        obs = env.reset()
        total_r = 0.0
        success = False
        for step in range(cfg.gw_max_steps):
            obs_t = tensorize(obs)
            z = agent.encode_latent(obs_t)
            action, info = agent.act(z, hypergraph, temperature=1.0)
            next_obs, reward, done = env.step(action)
            total_r += reward
            if done and reward > 0:
                success = True

            next_obs_t = tensorize(next_obs)
            z_next = agent.encode_latent(next_obs_t)

            hypergraph.add_experience(z, action, z_next, reward)

            q_pred = agent.q_head(z)
            q_val = q_pred[action]
            with torch.no_grad():
                q_next = agent.q_head(z_next)
                target = reward + (0.0 if done else gamma * q_next.max().item())
            loss_td = F.mse_loss(q_val, torch.tensor(target, device=device))

            optimizer.zero_grad()
            loss_td.backward()
            optimizer.step()

            obs = next_obs
            if done:
                break

        reward_history.append(total_r)
        success_history.append(1.0 if success else 0.0)

        if ep % 20 == 0 or ep == cfg.gw_n_episodes:
            window = min(20, len(reward_history))
            avg_r = sum(reward_history[-window:]) / window
            avg_succ = sum(success_history[-window:]) / window
            print(
                f"Episode {ep:3d} | avg_reward(last{window})={avg_r:.3f} "
                f"| success_rate(last{window})={avg_succ*100:4.1f}% "
                f"| mem_size={len(hypergraph.experience_ids)}"
            )

    print("\n" + "=" * 60)
    print("GRIDWORLD TRAINING COMPLETE")
    print("=" * 60)


# ---------------------------------------------------------------------
# ENTRY POINT
# ---------------------------------------------------------------------

def main():
    cfg = USMConfig()
    # you can tweak config here if you want, e.g.:
    # cfg.hb_split_mode = "heldout_pairs"
    # cfg.hb_num_heldout_patterns = 16

    if cfg.world_type == "hard_binding":
        run_hard_binding_experiment(cfg)
    elif cfg.world_type == "gridworld":
        run_gridworld_experiment(cfg)
    else:
        raise ValueError(f"Unknown world_type: {cfg.world_type}")


if __name__ == "__main__":
    main()