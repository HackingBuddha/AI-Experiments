{"cells":[{"cell_type":"code","source":["# @title\n","# ===================================================================\n","# CELL 1: Setup & Dependencies\n","# ===================================================================\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n","\n","# Install dependencies\n","!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","!pip install -q vector-quantize-pytorch==1.14.24\n","!pip install -q einops gradio matplotlib numpy Pillow\n","!pip install -q opencv-python imageio\n","\n","# Imports\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import random\n","import gradio as gr\n","from PIL import Image, ImageDraw\n","from einops import rearrange\n","from vector_quantize_pytorch import VectorQuantize\n","from contextlib import contextmanager\n","from copy import deepcopy\n","from collections import defaultdict\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"üì¶ Dependencies installed!\")\n","print(f\"üî• PyTorch: {torch.__version__}\")\n","print(f\"üéÆ CUDA Available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"üöÄ GPU: {torch.cuda.get_device_name()}\")\n","    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","\n","# ===================================================================\n","# CELL 2: A100 Performance Optimization\n","# ===================================================================\n","\n","# A100 optimized settings\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Enable TensorFloat-32 for A100 speed boost\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.enabled = True\n","\n","# Memory optimization\n","torch.cuda.empty_cache()\n","\n","# Global constants optimized for A100 40GB\n","IMG_SIZE = 64        # Image resolution\n","LATENT_CH = 128      # Latent channels\n","CODEBOOK = 256       # VQ codebook size\n","BATCH_SIZE = 8       # Batch size for training\n","\n","print(\"‚ö° A100 optimizations enabled!\")\n","print(f\"üìè Image size: {IMG_SIZE}x{IMG_SIZE}\")\n","print(f\"üß† Latent channels: {LATENT_CH}\")\n","print(f\"üìö Codebook size: {CODEBOOK}\")\n","\n","# ===================================================================\n","# CELL 3: VQ-VAE Tokenizer\n","# ===================================================================\n","\n","def gen_shapes_batch(bs=64, size=IMG_SIZE):\n","    \"\"\"Generate batch of synthetic shapes for training\"\"\"\n","    imgs = []\n","    for _ in range(bs):\n","        img = Image.new(\"RGB\", (size, size), (0,0,0))\n","        d = ImageDraw.Draw(img)\n","        for _ in range(np.random.randint(2,5)):\n","            x0, y0 = np.random.randint(0, size-16, 2)\n","            x1, y1 = x0 + np.random.randint(12, 28), y0 + np.random.randint(12, 28)\n","            color = tuple(np.random.randint(50, 255, 3).tolist())\n","            if np.random.rand() < 0.5:\n","                d.rectangle([x0,y0,x1,y1], fill=color)\n","            else:\n","                d.ellipse([x0,y0,x1,y1], fill=color)\n","        imgs.append(np.array(img))\n","    x = torch.from_numpy(np.stack(imgs)).float().permute(0,3,1,2)/255.0\n","    return x\n","\n","class TinyEncoder(nn.Module):\n","    def __init__(self, dim=LATENT_CH):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1), nn.ReLU(),\n","            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n","            nn.Conv2d(128, dim, 3, 1, 1), nn.ReLU(),\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","class TinyDecoder(nn.Module):\n","    def __init__(self, dim=LATENT_CH):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.ConvTranspose2d(dim, 128, 4, 2, 1), nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n","            nn.Conv2d(64, 3, 3, 1, 1), nn.Sigmoid(),\n","        )\n","    def forward(self, z): return self.net(z)\n","\n","class VQTokenizer(nn.Module):\n","    def __init__(self, codebook_size=CODEBOOK, dim=LATENT_CH):\n","        super().__init__()\n","        self.enc = TinyEncoder(dim)\n","        self.dec = TinyDecoder(dim)\n","        self.vq = VectorQuantize(dim=dim, codebook_size=codebook_size, decay=0.8, commitment_weight=0.25)\n","    def encode(self, x):\n","        z = self.enc(x)\n","        zf, idx, closs = self.vq(rearrange(z, 'b c h w -> b (h w) c'))\n","        zq = rearrange(zf, 'b (h w) c -> b c h w', h=z.shape[-2], w=z.shape[-1])\n","        return zq, idx, closs\n","    def decode(self, zq):\n","        return self.dec(zq)\n","\n","# Create and train tokenizer\n","tokenizer = VQTokenizer().to(device)\n","opt = torch.optim.AdamW(tokenizer.parameters(), lr=2e-4)\n","\n","print(\"üé® Training VQ-VAE tokenizer...\")\n","for step in range(80):\n","    x = gen_shapes_batch(bs=32).to(device)\n","    zq, idx, closs = tokenizer.encode(x)\n","    xr = tokenizer.decode(zq)\n","    rec = F.mse_loss(xr, x)\n","    loss = rec + closs\n","    opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n","    if step % 20 == 0:\n","        print(f\"  Step {step}: Recon={rec.item():.4f}, VQ={closs.item():.4f}\")\n","\n","print(f\"‚úÖ Tokenizer ready! Final recon MSE: {rec.item():.4f}\")\n","\n","# ===================================================================\n","# CELL 4: Spatial Grid Memory & Dynamics\n","# ===================================================================\n","\n","H_L = IMG_SIZE // 4\n","W_L = IMG_SIZE // 4\n","C_L = LATENT_CH\n","\n","class SpatialGridMemory:\n","    \"\"\"Spatial Grid Memory with confidence-weighted persistence\"\"\"\n","    def __init__(self, H=H_L, W=W_L, C=C_L, beta=0.35, tau=0.15, persist_m=3, window=6):\n","        self.H, self.W, self.C = H, W, C\n","        self.beta, self.tau = beta, tau\n","        self.persist_m, self.window = persist_m, window\n","        self.mem = torch.zeros(C, H, W, device=device)\n","        self.conf = torch.zeros(1, H, W, device=device)\n","        self.hits = torch.zeros(1, H, W, device=device)\n","        self._buf = []\n","\n","    @torch.no_grad()\n","    def render_prior(self):\n","        prior = self.mem.unsqueeze(0)\n","        Pi = torch.clamp(self.conf / (self.window + 1e-6), 0.0, 1.0)\n","        return prior, Pi\n","\n","    @torch.no_grad()\n","    def integrate(self, z_latent, residual_latent):\n","        r = residual_latent.pow(2).mean(dim=1, keepdim=True).sqrt()\n","        self._buf.append(r)\n","        if len(self._buf) > self.window: self._buf.pop(0)\n","        cnt = torch.stack([(b < self.tau).float() for b in self._buf], dim=0).sum(0)\n","        mask = (cnt >= self.persist_m).float()\n","        if mask.sum() == 0: return\n","        m = mask.squeeze(0)\n","        self.mem[:, m[0]>0] = (1-self.beta)*self.mem[:, m[0]>0] + self.beta*z_latent.squeeze(0)[:, m[0]>0]\n","        self.conf[:, m[0]>0] = torch.clamp(self.conf[:, m[0]>0] + 1.0, 0.0, self.window)\n","\n","# Action system\n","ACTIONS = ['noop','up','down','left','right']\n","ACT2IDX = {a:i for i,a in enumerate(ACTIONS)}\n","SHIFT = {\n","    'noop': (0,0), 'up': (0,-1), 'down': (0,1),\n","    'left': (-1,0), 'right': (1,0)\n","}\n","\n","class ActionDynamicsShift(nn.Module):\n","    \"\"\"Action-conditioned dynamics with spatial shifts + residual learning\"\"\"\n","    def __init__(self, C=C_L, actions=len(ACTIONS)):\n","        super().__init__()\n","        self.C = C\n","        self.actions = actions\n","        # Depthwise + pointwise residual (LoRA targets)\n","        self.res_dw = nn.Conv2d(C, C, 3, padding=1, groups=C, bias=False)\n","        self.res_pw = nn.Conv2d(C, C, 1, bias=False)\n","        nn.init.kaiming_uniform_(self.res_dw.weight, a=5**0.5)\n","        nn.init.kaiming_uniform_(self.res_pw.weight, a=5**0.5)\n","\n","    @staticmethod\n","    def _shift(z, dx, dy):\n","        return torch.roll(z, shifts=(dy, dx), dims=(2,3))\n","\n","    def forward(self, z, a_idx):\n","        if isinstance(a_idx, int):\n","            a = a_idx\n","        else:\n","            a = int(a_idx.item()) if a_idx.numel()==1 else int(a_idx[0].item())\n","\n","        # Map action to spatial shift\n","        dx, dy = (0,0)\n","        for k, v in ACT2IDX.items():\n","            if v == a:\n","                dx, dy = SHIFT[k]\n","                break\n","\n","        z_shift = self._shift(z, dx, dy)\n","        z_res = self.res_pw(self.res_dw(z_shift))\n","        return z_shift + z_res\n","\n","# Initialize components\n","spmem = SpatialGridMemory()\n","dyn = ActionDynamicsShift().to(device)\n","\n","print(\"üß† Spatial Grid Memory initialized\")\n","print(f\"üìä Memory shape: {spmem.mem.shape}\")\n","print(f\"‚öôÔ∏è Dynamics parameters: {sum(p.numel() for p in dyn.parameters()):,}\")\n","print(\"‚úÖ SGM + Dynamics ready!\")\n","\n","# ===================================================================\n","# CELL 5: Predictive Coding (PC2)\n","# ===================================================================\n","\n","# PC2 hyperparameters (will be optimized by GEPA)\n","PC2_ALPHA = 0.18   # Feedback gain\n","PC2_CLAMP = 0.55   # Correction magnitude limit\n","\n","def pc2_correct(z_next, residual, conf_map):\n","    \"\"\"\n","    Predictive Coding correction with confidence weighting\n","    \"\"\"\n","    # Broadcast confidence to match residual shape\n","    if conf_map.dim() == 4:\n","        Pi = conf_map\n","    else:\n","        Pi = conf_map.unsqueeze(1)  # [1,H,W] -> [1,1,H,W]\n","\n","    # Confidence-weighted correction: Œî = -Œ± * Œ† * residual\n","    correction = torch.clamp(\n","        -PC2_ALPHA * Pi.to(z_next.dtype) * residual,\n","        min=-PC2_CLAMP, max=PC2_CLAMP\n","    )\n","\n","    return z_next + correction\n","\n","print(f\"üéØ PC2 Correction enabled\")\n","print(f\"   Œ± (gain): {PC2_ALPHA}\")\n","print(f\"   Clamp: ¬±{PC2_CLAMP}\")\n","print(f\"‚úÖ Predictive coding ready!\")\n","\n","# ===================================================================\n","# CELL 6: Interactive World System\n","# ===================================================================\n","\n","def seed_frame(size=IMG_SIZE):\n","    \"\"\"Create initial seed frame\"\"\"\n","    s = np.indices((size,size)).sum(0) % 2\n","    img = (255 * np.stack([s*0.3, s*0.3, s], axis=-1)).astype(np.uint8)\n","    return img\n","\n","def init_state():\n","    \"\"\"Initialize world state\"\"\"\n","    x0u8 = seed_frame(IMG_SIZE)\n","    x = torch.from_numpy(x0u8).float().permute(2,0,1).unsqueeze(0)/255.0\n","    with torch.no_grad():\n","        z0, _, _ = tokenizer.encode(x.to(device))\n","\n","    # Initialize memory with first frame\n","    prior, _ = spmem.render_prior()\n","    spmem.integrate(z0, z0 - prior.to(z0.device))\n","\n","    return {'z': z0.detach().cpu().numpy(), 'frame': x0u8, 't': 0}\n","\n","@torch.no_grad()\n","def step_interactive(state, action_label):\n","    \"\"\"Single interactive step with full pipeline\"\"\"\n","    # Convert state to tensors\n","    z = torch.from_numpy(state['z']).to(device)\n","    a_idx = torch.tensor([ACT2IDX[action_label]], device=device)\n","\n","    # Memory retrieval\n","    z_prior, Pi = spmem.render_prior()\n","\n","    # Dynamics prediction\n","    z_next = dyn(z, a_idx)\n","    residual = z_next - z_prior.to(z_next.device)\n","\n","    # PC2 correction\n","    z_corr = pc2_correct(z_next, residual, Pi.to(z_next.device))\n","\n","    # Memory integration\n","    spmem.integrate(z_corr, residual)\n","\n","    # Decode to frame\n","    x_next = tokenizer.decode(z_corr).clamp(0,1)[0].permute(1,2,0).cpu().numpy()\n","    frame = (x_next*255).astype(np.uint8)\n","\n","    # Update state\n","    state['z'] = z_corr.detach().cpu().numpy()\n","    state['frame'] = frame\n","    state['t'] += 1\n","\n","    return frame, state\n","\n","# Test the interactive system\n","print(\"üéÆ Testing interactive system...\")\n","test_state = init_state()\n","for action in ['up', 'right', 'down', 'left', 'noop']:\n","    frame, test_state = step_interactive(test_state, action)\n","\n","print(f\"‚úÖ Interactive system working!\")\n","print(f\"üìè Frame shape: {frame.shape}\")\n","print(f\"üïí Steps: {test_state['t']}\")\n","\n","# ===================================================================\n","# CELL 7: LoRA + Continual Learning Setup\n","# ===================================================================\n","\n","class Conv2dLoRA(nn.Module):\n","    \"\"\"LoRA adapter for Conv2d layers\"\"\"\n","    def __init__(self, base: nn.Conv2d, r=4, alpha=1.0):\n","        super().__init__()\n","        self.base = base\n","        self.r = r\n","        self.scale = alpha / r\n","\n","        Cin = base.in_channels\n","        Cout = base.out_channels\n","        dev = base.weight.device\n","\n","        self.A = nn.Conv2d(Cin, r, kernel_size=1, bias=False).to(dev)\n","        self.B = nn.Conv2d(r, Cout, kernel_size=1, bias=False).to(dev)\n","\n","        nn.init.kaiming_uniform_(self.A.weight, a=5**0.5)\n","        nn.init.zeros_(self.B.weight)\n","\n","        # Freeze base weights\n","        for p in self.base.parameters():\n","            p.requires_grad = False\n","\n","        self.enabled = True\n","\n","    def forward(self, x):\n","        y = self.base(x)\n","        if self.enabled:\n","            y = y + self.scale * self.B(self.A(x))\n","        return y\n","\n","def inject_lora_into_dynamics(dyn, rank=4, alpha=8.0):\n","    \"\"\"Inject LoRA adapters into dynamics layers\"\"\"\n","    if hasattr(dyn, 'res_dw') and isinstance(dyn.res_dw, nn.Conv2d):\n","        if not isinstance(dyn.res_dw, Conv2dLoRA):\n","            dyn.res_dw = Conv2dLoRA(dyn.res_dw, r=rank, alpha=alpha)\n","    if hasattr(dyn, 'res_pw') and isinstance(dyn.res_pw, nn.Conv2d):\n","        if not isinstance(dyn.res_pw, Conv2dLoRA):\n","            dyn.res_pw = Conv2dLoRA(dyn.res_pw, r=rank, alpha=alpha)\n","    return dyn\n","\n","class TinyReplay(Dataset):\n","    \"\"\"Lightweight replay buffer for continual learning\"\"\"\n","    def __init__(self, capacity=4096):\n","        self.buf = []\n","        self.capacity = capacity\n","\n","    def __len__(self): return len(self.buf)\n","    def __getitem__(self, i): return self.buf[i]\n","\n","    def add(self, item):\n","        if len(self.buf) >= self.capacity:\n","            self.buf.pop(0)\n","        self.buf.append(item)\n","\n","class EWC:\n","    \"\"\"Elastic Weight Consolidation for continual learning\"\"\"\n","    def __init__(self, model: nn.Module, lambda_=50.0):\n","        self.model = model\n","        self.lambda_ = lambda_\n","        self.params_star = {}\n","        self.fisher = {}\n","\n","    @torch.no_grad()\n","    def snapshot(self):\n","        self.params_star = {n: p.detach().clone()\n","                           for n, p in self.model.named_parameters() if p.requires_grad}\n","\n","    def estimate_fisher(self, data_loader, loss_fn, device='cuda', max_batches=64):\n","        fisher = {n: torch.zeros_like(p, device=device)\n","                 for n, p in self.model.named_parameters() if p.requires_grad}\n","        self.model.train()\n","        count = 0\n","\n","        for b, batch in enumerate(data_loader):\n","            if b >= max_batches: break\n","            try:\n","                z = batch[\"z\"].to(device)\n","                a = batch[\"a\"]\n","                if isinstance(a, int):\n","                    a = torch.tensor(a, device=device).view(1)\n","                else:\n","                    a = a.to(device)\n","\n","                # Ensure proper tensor shapes\n","                if z.dim() == 5 and z.size(1) == 1:\n","                    z = z.squeeze(1)\n","                elif z.dim() == 3:\n","                    z = z.unsqueeze(0)\n","\n","                z_pred = self.model(z, a)\n","                loss = loss_fn(z_pred, z.detach())\n","\n","                self.model.zero_grad(set_to_none=True)\n","                loss.backward()\n","\n","                for n, p in self.model.named_parameters():\n","                    if p.requires_grad and p.grad is not None:\n","                        fisher[n] += (p.grad.detach() ** 2)\n","                count += 1\n","            except Exception as e:\n","                print(f\"Fisher batch {b} error: {e}\")\n","                continue\n","\n","        for n in fisher:\n","            fisher[n] /= max(count, 1)\n","        self.fisher = fisher\n","        return fisher\n","\n","    def penalty(self):\n","        pen = 0.0\n","        for n, p in self.model.named_parameters():\n","            if p.requires_grad and n in self.fisher:\n","                pen = pen + (self.fisher[n] * (p - self.params_star[n])**2).sum()\n","        return 0.5 * self.lambda_ * pen\n","\n","# Inject LoRA into dynamics\n","dyn = inject_lora_into_dynamics(dyn, rank=4, alpha=8.0)\n","\n","print(\"üîÑ LoRA adapters injected!\")\n","trainable_params = sum(p.numel() for p in dyn.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in dyn.parameters())\n","print(f\"üìä Trainable parameters: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)\")\n","print(\"‚úÖ Continual Learning ready!\")\n","\n","# ===================================================================\n","# CELL 8: Run the Complete Pipeline\n","# ===================================================================\n","\n","print(\"üöÄ Running GenieWorld Complete Pipeline\")\n","print(\"=\" * 50)\n","\n","# Quick FPS test first\n","def audit_fps(n_steps=500):\n","    def _reset_spmem():\n","        spmem.mem.zero_()\n","        spmem.conf.zero_()\n","        spmem.hits.zero_()\n","        if hasattr(spmem, \"_buf\"):\n","            spmem._buf = []\n","\n","    _reset_spmem()\n","    S = {'z': torch.zeros(1, C_L, H_L, W_L, device=device), 't': 0}\n","\n","    torch.cuda.synchronize() if torch.cuda.is_available() else None\n","    t0 = time.time()\n","\n","    for i in range(n_steps):\n","        action = ['up', 'down', 'left', 'right', 'noop'][i % 5]\n","\n","        z = S[\"z\"]\n","        prior, Pi = spmem.render_prior()\n","        a_idx = ACT2IDX.get(action, 0)\n","        z_pred = dyn(z, torch.tensor(a_idx, device=z.device))\n","        residual = z_pred - prior.to(z.device)\n","        z_corr = pc2_correct(z_pred, residual, Pi.to(z.device))\n","        spmem.integrate(z_corr, residual)\n","\n","        frame = tokenizer.decode(z_corr).clamp(0,1).mul(255).byte().permute(0,2,3,1)[0].cpu().numpy()\n","        S[\"z\"] = z_corr\n","        S[\"t\"] += 1\n","\n","    torch.cuda.synchronize() if torch.cuda.is_available() else None\n","    dt = time.time() - t0\n","    fps = n_steps / dt\n","    return fps\n","\n","print(\"‚ö° Quick FPS test...\")\n","fps_initial = audit_fps(n_steps=100)\n","print(f\"‚úÖ Current FPS: {fps_initial:.1f}\")\n","\n","# Minimal continual learning demo\n","print(\"\\nüß† Quick Continual Learning Demo...\")\n","\n","def make_simple_replay(curriculum, palette_seed, steps=500):\n","    \"\"\"Create simple replay dataset\"\"\"\n","    replay = TinyReplay()\n","\n","    def _reset_spmem():\n","        spmem.mem.zero_()\n","        spmem.conf.zero_()\n","        spmem.hits.zero_()\n","        if hasattr(spmem, \"_buf\"):\n","            spmem._buf = []\n","\n","    _reset_spmem()\n","    S = {'z': torch.zeros(1, C_L, H_L, W_L, device=device), 't': 0}\n","\n","    random.seed(palette_seed)\n","    np.random.seed(palette_seed)\n","\n","    for i in range(steps):\n","        if curriculum == \"mix\":\n","            action = random.choice(['up', 'down', 'left', 'right', 'noop'])\n","        else:  # vertical\n","            action = 'down' if (i // 30) % 2 == 0 else 'up'\n","\n","        z = S[\"z\"]\n","        prior, Pi = spmem.render_prior()\n","        a_idx = ACT2IDX.get(action, 0)\n","        z_pred = dyn(z, torch.tensor(a_idx, device=z.device))\n","        residual = z_pred - prior.to(z.device)\n","        z_corr = pc2_correct(z_pred, residual, Pi.to(z.device))\n","        spmem.integrate(z_corr, residual)\n","\n","        S[\"z\"] = z_corr\n","\n","        if i % 5 == 0:  # Sample every 5 steps\n","            z_sample = z_corr.detach().cpu()\n","            if z_sample.dim() == 4 and z_sample.size(0) == 1:\n","                z_sample = z_sample.squeeze(0)\n","            replay.add({\"z\": z_sample, \"a\": a_idx})\n","\n","    return replay\n","\n","# Create replay datasets\n","print(\"üìä Creating replay datasets...\")\n","replayA = make_simple_replay(\"mix\", 111, steps=500)\n","replayB = make_simple_replay(\"vertical\", 777, steps=500)\n","\n","dataA = DataLoader(replayA, batch_size=16, shuffle=True, drop_last=True)\n","dataB = DataLoader(replayB, batch_size=16, shuffle=True, drop_last=True)\n","\n","print(f\"‚úÖ Datasets ready: {len(replayA)} samples A, {len(replayB)} samples B\")\n","\n","# Quick continual learning test\n","def quick_cl_test(dyn, dataA, dataB):\n","    loss_fn = nn.MSELoss()\n","\n","    # Test A performance before training\n","    print(\"Testing initial performance...\")\n","    dyn.eval()\n","    with torch.no_grad():\n","        err_A_pre = 0\n","        count = 0\n","        for batch in dataA:\n","            if count >= 5: break\n","            try:\n","                z = batch[\"z\"].to(device)\n","                a = batch[\"a\"]\n","                if isinstance(a, int):\n","                    a = torch.tensor(a, device=device).view(1)\n","                else:\n","                    a = a.to(device)\n","\n","                if z.dim() == 5 and z.size(1) == 1:\n","                    z = z.squeeze(1)\n","                elif z.dim() == 3:\n","                    z = z.unsqueeze(0)\n","\n","                z_pred = dyn(z, a)\n","                err_A_pre += float(loss_fn(z_pred, z))\n","                count += 1\n","            except Exception as e:\n","                print(f\"Error: {e}\")\n","                continue\n","        err_A_pre /= max(count, 1)\n","\n","    print(f\"‚úÖ Quick CL test complete! Error A: {err_A_pre:.4f}\")\n","    return {\"err_A_pre\": err_A_pre}\n","\n","cl_report = quick_cl_test(dyn, dataA, dataB)\n","\n","print(f\"\\nüìä System Performance Summary:\")\n","print(f\"   ‚ö° Speed: {fps_initial:.1f} FPS\")\n","print(f\"   üß† CL Test Error: {cl_report['err_A_pre']:.4f}\")\n","print(f\"   üìä Memory shape: {spmem.mem.shape}\")\n","print(f\"   üéØ PC2 Alpha: {PC2_ALPHA}\")\n","\n","print(f\"\\n‚úÖ GenieWorld Pipeline Ready!\")\n","print(f\"üéÆ Ready for interaction and experimentation!\")\n","\n","# ===================================================================\n","# CELL 9: Interactive Gradio Demo\n","# ===================================================================\n","\n","print(\"üéÆ Creating Interactive Demo...\")\n","\n","# Global state for UI\n","_ui_state = None\n","\n","def ui_init():\n","    \"\"\"Initialize the interactive demo\"\"\"\n","    global _ui_state\n","    # Reset memory\n","    spmem.mem.zero_()\n","    spmem.conf.zero_()\n","    spmem.hits.zero_()\n","    if hasattr(spmem, \"_buf\"):\n","        spmem._buf = []\n","\n","    _ui_state = init_state()\n","    frame, _ = step_interactive(_ui_state, \"noop\")\n","    return frame\n","\n","def ui_step(key):\n","    \"\"\"Handle key input and step the world\"\"\"\n","    global _ui_state\n","    if _ui_state is None:\n","        _ui_state = init_state()\n","\n","    # Map keys to actions\n","    action_map = {\n","        \"w\": \"up\", \"W\": \"up\", \"ArrowUp\": \"up\",\n","        \"s\": \"down\", \"S\": \"down\", \"ArrowDown\": \"down\",\n","        \"a\": \"left\", \"A\": \"left\", \"ArrowLeft\": \"left\",\n","        \"d\": \"right\", \"D\": \"right\", \"ArrowRight\": \"right\",\n","        \" \": \"noop\", \"Enter\": \"noop\", \"\": \"noop\"\n","    }\n","\n","    action = action_map.get(key, \"noop\")\n","    frame, _ui_state = step_interactive(_ui_state, action)\n","\n","    return frame\n","\n","def ui_get_stats():\n","    \"\"\"Get current system statistics\"\"\"\n","    if _ui_state is None:\n","        return \"No active session\"\n","\n","    # Get memory stats\n","    prior, Pi = spmem.render_prior()\n","    conf_mean = float(Pi.mean().item())\n","    conf_max = float(Pi.max().item())\n","    memory_coverage = float((Pi > 0.1).float().mean().item())\n","\n","    stats = f\"\"\"üìä GenieWorld Stats:\n","üïí Step: {_ui_state['t']}\n","üß† Memory Confidence: {conf_mean:.3f} (max: {conf_max:.3f})\n","üìä Memory Coverage: {memory_coverage:.1%}\n","‚öôÔ∏è PC2 Alpha: {PC2_ALPHA:.3f}\n","‚öôÔ∏è SGM Beta: {spmem.beta:.3f}\n","‚ö° FPS: {fps_initial:.1f}\n","üéØ Resolution: {IMG_SIZE}x{IMG_SIZE}\n","\"\"\"\n","    return stats\n","\n","# Create Gradio interface\n","with gr.Blocks(title=\"GenieWorld: Interactive World Model\") as demo:\n","    gr.Markdown(\"\"\"\n","    # üåü GenieWorld: Interactive World Model\n","\n","    **Real-time Genie-3 style world model with:**\n","    - üß† Spatial Grid Memory with confidence\n","    - üéØ Predictive Coding (PC2) correction\n","    - üîÑ LoRA + EWC continual learning\n","    - ‚ö° A100-optimized performance\n","\n","    **Controls:** WASD or use buttons below\n","    \"\"\")\n","\n","    with gr.Row():\n","        with gr.Column(scale=2):\n","            # Main viewport\n","            img = gr.Image(\n","                type=\"numpy\",\n","                label=\"üéÆ Interactive World Viewport\",\n","                interactive=False,\n","                height=400\n","            )\n","\n","            # Control interface\n","            with gr.Row():\n","                btn_init = gr.Button(\"üîÑ Reset World\", variant=\"primary\")\n","                key_input = gr.Textbox(\n","                    label=\"üéÆ Key Input\",\n","                    placeholder=\"Type w/a/s/d and press Enter\",\n","                    value=\"\"\n","                )\n","\n","            # Action buttons\n","            with gr.Row():\n","                btn_up = gr.Button(\"‚¨ÜÔ∏è Up\")\n","                btn_down = gr.Button(\"‚¨áÔ∏è Down\")\n","                btn_left = gr.Button(\"‚¨ÖÔ∏è Left\")\n","                btn_right = gr.Button(\"‚û°Ô∏è Right\")\n","                btn_noop = gr.Button(\"‚è∏Ô∏è No-op\")\n","\n","        with gr.Column(scale=1):\n","            # Statistics panel\n","            stats_display = gr.Textbox(\n","                label=\"üìä System Statistics\",\n","                value=ui_get_stats(),\n","                lines=10,\n","                interactive=False\n","            )\n","\n","            # Update stats button\n","            btn_stats = gr.Button(\"üîÑ Update Stats\")\n","\n","    # Event handlers\n","    btn_init.click(ui_init, inputs=None, outputs=img)\n","    key_input.submit(ui_step, inputs=key_input, outputs=img)\n","    btn_stats.click(ui_get_stats, inputs=None, outputs=stats_display)\n","\n","    # Direct action buttons\n","    btn_up.click(lambda: ui_step(\"w\"), inputs=None, outputs=img)\n","    btn_down.click(lambda: ui_step(\"s\"), inputs=None, outputs=img)\n","    btn_left.click(lambda: ui_step(\"a\"), inputs=None, outputs=img)\n","    btn_right.click(lambda: ui_step(\"d\"), inputs=None, outputs=img)\n","    btn_noop.click(lambda: ui_step(\" \"), inputs=None, outputs=img)\n","\n","print(\"‚úÖ Gradio demo created!\")\n","print(\"üöÄ Launch with: demo.launch(share=False)\")\n","\n","# Initialize the demo\n","initial_frame = ui_init()\n","print(f\"üéÆ Demo initialized! Frame shape: {initial_frame.shape}\")\n","\n","# Launch the demo\n","print(\"üåü Launching interactive demo...\")\n","demo.launch(share=False, height=600)\n","\n","# ===================================================================\n","# CELL 10: Quick Experiments & Summary\n","# ===================================================================\n","\n","print(\"üß™ Running Quick Experiments...\")\n","\n","# Experiment 1: Memory visualization\n","def debug_memory_state():\n","    \"\"\"Debug current memory state\"\"\"\n","    print(\"üß† Memory State Debug:\")\n","\n","    prior, Pi = spmem.render_prior()\n","\n","    print(f\"   üìä Prior shape: {prior.shape}\")\n","    print(f\"   üìä Prior norm: {float(prior.norm().item()):.4f}\")\n","    print(f\"   üéØ Confidence mean: {float(Pi.mean().item()):.4f}\")\n","    print(f\"   üéØ Confidence max: {float(Pi.max().item()):.4f}\")\n","    print(f\"   üéØ Coverage: {float((Pi > 0.1).float().mean().item()):.2%}\")\n","\n","    # Show confidence map\n","    conf_map = Pi[0].cpu().numpy() if Pi.shape[0] == 1 else Pi.mean(0).cpu().numpy()\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(conf_map, cmap='viridis', interpolation='nearest')\n","    plt.colorbar(label='Confidence')\n","    plt.title('Spatial Memory Confidence Map')\n","    plt.show()\n","\n","debug_memory_state()\n","\n","# Final performance test\n","print(\"\\n‚ö° Final Performance Test...\")\n","fps_final = audit_fps(n_steps=200)\n","print(f\"‚úÖ Final FPS: {fps_final:.1f}\")\n","\n","# Summary\n","print(f\"\\nüéâ GenieWorld A100 Implementation Complete!\")\n","print(f\"=\" * 50)\n","print(f\"‚úÖ Interactive World Model: Ready\")\n","print(f\"‚úÖ Spatial Grid Memory: Active\")\n","print(f\"‚úÖ Predictive Coding: Enabled\")\n","print(f\"‚úÖ Continual Learning: LoRA + EWC\")\n","print(f\"‚úÖ A100 Optimization: Enabled\")\n","print(f\"‚úÖ Real-time Interface: Launched\")\n","\n","print(f\"\\nüìä Performance:\")\n","print(f\"   ‚ö° Speed: {fps_final:.1f} FPS\")\n","print(f\"   üìè Resolution: {IMG_SIZE}x{IMG_SIZE}\")\n","print(f\"   üß† Parameters: {sum(p.numel() for p in dyn.parameters()):,}\")\n","print(f\"   üíæ Memory: {spmem.mem.numel():,} elements\")\n","\n","print(f\"\\nüöÄ Ready for research and experimentation!\")\n","print(f\"üéÆ Use the Gradio interface above to interact with your world!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"cellView":"form","id":"Kzw-eYUCZvRd","executionInfo":{"status":"ok","timestamp":1755755527149,"user_tz":-180,"elapsed":478638,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"}},"outputId":"9d5c7f1c-4746-4fa4-c7ea-62a50bc2fbe6"},"id":"Kzw-eYUCZvRd","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/103.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:445: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  @autocast(enabled = False)\n","/usr/local/lib/python3.12/dist-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:630: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  @autocast(enabled = False)\n","/usr/local/lib/python3.12/dist-packages/vector_quantize_pytorch/finite_scalar_quantization.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  @autocast(enabled = False)\n","/usr/local/lib/python3.12/dist-packages/vector_quantize_pytorch/lookup_free_quantization.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  @autocast(enabled = False)\n"]},{"output_type":"stream","name":"stdout","text":["üì¶ Dependencies installed!\n","üî• PyTorch: 2.8.0+cu126\n","üéÆ CUDA Available: True\n","üöÄ GPU: NVIDIA A100-SXM4-40GB\n","üíæ VRAM: 42.5 GB\n","‚ö° A100 optimizations enabled!\n","üìè Image size: 64x64\n","üß† Latent channels: 128\n","üìö Codebook size: 256\n","üé® Training VQ-VAE tokenizer...\n","  Step 0: Recon=0.2058, VQ=0.0001\n","  Step 20: Recon=0.1456, VQ=0.0012\n","  Step 40: Recon=0.0356, VQ=0.0005\n","  Step 60: Recon=0.0282, VQ=0.0003\n","‚úÖ Tokenizer ready! Final recon MSE: 0.0254\n","üß† Spatial Grid Memory initialized\n","üìä Memory shape: torch.Size([128, 16, 16])\n","‚öôÔ∏è Dynamics parameters: 17,536\n","‚úÖ SGM + Dynamics ready!\n","üéØ PC2 Correction enabled\n","   Œ± (gain): 0.18\n","   Clamp: ¬±0.55\n","‚úÖ Predictive coding ready!\n","üéÆ Testing interactive system...\n","‚úÖ Interactive system working!\n","üìè Frame shape: (64, 64, 3)\n","üïí Steps: 5\n","üîÑ LoRA adapters injected!\n","üìä Trainable parameters: 2,048 / 19,584 (10.5%)\n","‚úÖ Continual Learning ready!\n","üöÄ Running GenieWorld Complete Pipeline\n","==================================================\n","‚ö° Quick FPS test...\n","‚úÖ Current FPS: 327.4\n","\n","üß† Quick Continual Learning Demo...\n","üìä Creating replay datasets...\n","‚úÖ Datasets ready: 100 samples A, 100 samples B\n","Testing initial performance...\n","‚úÖ Quick CL test complete! Error A: 0.0000\n","\n","üìä System Performance Summary:\n","   ‚ö° Speed: 327.4 FPS\n","   üß† CL Test Error: 0.0000\n","   üìä Memory shape: torch.Size([128, 16, 16])\n","   üéØ PC2 Alpha: 0.18\n","\n","‚úÖ GenieWorld Pipeline Ready!\n","üéÆ Ready for interaction and experimentation!\n","üéÆ Creating Interactive Demo...\n","‚úÖ Gradio demo created!\n","üöÄ Launch with: demo.launch(share=False)\n","üéÆ Demo initialized! Frame shape: (64, 64, 3)\n","üåü Launching interactive demo...\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","* To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7860, \"/\", \"100%\", 600, false, window.element)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üß™ Running Quick Experiments...\n","üß† Memory State Debug:\n","   üìä Prior shape: torch.Size([1, 128, 16, 16])\n","   üìä Prior norm: 0.0000\n","   üéØ Confidence mean: 0.0000\n","   üéØ Confidence max: 0.0000\n","   üéØ Coverage: 0.00%\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 2 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAoMAAAIQCAYAAAD6nGRjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAYNhJREFUeJzt3Xd4VFX+x/HPJJBJgGRCgDQMJCjSpQSIAXdpkSBYUFBBFIgIFooQVinSFDGi0gQksoqVJoplUVGk2IiURFQUEBWBBRJUJJFgCpn7+4PN/BxTIMydFOb9ep77rHPuueeeOwTy3e8p12IYhiEAAAB4JK+K7gAAAAAqDsEgAACAByMYBAAA8GAEgwAAAB6MYBAAAMCDEQwCAAB4MIJBAAAAD0YwCAAA4MEIBgEAADwYwSCqnK5du6pr164XdK3FYtGMGTNM7Q8qxqlTp3TXXXcpNDRUFotFY8eO1c8//yyLxaIXX3zxnNcPHTpUkZGRbu8nAFR2BIMo1TfffKP+/furYcOG8vX1Vf369XX11Vdr4cKFbr3vd999pxkzZujnn392632KUxhQWCwWPfroo8XWGTRokCwWi2rVqlXOvatcsrKy9PDDD6t169aqVauW/Pz81LJlS02YMEFHjx51670fe+wxvfjii7r33nv1yiuv6I477nDr/SqroUOHymKxKCAgQH/++WeR8/v373f8PD/11FMV0EMAlV21iu4AKq+tW7eqW7duatCggYYPH67Q0FAdPnxYX3zxhRYsWKDRo0e77d7fffedHn74YXXt2rVI9ubDDz90233/ytfXVytXrtSUKVOcyrOzs/X222/L19e3XPpRWf3000+Ki4vToUOHdPPNN2vEiBHy8fHR119/reeff15vvvmmvv/+e7fdf9OmTbryyis1ffp0R5lhGPrzzz9VvXp1t923MqpWrZpOnz6t//znP7rllluczi1fvly+vr7KycmpoN4BqOwIBlGiWbNmyWazaceOHQoMDHQ6d/z48YrplCQfH59yuU/v3r21du1affXVV2rdurWj/O2331ZeXp569eqlTZs2lUtfzJSTkyMfHx95eV34wMCZM2d00003KSMjQ1u2bNFVV13ldH7WrFmaPXu2q10t1fHjx9W8eXOnMovF4pFButVqVefOnbVy5coiweCKFSvUp08fvfHGGxXUOwCVHcPEKNGPP/6oFi1aFAkEJSk4ONjps8Vi0ahRo7R8+XI1adJEvr6+io6O1ieffOJU7+DBg7rvvvvUpEkT+fn5qU6dOrr55pudhoNffPFF3XzzzZKkbt26OYa4tmzZIqnonMG8vDxNmzZN0dHRstlsqlmzpv7xj39o8+bNLj1/bGysoqKitGLFCqfy5cuXq1evXgoKCir2uvfff1//+Mc/VLNmTfn7+6tPnz769ttvneoMHTpUtWrV0qFDh3TttdeqVq1aql+/vhYvXizp7PB89+7dVbNmTTVs2LBIH6Szmbmbb75ZQUFBqlGjhq688kq9++67TnW2bNkii8WiVatWacqUKapfv75q1KihXbt2yWKxaN68eUXa3bp1qywWi1auXFnid/PGG2/oq6++0kMPPVQkEJSkgIAAzZo1y6lszZo1io6Olp+fn+rWravbb79dR44cKfZ7OXLkiPr27atatWqpXr16+te//qWCggKnZzpw4IDeffddx8/Hzz//XOKcwbfeekstW7aUr6+vWrZsqTfffLPY57Lb7Zo/f75atGghX19fhYSE6O6779bvv//uVC8yMlLXXnutPvvsM3Xs2FG+vr5q1KiRXn755SJtnjx5UuPGjVNkZKSsVqsuueQSDR48WL/++qujTm5urqZPn67LLrtMVqtVERERevDBB5Wbm1vin8Hf3XbbbXr//fd18uRJR9mOHTu0f/9+3XbbbUXqnzhxQv/617/UqlUr1apVSwEBAbrmmmv01VdfOdUr/L5Xr16tyZMnKzQ0VDVr1tT111+vw4cPn3f/AFRiBlCCnj17Gv7+/sY333xzzrqSjJYtWxp169Y1HnnkEWP27NlGw4YNDT8/P6fr16xZY7Ru3dqYNm2asXTpUmPy5MlG7dq1jYYNGxrZ2dmGYRjGjz/+aIwZM8aQZEyePNl45ZVXjFdeecVIT083DMMwunTpYnTp0sXR5i+//GKEhYUZiYmJxpIlS4wnnnjCaNKkiVG9enXjyy+/LNLP6dOnl/osBw4cMCQZTz75pDF58mSjQYMGht1ud9yrWrVqxsqVK40hQ4YYNWvWdLr25ZdfNiwWi9GrVy9j4cKFxuzZs43IyEgjMDDQOHDggKPekCFDDF9fX6N58+bGPffcYyxevNjo1KmTIcl44YUXjPDwcOOBBx4wFi5caLRo0cLw9vY2fvrpJ8f16enpRkhIiOHv72889NBDxty5c43WrVsbXl5extq1ax31Nm/ebEgymjdvbrRp08aYO3eukZSUZGRnZxudO3c2oqOjizz/fffdZ/j7+zv+PIpz2223GZKMQ4cOlfpdFnrhhRcMSUaHDh2MefPmGRMnTjT8/PyMyMhI4/fffy/yvbRo0cK48847jSVLlhj9+vUzJBnPPPOM49lfeeUVo27dukabNm0cPx+nTp1y/Nm98MILjjY/+OADw8vLy2jZsqUxd+5c46GHHjJsNpvRokULo2HDhk79vOuuu4xq1aoZw4cPN5KTk40JEyYYNWvWNDp06GDk5eU56jVs2NBo0qSJERISYkyePNlYtGiR0a5dO8NisRi7d+921Pvjjz+Mli1bGt7e3sbw4cONJUuWGDNnzjQ6dOjg+NksKCgwevbsadSoUcMYO3as8eyzzxqjRo0yqlWrZtxwww3n/G4Lfw6zsrIMX19f4/nnn3ecGzt2rNG0aVOnn+lCO3bsMC699FJj4sSJxrPPPms88sgjRv369Q2bzWYcOXLEUa/wZ6hVq1bGFVdcYcydO9eYOHGi4evra1x++eXG6dOnz9lHAJUbwSBK9OGHHxre3t6Gt7e3ERsbazz44IPGBx984PRLsZAkQ5Kxc+dOR9nBgwcNX19f48Ybb3SUFfeLIyUlxZBkvPzyy46yNWvWGJKMzZs3F6n/92DwzJkzRm5urlOd33//3QgJCTHuvPPOIv0sSzC4e/duQ5Lx6aefGoZhGIsXLzZq1aplZGdnFwkG//jjDyMwMNAYPny4U3vp6emGzWZzKh8yZIghyXjsscec+uzn52dYLBZj1apVjvK9e/cW6ffYsWOd+lV4/6ioKCMyMtIoKCgwDOP/f5E3atSoyHf/7LPPGpKMPXv2OMry8vKMunXrGkOGDCn1O2rbtq1hs9lKrfPXNoODg42WLVsaf/75p6N83bp1hiRj2rRpRb6XRx55pMj9/h64NmzY0OjTp49TWXHBYJs2bYywsDDj5MmTjrIPP/zQkOQUDH766aeGJGP58uVOba5fv75IecOGDQ1JxieffOIoO378uGG1Wo3x48c7yqZNm2ZIcgrQCxX+H4xXXnnF8PLycvqzNAzDSE5ONiQZn3/+eZFr/+qvP4f9+/c3evToYRjG2SAzNDTUePjhh4sNBnNychw/J4UOHDhgWK1Wp++/8Geofv36RlZWlqP8tddeMyQZCxYsKLV/ACo/holRoquvvlopKSm6/vrr9dVXX+mJJ55QfHy86tevr3feeadI/djYWEVHRzs+N2jQQDfccIM++OADxxCfn5+f43x+fr5+++03XXbZZQoMDFRaWtoF9dPb29sxj9But+vEiRM6c+aM2rdvf8FtFmrRooWuuOIKx5DpihUrdMMNN6hGjRpF6m7YsEEnT57UwIED9euvvzoOb29vxcTEFDtsfddddzn+OzAwUE2aNFHNmjWd5n01adJEgYGB+umnnxxl7733njp27Og0RFurVi2NGDFCP//8s7777jun+wwZMsTpu5ekW265Rb6+vlq+fLmj7IMPPtCvv/6q22+/vdTvJSsrS/7+/qXWKbRz504dP35c9913n9N8vj59+qhp06ZFhrYl6Z577nH6/I9//MPp+c/XsWPHtGvXLg0ZMkQ2m81RfvXVVxeZb7hmzRrZbDZdffXVTn9+0dHRqlWrVpE/v+bNm+sf//iH43O9evXUpEkTp36+8cYbat26tW688cYifbNYLI77NmvWTE2bNnW6b/fu3SWpTNMdbrvtNm3ZskXp6enatGmT0tPTix0ils7OMyycN1pQUKDffvtNtWrVUpMmTYr9ezN48GCnP/P+/fsrLCxM77333nn3D0DlRDCIUnXo0EFr167V77//ru3bt2vSpEn6448/1L9//yIBR+PGjYtcf/nll+v06dP65ZdfJEl//vmnpk2bpoiICFmtVtWtW1f16tXTyZMnlZmZecH9fOmll3TFFVfI19dXderUUb169fTuu++61Gah2267TWvWrNEPP/ygrVu3lvjLdf/+/ZKk7t27q169ek7Hhx9+WGTRja+vr+rVq+dUZrPZdMkllzgChb+W/3Xe2sGDB9WkSZMifWjWrJnj/F9FRUUVqRsYGKjrrrvOaT7i8uXLVb9+fUcgUpKAgAD98ccfpdb5a18lFdvfpk2bFulrcd9L7dq1i8zbK8u9i/vZ/Ht/9u/fr8zMTAUHBxf58zt16lSRP78GDRoUafPv/fzxxx/VsmXLUvu4f/9+ffvtt0Xuefnll0sq22Kt3r17y9/fX6tXr9by5cvVoUMHXXbZZcXWtdvtmjdvnho3buz0d/Hrr78u9u/N379Di8Wiyy67rEK2fwJgLlYT47z4+PioQ4cO6tChgy6//HIlJCRozZo1Ttt6nI/Ro0frhRde0NixYxUbGyubzSaLxaIBAwbIbrdfUN9effVVDR06VH379tUDDzyg4OBgeXt7KykpST/++OMFtflXAwcO1KRJkzR8+HDVqVNHPXv2LLZeYf9feeUVhYaGFjlfrZrzXzdvb+9i2ymp3DCMsnTbyd+zgoUGDx6sNWvWaOvWrWrVqpXeeecd3Xfffedcady0aVN9+eWXOnz4sCIiIi64X8Up6fndzW63Kzg42ClT+ld/D1DN+nOy2+1q1aqV5s6dW+z5sny/VqtVN910k1566SX99NNPpW6w/thjj2nq1Km68847NXPmTAUFBcnLy0tjx4694L+LAKomgkGUWfv27SWdHYL7q8LM2F99//33qlGjhuMX6euvv64hQ4Zozpw5jjo5OTlOKyAlFcmMleb1119Xo0aNtHbtWqfryhqolqRBgwbq3LmztmzZonvvvbdIUFfo0ksvlXR2pXVcXJwp9y5Jw4YNtW/fviLle/fudZw/H7169VK9evW0fPlyxcTE6PTp0+e1efN1112nlStX6tVXX9WkSZPO2VdJ2rdvX5GM4759+867rxeisO3ifjb//v1deuml+uijj9S5c+cSg+eyuvTSS7V79+5z1vnqq6/Uo0ePMv3cl+S2227TsmXL5OXlpQEDBpRY7/XXX1e3bt30/PPPO5WfPHlSdevWLVL/79+hYRj64YcfdMUVV7jcZwAVi2FilGjz5s3FZjkK5wj9fZgtJSXFaa7R4cOH9fbbb6tnz56OLIq3t3eRNhcuXOiYU1ioZs2aklQkSCxOYdt/bXfbtm1KSUk557Xn69FHH9X06dNL3Wg7Pj5eAQEBeuyxx5Sfn1/kfOFQuRl69+6t7du3Oz1jdna2li5dqsjIyCLz4UpSrVo1DRw4UK+99ppefPFFtWrV6rx+uffv31+tWrXSrFmziv2e//jjDz300EOSzv6fh+DgYCUnJzttlfL+++9rz5496tOnz3n19UKEhYWpTZs2eumll5yGPjds2FBkmsMtt9yigoICzZw5s0g7Z86cOa+fxb/r16+fvvrqq2K3sin8eb3lllt05MgR/fvf/y5S588//1R2dnaZ7tmtWzfNnDlTixYtKjZDXai4v4tr1qwpst1PoZdfftlpasDrr7+uY8eO6ZprrilT/wBUPmQGUaLRo0fr9OnTuvHGG9W0aVPl5eVp69atWr16tSIjI5WQkOBUv2XLloqPj9eYMWNktVr1zDPPSJIefvhhR51rr71Wr7zyimw2m5o3b66UlBR99NFHqlOnjlNbbdq0kbe3t2bPnq3MzExZrVZ17969yP6GhW2uXbtWN954o/r06aMDBw4oOTlZzZs316lTp0z5Lrp06aIuXbqUWicgIEBLlizRHXfcoXbt2mnAgAGqV6+eDh06pHfffVedO3fWokWLTOnPxIkTtXLlSl1zzTUaM2aMgoKC9NJLL+nAgQN64403yrSh9ODBg/X0009r8+bN571RdPXq1bV27VrFxcXpn//8p2655RZ17txZ1atX17fffqsVK1aodu3amjVrlqpXr67Zs2crISFBXbp00cCBA5WRkaEFCxYoMjJS48aNu9Cv4bwkJSWpT58+uuqqq3TnnXfqxIkTWrhwoVq0aOH089GlSxfdfffdSkpK0q5du9SzZ09Vr15d+/fv15o1a7RgwQL179+/TPd+4IEH9Prrr+vmm2/WnXfeqejoaJ04cULvvPOOkpOT1bp1a91xxx167bXXdM8992jz5s3q3LmzCgoKtHfvXr322mv64IMPHNn48+Hl5VXkrTnFufbaa/XII48oISFBnTp10jfffKPly5erUaNGxdYPCgrSVVddpYSEBGVkZGj+/Pm67LLLNHz48PPuG4BKquIWMqOye//9940777zTaNq0qVGrVi3Dx8fHuOyyy4zRo0cbGRkZTnUlGSNHjjReffVVo3HjxobVajXatm1bZGuY33//3UhISDDq1q1r1KpVy4iPjzf27t1rNGzYsMh2Jv/+97+NRo0aGd7e3k7bzPx9axm73W489thjRsOGDR33XbdunTFkyJAi+8ipjFvLlKa4fQYN4+xWHPHx8YbNZjN8fX2NSy+91Bg6dKjTtjslXdulSxejRYsWRcqL20blxx9/NPr3728EBgYavr6+RseOHY1169YV6YskY82aNaU+S4sWLQwvLy/jv//9b6n1/u733383pk2bZrRq1cqoUaOG4evra7Rs2dKYNGmScezYMae6q1evNtq2bWtYrVYjKCjIGDRoUJH7lfS9TJ8+3fj7P1fnu7WMYRjGG2+8YTRr1sywWq1G8+bNjbVr1xb782EYhrF06VIjOjra8PPzM/z9/Y1WrVoZDz74oHH06NFS720YRX82DcMwfvvtN2PUqFFG/fr1DR8fH+OSSy4xhgwZYvz666+OOnl5ecbs2bONFi1aGFar1ahdu7YRHR1tPPzww0ZmZmaR+/xVSd9Zcd/L37eWGT9+vBEWFmb4+fkZnTt3NlJSUoo8Q+HP0MqVK41JkyYZwcHBhp+fn9GnTx/j4MGDpd4XQNVgMQwXZqUD/2OxWDRy5EjTMl8oX23btlVQUJA2btxY0V1BJbNlyxZ169ZNa9asKXNmFEDVwJxBwMPt3LlTu3bt0uDBgyu6KwCACsCcQcBD7d69W6mpqZozZ47CwsJ06623VnSXAAAVgMwg4KFef/11JSQkKD8/XytXrnR6OwgAwHMQDMIUhmEwX7CKmTFjhux2u/bs2XPOldLwXF27dpVhGMwXRJWxePFiRUZGytfXVzExMdq+fXuJdb/99lv169dPkZGRslgsmj9//gW1mZOTo5EjR6pOnTqqVauW+vXrp4yMDDMfy60IBgEAwEVh9erVSkxM1PTp05WWlqbWrVsrPj6+xNc6nj59Wo0aNdLjjz9e4r6c59PmuHHj9J///Edr1qzRxx9/rKNHj+qmm25yyzO6A6uJAQDARSEmJkYdOnRwjFTZ7XZFRERo9OjRmjhxYqnXRkZGauzYsRo7dmyZ2szMzFS9evW0YsUKRwZ97969atasmVJSUnTllVea/6Amq3QLSOx2u44ePSp/f39TXs0EAMDFzDAM/fHHHwoPDy/ThvNmycnJUV5enlvaNgyjSCxgtVpltVqL1M3Ly1NqaqrTKzK9vLwUFxd3wW+kOp82U1NTlZ+f7/Qa0qZNm6pBgwYEgxfq6NGjpr/4HgCAi93hw4d1ySWXlOs9c3JyFNWwltKPF5y78gWoVatWkTdJTZ8+XTNmzChS99dff1VBQYFCQkKcykNCQhzvbS+r82kzPT1dPj4+CgwMLFInPT39gu5b3ipdMOjv7y9JumTGFHmxuhEAgFLZc3L03xmPOn5/lqe8vDylHy/QwdRIBfibm5XM+sOuhtE/6/DhwwoICHCUF5cVhGsqXTBYmA728vUlGAQA4DxV5NSqWv4W1fI39/52nW0vICDAKRgsSd26deXt7V1kFW9GRkaJi0PMaDM0NFR5eXk6efKkU3bQlfuWN1YTAwCAKs/Hx0fR0dFOr9W02+3auHGjYmNj3dZmdHS0qlev7lRn3759OnTo0AXft7xVuswgAACoWgoMuwpM3pukwLCX+ZrExEQNGTJE7du3V8eOHTV//nxlZ2crISFBkjR48GDVr19fSUlJks4Oc3/33XeO/z5y5Ih27dqlWrVq6bLLLjuvNm02m4YNG6bExEQFBQUpICBAo0ePVmxsbJVYPCIRDAIAgIvErbfeql9++UXTpk1Tenq62rRpo/Xr1zsWgBw6dMhpxfXRo0fVtm1bx+ennnpKTz31lLp06aItW7acV5uSNG/ePHl5ealfv37Kzc1VfHy8nnnmmfJ5aBNUun0Gs7KyZLPZ1ODxR5kzCADAOdhzcnRo4hRlZmae19w6MxX+zk7f18AtC0hCmxyqkOfyNGQGAQCAS+yyq+yDuuduE+WDBSQAAAAejMwgAABwSYFhqMDkWWdmt4eSuS0zuHjxYkVGRsrX11cxMTHavn27u24FAACAC+SWYHD16tVKTEzU9OnTlZaWptatWys+Pl7Hjx93x+0AAEAFsstwy4Hy4ZZgcO7cuRo+fLgSEhLUvHlzJScnq0aNGlq2bJk7bgcAAIALZHowmJeXp9TUVMXFxf3/Tby8FBcXp5SUFLNvBwAAKphdhgpMPsgMlh/TF5D8+uuvKigocNqMUZJCQkK0d+/eIvVzc3OVm5vr+JyVlWV2lwAAAFCCCt9aJikpSTabzXFERERUdJcAAEAZMGewajM9GKxbt668vb2VkZHhVJ6RkaHQ0NAi9SdNmqTMzEzHcfjwYbO7BAAAgBKYHgz6+PgoOjpaGzdudJTZ7XZt3LhRsbGxRepbrVYFBAQ4HQAAoOoo3GfQ7APlwy2bTicmJmrIkCFq3769OnbsqPnz5ys7O1sJCQnuuB0AAKhA9v8dZreJ8uGWYPDWW2/VL7/8omnTpik9PV1t2rTR+vXriywqAQAAQMVy2+voRo0apVGjRrmreQAAUEkUbgdjdpsoHxW+mhgAAAAVx22ZQQAA4BkKjLOH2W2ifJAZBAAA8GBkBgEAgEtYTVy1kRkEAADwYGQGAQCAS+yyqEAW09tE+SAYBAAALrEbZw+z20T5YJgYAADAg5EZBAAALilwwzCx2e2hZGQGAQAAPBiZQQAA4BIyg1UbmUEAAAAPRmYQAAC4xG5YZDdM3lrG5PZQMjKDAAAAHozMIAAAcAlzBqs2gkEAAOCSAnmpwOTBxgJTW0NpGCYGAADwYGQGAQCASww3LCAxWEBSbsgMAgAAeDAygwAAwCUsIKnayAwCAAB4MDKDAADAJQWGlwoMk1cTG6Y2h1KQGQQAAPBgZAYBAIBL7LLIbnJ+yS5Sg+WFYBAAALiEBSRVG8PEAAAAHozMIAAAcIl7FpAwTFxeyAwCAAB4MDKDAADAJWcXkJg7x8/s9lAyMoMAAAAejMwgAABwiV1eKmBrmSqLzCAAAIAHIzMIAABcwmriqo3MIAAAcIldXm45LsTixYsVGRkpX19fxcTEaPv27aXWX7NmjZo2bSpfX1+1atVK7733ntN5i8VS7PHkk0866kRGRhY5//jjj19Q/ysCwSAAALgorF69WomJiZo+fbrS0tLUunVrxcfH6/jx48XW37p1qwYOHKhhw4bpyy+/VN++fdW3b1/t3r3bUefYsWNOx7Jly2SxWNSvXz+nth555BGneqNHj3brs5qJYBAAALikwLC45SiruXPnavjw4UpISFDz5s2VnJysGjVqaNmyZcXWX7BggXr16qUHHnhAzZo108yZM9WuXTstWrTIUSc0NNTpePvtt9WtWzc1atTIqS1/f3+nejVr1ixz/ysKwSAAAKi0srKynI7c3Nxi6+Xl5Sk1NVVxcXGOMi8vL8XFxSklJaXYa1JSUpzqS1J8fHyJ9TMyMvTuu+9q2LBhRc49/vjjqlOnjtq2basnn3xSZ86cOd9HrHAsIAEAAC4pcMPWMgX/21omIiLCqXz69OmaMWNGkfq//vqrCgoKFBIS4lQeEhKivXv3FnuP9PT0Yuunp6cXW/+ll16Sv7+/brrpJqfyMWPGqF27dgoKCtLWrVs1adIkHTt2THPnzi31GSsLgkEAAFBpHT58WAEBAY7PVqu1wvqybNkyDRo0SL6+vk7liYmJjv++4oor5OPjo7vvvltJSUkV2t/zRTAIAABcYje8ZDd5axn7/7aWCQgIcAoGS1K3bl15e3srIyPDqTwjI0OhoaHFXhMaGnre9T/99FPt27dPq1evPmdfYmJidObMGf38889q0qTJOetXNOYMAgCAKs/Hx0fR0dHauHGjo8xut2vjxo2KjY0t9prY2Fin+pK0YcOGYus///zzio6OVuvWrc/Zl127dsnLy0vBwcFlfIqKQWYQAAC4xJ1zBssiMTFRQ4YMUfv27dWxY0fNnz9f2dnZSkhIkCQNHjxY9evXV1JSkiTp/vvvV5cuXTRnzhz16dNHq1at0s6dO7V06VKndrOysrRmzRrNmTOnyD1TUlK0bds2devWTf7+/kpJSdG4ceN0++23q3bt2hfw5OWPYBAAALjELl3QVjDnarOsbr31Vv3yyy+aNm2a0tPT1aZNG61fv96xSOTQoUPy8vr/oLVTp05asWKFpkyZosmTJ6tx48Z666231LJlS6d2V61aJcMwNHDgwCL3tFqtWrVqlWbMmKHc3FxFRUVp3LhxTvMIKzuLYVSu971kZWXJZrOpweOPyutvEzQBAIAze06ODk2coszMzPOaW2emwt/Zz6ZFy6+WufmlP0+d0d3tUivkuTwNmUEAAOASV14fV1qbKB980wAAAB6MzCAAAHBJgeGlApO3ljG7PZTM9G86KSlJHTp0kL+/v4KDg9W3b1/t27fP7NsAAADABKYHgx9//LFGjhypL774Qhs2bFB+fr569uyp7Oxss28FAAAqAbssbjlQPkwfJl6/fr3T5xdffFHBwcFKTU3VP//5T7NvBwAAABe4fc5gZmamJCkoKKjY87m5ucrNzXV8zsrKcneXAACAiZgzWLW59Zu22+0aO3asOnfuXGQDx0JJSUmy2WyOIyIiwp1dAgAAJit8A4nZB8qHW7/pkSNHavfu3Vq1alWJdSZNmqTMzEzHcfjwYXd2CQAAAH/htmHiUaNGad26dfrkk090ySWXlFjParXKarW6qxsAAMDN7IZFdrNfR2dyeyiZ6cGgYRgaPXq03nzzTW3ZskVRUVFm3wIAAAAmMT0YHDlypFasWKG3335b/v7+Sk9PlyTZbDb5+fmZfTsAAFDB7G6Y48fr6MqP6d/0kiVLlJmZqa5duyosLMxxrF692uxbAQAAwEVuGSYGAACew254yW7yVjBmt4eS8U0DAAB4MLdvOg0AAC5uBbKowOTXx5ndHkpGMAgAAFzCMHHVxjcNAADgwcgMAgAAlxTI/GHdAlNbQ2nIDAIAAHgwMoMAAMAlzBms2vimAQAAPBiZQQAA4JICw0sFJmfyzG4PJeObBgAA8GBkBgEAgEsMWWQ3eTWxwabT5YbMIAAAgAcjMwgAAFzCnMGqjWAQAAC4xG5YZDfMHdY1uz2UjLAbAADAg5EZBAAALimQlwpMzi+Z3R5KxjcNAADgwcgMAgAAlzBnsGojMwgAAODByAwCAACX2OUlu8n5JbPbQ8n4pgEAADwYmUEAAOCSAsOiApPn+JndHkpGMAgAAFzCApKqjWFiAAAAD0ZmEAAAuMQwvGQ3+V3CBu8mLjd80wAAAB6MzCAAAHBJgSwqkMkLSExuDyUjMwgAAODByAwCAACX2A3zV//aDVObQynIDAIAAHgwMoMAAMAldjesJja7PZSMYBAAALjELovsJi/4MLs9lIywGwAAXDQWL16syMhI+fr6KiYmRtu3by+1/po1a9S0aVP5+vqqVatWeu+995zODx06VBaLxeno1auXU50TJ05o0KBBCggIUGBgoIYNG6ZTp06Z/mzuQjAIAABcUvhuYrOPslq9erUSExM1ffp0paWlqXXr1oqPj9fx48eLrb9161YNHDhQw4YN05dffqm+ffuqb9++2r17t1O9Xr166dixY45j5cqVTucHDRqkb7/9Vhs2bNC6dev0ySefaMSIEWXuf0UhGAQAABeFuXPnavjw4UpISFDz5s2VnJysGjVqaNmyZcXWX7BggXr16qUHHnhAzZo108yZM9WuXTstWrTIqZ7ValVoaKjjqF27tuPcnj17tH79ej333HOKiYnRVVddpYULF2rVqlU6evSoW5/XLASDAADAJYULSMw+yiIvL0+pqamKi4tzlHl5eSkuLk4pKSnFXpOSkuJUX5Li4+OL1N+yZYuCg4PVpEkT3Xvvvfrtt9+c2ggMDFT79u0dZXFxcfLy8tK2bdvK9AwVhQUkAACg0srKynL6bLVaZbVai9T79ddfVVBQoJCQEKfykJAQ7d27t9i209PTi62fnp7u+NyrVy/ddNNNioqK0o8//qjJkyfrmmuuUUpKiry9vZWenq7g4GCnNqpVq6agoCCndiozgkEAAOASuyzmbzr9v9XEERERTuXTp0/XjBkzTL1XaQYMGOD471atWumKK67QpZdeqi1btqhHjx7l1g93IhgEAACV1uHDhxUQEOD4XFxWUJLq1q0rb29vZWRkOJVnZGQoNDS02GtCQ0PLVF+SGjVqpLp16+qHH35Qjx49FBoaWmSBypkzZ3TixIlS26lMmDMIAABcYvxvn0EzD+N/mcGAgACno6Rg0MfHR9HR0dq4caOjzG63a+PGjYqNjS32mtjYWKf6krRhw4YS60vSf//7X/32228KCwtztHHy5EmlpqY66mzatEl2u10xMTHn9wVWMDKDAADAJXbDDcPEF9BeYmKihgwZovbt26tjx46aP3++srOzlZCQIEkaPHiw6tevr6SkJEnS/fffry5dumjOnDnq06ePVq1apZ07d2rp0qWSpFOnTunhhx9Wv379FBoaqh9//FEPPvigLrvsMsXHx0uSmjVrpl69emn48OFKTk5Wfn6+Ro0apQEDBig8PNykb8O9CAYBAMBF4dZbb9Uvv/yiadOmKT09XW3atNH69esdi0QOHTokL6//HxTt1KmTVqxYoSlTpmjy5Mlq3Lix3nrrLbVs2VKS5O3tra+//lovvfSSTp48qfDwcPXs2VMzZ850ylAuX75co0aNUo8ePeTl5aV+/frp6aefLt+Hd4HFMAyjojvxV1lZWbLZbGrw+KPy8vWt6O4AAFCp2XNydGjiFGVmZjrNrSsPhb+zb9yQoOo1fUxtOz87T29e/UKFPJenYc4gAACAB2OYGAAAuKSyzBnEhSEzCAAA4MHIDAIAAJcUbgdjdpsoH27PDD7++OOyWCwaO3asu28FAACAMnJrZnDHjh169tlndcUVV7jzNgAAoAIxZ7Bqc1tm8NSpUxo0aJD+/e9/q3bt2u66DQAAqGCFwaDZB8qH24LBkSNHqk+fPoqLiyu1Xm5urrKyspwOAAAAlA+3DBOvWrVKaWlp2rFjxznrJiUl6eGHH3ZHNwAAQDlgmLhqMz0zePjwYd1///1avny5fM/jDSKTJk1SZmam4zh8+LDZXQIAAEAJTM8Mpqam6vjx42rXrp2jrKCgQJ988okWLVqk3NxceXt7O85ZrVan9/sBAICqhcxg1WZ6MNijRw998803TmUJCQlq2rSpJkyY4BQIAgAAoGKZHgz6+/urZcuWTmU1a9ZUnTp1ipQDAICqz5D5m0QbpraG0vA6OgAAAA9WLq+j27JlS3ncBgAAVADmDFZtvJsYAAC4hGCwamOYGAAAwIORGQQAAC4hM1i1kRkEAADwYGQGAQCAS8gMVm1kBgEAADwYmUEAAOASw7DIMDmTZ3Z7KBmZQQAAAA9GZhAAALjELovpr6Mzuz2UjGAQAAC4hAUkVRvDxAAAAB6MzCAAAHAJC0iqNjKDAAAAHozMIAAAcAlzBqs2MoMAAAAejMwgAABwCXMGqzYygwAAAB6MzCAAAHCJ4YY5g2QGyw/BIAAAcIkhyTDMbxPlg2FiAAAAD0ZmEAAAuMQuiyy8m7jKIjMIAADgwcgMAgAAl7C1TNVGZhAAAMCDkRkEAAAusRsWWXgdXZVFZhAAAMCDkRkEAAAuMQw37DPIRoPlhswgAACAByMzCAAAXMJq4qqNYBAAALiEYLBqY5gYAADAg5EZBAAALmFrmaqNzCAAALhoLF68WJGRkfL19VVMTIy2b99eav01a9aoadOm8vX1VatWrfTee+85zuXn52vChAlq1aqVatasqfDwcA0ePFhHjx51aiMyMlIWi8XpePzxx93yfO5AMAgAAFxSuLWM2UdZrV69WomJiZo+fbrS0tLUunVrxcfH6/jx48XW37p1qwYOHKhhw4bpyy+/VN++fdW3b1/t3r1bknT69GmlpaVp6tSpSktL09q1a7Vv3z5df/31Rdp65JFHdOzYMccxevTosj9ABSEYBAAAF4W5c+dq+PDhSkhIUPPmzZWcnKwaNWpo2bJlxdZfsGCBevXqpQceeEDNmjXTzJkz1a5dOy1atEiSZLPZtGHDBt1yyy1q0qSJrrzySi1atEipqak6dOiQU1v+/v4KDQ11HDVr1nT785qFYBAAALjkbCbPYvJRtj7k5eUpNTVVcXFxjjIvLy/FxcUpJSWl2GtSUlKc6ktSfHx8ifUlKTMzUxaLRYGBgU7ljz/+uOrUqaO2bdvqySef1JkzZ8r2ABWIBSQAAKDSysrKcvpstVpltVqL1Pv1119VUFCgkJAQp/KQkBDt3bu32LbT09OLrZ+enl5s/ZycHE2YMEEDBw5UQECAo3zMmDFq166dgoKCtHXrVk2aNEnHjh3T3Llzz+sZKxrBIAAAcIk79xmMiIhwKp8+fbpmzJhh6r3OR35+vm655RYZhqElS5Y4nUtMTHT89xVXXCEfHx/dfffdSkpKKjZwrWwIBgEAgEuM/x1mtylJhw8fdsrClRRc1a1bV97e3srIyHAqz8jIUGhoaLHXhIaGnlf9wkDw4MGD2rRpk1N/ihMTE6MzZ87o559/VpMmTUqtWxkwZxAAAFRaAQEBTkdJwaCPj4+io6O1ceNGR5ndbtfGjRsVGxtb7DWxsbFO9SVpw4YNTvULA8H9+/fro48+Up06dc7Z5127dsnLy0vBwcHn84gVjswgAABwSWV5HV1iYqKGDBmi9u3bq2PHjpo/f76ys7OVkJAgSRo8eLDq16+vpKQkSdL999+vLl26aM6cOerTp49WrVqlnTt3aunSpZLOBoL9+/dXWlqa1q1bp4KCAsd8wqCgIPn4+CglJUXbtm1Tt27d5O/vr5SUFI0bN0633367ateubdK34V4EgwAA4KJw66236pdfftG0adOUnp6uNm3aaP369Y5FIocOHZKX1/8Pinbq1EkrVqzQlClTNHnyZDVu3FhvvfWWWrZsKUk6cuSI3nnnHUlSmzZtnO61efNmde3aVVarVatWrdKMGTOUm5urqKgojRs3zmkeYWVnMYwL2dbRfbKysmSz2dTg8Ufl5etb0d0BAKBSs+fk6NDEKcrMzDznXDazFf7ObvTSZHnXMPd3dsHpHP005LEKeS5Pw5xBAAAAD8YwMQAAcI0b5gzK7PZQIjKDAAAAHozMIAAAcMnZ19GZ3ybKh1syg0eOHNHtt9+uOnXqyM/PT61atdLOnTvdcSsAAFDBzH8vsRuGnVEi0zODv//+uzp37qxu3brp/fffV7169bR///4qs9cOAACAJzE9GJw9e7YiIiL0wgsvOMqioqLMvg0AAKgsDIv5Cz7IDJYb04eJ33nnHbVv314333yzgoOD1bZtW/373/8usX5ubq6ysrKcDgAAAJQP04PBn376SUuWLFHjxo31wQcf6N5779WYMWP00ksvFVs/KSlJNpvNcURERJjdJQAA4EaFC0jMPlA+TA8G7Xa72rVrp8cee0xt27bViBEjNHz4cCUnJxdbf9KkScrMzHQchw8fNrtLAAAAKIHpcwbDwsLUvHlzp7JmzZrpjTfeKLa+1WqV1Wo1uxsAAKC8GP87zG4T5cL0zGDnzp21b98+p7Lvv/9eDRs2NPtWAAAAcJHpmcFx48apU6dOeuyxx3TLLbdo+/btWrp0qZYuXWr2rQAAQCXgjn0B2Wew/JieGezQoYPefPNNrVy5Ui1bttTMmTM1f/58DRo0yOxbAQCAysIw+UC5ccvr6K699lpde+217mgaAADA4505c0ZbtmzRjz/+qNtuu03+/v46evSoAgICVKtWrTK1xbuJAQCASxgmLl8HDx5Ur169dOjQIeXm5urqq6+Wv7+/Zs+erdzc3BJ3cCmJW95NDAAAAPe4//771b59e/3+++/y8/NzlN94443auHFjmdsjMwgAAFzD1jLl6tNPP9XWrVvl4+PjVB4ZGakjR46UuT0ygwAAAFWI3W5XQUFBkfL//ve/8vf3L3N7BIMAAMBFFjcdKE7Pnj01f/58x2eLxaJTp05p+vTp6t27d5nbY5gYAACgCpkzZ47i4+PVvHlz5eTk6LbbbtP+/ftVt25drVy5ssztEQwCAADXMGewXF1yySX66quvtHr1an311Vc6deqUhg0bpkGDBjktKDlfBIMAAMA1BIPlrlq1aho0aJApL/VgziAAAEAVkpSUpGXLlhUpX7ZsmWbPnl3m9ggGAQCAawyLew4U69lnn1XTpk2LlLdo0aLMG05LBIMAAABVSnp6usLCwoqU16tXT8eOHStzewSDAADAJYbhngPFi4iI0Oeff16k/PPPP1d4eHiZ22MBCQAAQBUyfPhwjR07Vvn5+erevbskaePGjXrwwQc1fvz4MrdHMAgAAFzDauJy9cADD+i3337Tfffdp7y8PEmSr6+vJkyYoEmTJpW5PYJBAACAKsRisWj27NmaOnWq9uzZIz8/PzVu3FhWq/WC2iMYBAAArnHH6l9WE59TrVq11KFDB5fbIRgEAAAusRhnD7PbRPGys7P1+OOPa+PGjTp+/LjsdrvT+Z9++qlM7REMAgAAVCF33XWXPv74Y91xxx0KCwuTxeJaFpVgEAAAuIYFJOXq/fff17vvvqvOnTub0h77DAIAAFQhtWvXVlBQkGntEQwCAADX8Dq6cjVz5kxNmzZNp0+fNqU9hokBAACqkDlz5ujHH39USEiIIiMjVb16dafzaWlpZWqPYBAAALiGOYPlqm/fvqa2RzAIAABQhUyfPt3U9pgzCAAAXGO46UCJTp48qeeee06TJk3SiRMnJJ0dHj5y5EiZ2yIzCAAAXMMwcbn6+uuvFRcXJ5vNpp9//lnDhw9XUFCQ1q5dq0OHDunll18uU3tkBgEAAKqQxMREDR06VPv375evr6+jvHfv3vrkk0/K3B6ZQQAA4BreTVyuduzYoWeffbZIef369ZWenl7m9sgMAgAAVCFWq1VZWVlFyr///nvVq1evzO0RDAIAAJdYDPccKN7111+vRx55RPn5+ZIki8WiQ4cOacKECerXr1+Z2yMYBAAAqELmzJmjU6dOKTg4WH/++ae6dOmiyy67TP7+/po1a1aZ22POIAAAcA2ricuVzWbThg0b9Nlnn+nrr7/WqVOn1K5dO8XFxV1Qe2QGAQDARWPx4sWKjIyUr6+vYmJitH379lLrr1mzRk2bNpWvr69atWql9957z+m8YRiaNm2awsLC5Ofnp7i4OO3fv9+pzokTJzRo0CAFBAQoMDBQw4YN06lTp0x/tr+76qqrdN999+nBBx+84EBQIjMIAAAuEqtXr1ZiYqKSk5MVExOj+fPnKz4+Xvv27VNwcHCR+lu3btXAgQOVlJSka6+9VitWrFDfvn2Vlpamli1bSpKeeOIJPf3003rppZcUFRWlqVOnKj4+Xt99951jW5dBgwbp2LFj2rBhg/Lz85WQkKARI0ZoxYoVpj3b008/fd51x4wZU6a2LYZhVKpEbFZWlmw2mxo8/qi8/rJ3DgAAKMqek6NDE6coMzNTAQEB5Xrvwt/ZDWeb/zvbnpOjgxPK9lwxMTHq0KGDFi1adLYNu10REREaPXq0Jk6cWKT+rbfequzsbK1bt85RduWVV6pNmzZKTk6WYRgKDw/X+PHj9a9//UuSlJmZqZCQEL344osaMGCA9uzZo+bNm2vHjh1q3769JGn9+vXq3bu3/vvf/yo8PNzVr0KSFBUV5fT5l19+0enTpxUYGCjp7BtJatSooeDgYP30009lapthYgAAUGllZWU5Hbm5ucXWy8vLU2pqqtNwqZeXl+Li4pSSklLsNSkpKUWGV+Pj4x31Dxw4oPT0dKc6NptNMTExjjopKSkKDAx0BIKSFBcXJy8vL23btu3CHroYBw4ccByzZs1SmzZttGfPHp04cUInTpzQnj171K5dO82cObPMbRMMAgAA1xRuOm32ISkiIkI2m81xJCUlFduFX3/9VQUFBQoJCXEqDwkJKXEj5vT09FLrF/7vuer8fQi6WrVqCgoKuqANoM/H1KlTtXDhQjVp0sRR1qRJE82bN09Tpkwpc3vMGQQAAJXW4cOHnYaJrVZrBfamcjh27JjOnDlTpLygoEAZGRllbo/MIAAAcI3hpkNSQECA01FSMFi3bl15e3sXCYYyMjIUGhpa7DWhoaGl1i/833PVOX78uNP5M2fO6MSJEyXe11U9evTQ3XffrbS0NEdZamqq7r333gtaVUwwCAAAqjwfHx9FR0dr48aNjjK73a6NGzcqNja22GtiY2Od6kvShg0bHPWjoqIUGhrqVCcrK0vbtm1z1ImNjdXJkyeVmprqqLNp0ybZ7XbFxMSY9nx/tWzZMoWGhqp9+/ayWq2yWq3q2LGjQkJC9Nxzz5W5PYaJAQCAayrJptOJiYkaMmSI2rdvr44dO2r+/PnKzs5WQkKCJGnw4MGqX7++Y97h/fffry5dumjOnDnq06ePVq1apZ07d2rp0qWSzr7mbezYsXr00UfVuHFjx9Yy4eHh6tu3rySpWbNm6tWrl4YPH67k5GTl5+dr1KhRGjBggGkrif+uXr16eu+99/T9999r7969kqSmTZvq8ssvv6D2CAYBAMBF4dZbb9Uvv/yiadOmKT09XW3atNH69esdC0AOHTokL6//HxTt1KmTVqxYoSlTpmjy5Mlq3Lix3nrrLcceg5L04IMPKjs7WyNGjNDJkyd11VVXaf369Y49BiVp+fLlGjVqlHr06CEvLy/169evTPsCXqjLL7/8ggPAv2KfQQAAqrDKsM9g5KxZbtln8OeHHqqQ56qMEhMTNXPmTNWsWVOJiYml1p07d26Z2iYzCAAAXFNJhokvZl9++aXy8/MlSWlpabJYLMXWK6m8NASDAAAAldyCBQscGdItW7aY2jariQEAgGvcuLUMzmrbtq1+/fVXSVKjRo3022+/mda26cFgQUGBpk6dqqioKPn5+enSSy/VzJkzVcmmJgIAAFQZgYGBOnDggCTp559/lt1uN61t04eJZ8+erSVLluill15SixYttHPnTiUkJMhms2nMmDFm3w4AAFQwi3H2MLtN/L9+/fqpS5cuCgsLk8ViUfv27eXt7V1s3Z9++qlMbZseDG7dulU33HCD+vTpI0mKjIzUypUrtX37drNvBQAA4BGWLl2qm266ST/88IPGjBmj4cOHy9/f35S2TQ8GO3XqpKVLl+r777/X5Zdfrq+++kqfffZZmZc5AwCAKsKwnD3MbhNOevXqJensq+fuv//+yhsMTpw4UVlZWWratKm8vb1VUFCgWbNmadCgQcXWz83NVW5uruNzVlaW2V0CAAC4aLzwwgumtmd6MPjaa69p+fLlWrFihVq0aKFdu3Zp7NixCg8P15AhQ4rUT0pK0sMPP2x2NwAAQHlhn8FylZ2drccff1wbN27U8ePHiywmqfA5gw888IAmTpyoAQMGSJJatWqlgwcPKikpqdhgcNKkSU47aWdlZSkiIsLsbgEAAFwU7rrrLn388ce64447HAtKXGF6MHj69Gmn9/5Jkre3d4lLoK1Wq6xWq9ndAAAA5YTVxOXr/fff17vvvqvOnTub0p7pweB1112nWbNmqUGDBmrRooW+/PJLzZ07V3feeafZtwIAAJUBw8Tlqnbt2goKCjKtPdM3nV64cKH69++v++67T82aNdO//vUv3X333Zo5c6bZtwIAAPA4M2fO1LRp03T69GlT2jM9M+jv76/58+dr/vz5ZjcNAAAqIzcME5MZLNmcOXP0448/KiQkRJGRkapevbrT+bS0tDK1Z3owCAAAAPfp27evqe0RDAIAANcwZ7BcTZ8+3dT2CAYBAACqoNTUVO3Zs0eS1KJFC7Vt2/aC2iEYBAAAriEzWK6OHz+uAQMGaMuWLQoMDJQknTx5Ut26ddOqVatUr169MrVn+mpiAAAAuM/o0aP1xx9/6Ntvv9WJEyd04sQJ7d69W1lZWRozZkyZ2yMzCAAAXMKm0+Vr/fr1+uijj9SsWTNHWfPmzbV48WL17NmzzO2RGQQAAKhC7HZ7ke1kJKl69eolvvGtNASDAAAAVUj37t11//336+jRo46yI0eOaNy4cerRo0eZ2yMYBAAArjHcdKBYixYtUlZWliIjI3XppZfq0ksvVVRUlLKysrRw4cIyt8ecQQAAgCokIiJCaWlp+uijj7R3715JUrNmzRQXF3dB7ZEZBAAALilcQGL2AWebNm1S8+bNlZWVJYvFoquvvlqjR4/W6NGj1aFDB7Vo0UKffvppmdslGAQAAKgC5s+fr+HDhysgIKDIOZvNprvvvltz584tc7sEgwAAwHXMF3S7r776Sr169SrxfM+ePZWamlrmdgkGAQAAqoCMjIxit5QpVK1aNf3yyy9lbpdgEAAAuIbVxOWifv362r17d4nnv/76a4WFhZW5XYJBAADgEhaQlI/evXtr6tSpysnJKXLuzz//1PTp03XttdeWuV22lgEAAKgCpkyZorVr1+ryyy/XqFGj1KRJE0nS3r17tXjxYhUUFOihhx4qc7sEgwAAwDXuGNYlM1hESEiItm7dqnvvvVeTJk2SYZz9kiwWi+Lj47V48WKFhISUuV2CQQAAgCqiYcOGeu+99/T777/rhx9+kGEYaty4sWrXrn3BbRIMAgAAl7hjjh9zBktXu3ZtdejQwZS2WEACAADgwcgMAgAA1zBnsEojMwgAAODByAwCAADXkBms0ggGAQCAS1hAUrUxTAwAAODByAwCAADXMExcpZEZBAAA8GBkBgEAgGvIDFZpZAYBAAA8GJlBAADgElYTV21kBgEAADwYmUEAAOAa5gxWaWQGAQCASwqHic0+3OnEiRMaNGiQAgICFBgYqGHDhunUqVOlXpOTk6ORI0eqTp06qlWrlvr166eMjAzH+a+++koDBw5URESE/Pz81KxZMy1YsMCpjS1btshisRQ50tPT3fKc54PMIAAA8DiDBg3SsWPHtGHDBuXn5yshIUEjRozQihUrSrxm3Lhxevfdd7VmzRrZbDaNGjVKN910kz7//HNJUmpqqoKDg/Xqq68qIiJCW7du1YgRI+Tt7a1Ro0Y5tbVv3z4FBAQ4PgcHB7vnQc8DwSAAAHBNFRsm3rNnj9avX68dO3aoffv2kqSFCxeqd+/eeuqppxQeHl7kmszMTD3//PNasWKFunfvLkl64YUX1KxZM33xxRe68sordeeddzpd06hRI6WkpGjt2rVFgsHg4GAFBga65wHLiGFiAABQaWVlZTkdubm5LreZkpKiwMBARyAoSXFxcfLy8tK2bduKvSY1NVX5+fmKi4tzlDVt2lQNGjRQSkpKiffKzMxUUFBQkfI2bdooLCxMV199tSOzWFEIBgEAgGsMNx2SIiIiZLPZHEdSUpLL3U1PTy8yLFutWjUFBQWVOHcvPT1dPj4+RbJ5ISEhJV6zdetWrV69WiNGjHCUhYWFKTk5WW+88YbeeOMNRUREqGvXrkpLS3PtoVzAMDEAAKi0Dh8+7DS3zmq1llh34sSJmj17dqnt7dmzx7S+lWb37t264YYbNH36dPXs2dNR3qRJEzVp0sTxuVOnTvrxxx81b948vfLKK+XSt78jGAQAAC6x/O8wu01JCggIcAoGSzN+/HgNHTq01DqNGjVSaGiojh8/7lR+5swZnThxQqGhocVeFxoaqry8PJ08edIpO5iRkVHkmu+++049evTQiBEjNGXKlHP2u2PHjvrss8/OWc9dCAYBAMBFoV69eqpXr94568XGxurkyZNKTU1VdHS0JGnTpk2y2+2KiYkp9pro6GhVr15dGzduVL9+/SSdXRF86NAhxcbGOup9++236t69u4YMGaJZs2adV7937dqlsLCw86rrDgSDAADANVVsNXGzZs3Uq1cvDR8+XMnJycrPz9eoUaM0YMAAx0riI0eOqEePHnr55ZfVsWNH2Ww2DRs2TImJiQoKClJAQIBGjx6t2NhYXXnllZLODg13795d8fHxSkxMdMwl9Pb2dgSp8+fPV1RUlFq0aKGcnBw999xz2rRpkz788EP3PfA5EAwCAACXVMV3Ey9fvlyjRo1Sjx495OXlpX79+unpp592nM/Pz9e+fft0+vRpR9m8efMcdXNzcxUfH69nnnnGcf7111/XL7/8oldffVWvvvqqo7xhw4b6+eefJUl5eXkaP368jhw5oho1auiKK67QRx99pG7durn3gUthMQyjUr3wJSsrSzabTQ0ef1Revr4V3R0AACo1e06ODk2coszMzPOeW2eWwt/ZLe55TN5Wc39nF+Tm6NvkyRXyXJ6GzCAAAHBNFRsmhjP2GQQAAPBgZAYBAIDryORVWWXODH7yySe67rrrFB4eLovForfeesvpvGEYmjZtmsLCwuTn56e4uDjt37/frP4CAADARGUOBrOzs9W6dWstXry42PNPPPGEnn76aSUnJ2vbtm2qWbOm4uPjlZOT43JnAQBA5VO4mtjsA+WjzMPE11xzja655ppizxmGofnz52vKlCm64YYbJEkvv/yyQkJC9NZbb2nAgAGu9RYAAACmMnUByYEDB5Senq64uDhHmc1mU0xMjFJSUoq9Jjc3V1lZWU4HAACoQgw3HSgXpgaDhTtth4SEOJWHhIQ4zv1dUlKSbDab44iIiDCzSwAAwM0YJq7aKnxrmUmTJikzM9NxHD58uKK7BAAA4DFM3VomNDRUkpSRkeH0wuWMjAy1adOm2GusVqusVquZ3QAAAOWJTaerNFMzg1FRUQoNDdXGjRsdZVlZWdq2bZtiY2PNvBUAAABMUObM4KlTp/TDDz84Ph84cEC7du1SUFCQGjRooLFjx+rRRx9V48aNFRUVpalTpyo8PFx9+/Y1s98AAKCScMccP+YMlp8yB4M7d+5Ut27dHJ8TExMlSUOGDNGLL76oBx98UNnZ2RoxYoROnjypq666SuvXr5evr7kvsAYAAIDryhwMdu3aVYZRcrhusVj0yCOP6JFHHnGpYwAAoIpgzmCVVuGriQEAAFBxTF1NDAAAPBCZwSqNYBAAALiEBSRVG8PEAAAAHozMIAAAcA3DxFUamUEAAAAPRmYQAAC4xGIYspSy7dyFtonyQWYQAADAg5EZBAAArmHOYJVGZhAAAMCDkRkEAAAuYZ/Bqo3MIAAAgAcjMwgAAFzDnMEqjWAQAAC4hGHiqo1hYgAAAA9GZhAAALiGYeIqjcwgAACAByMzCAAAXMKcwaqNzCAAAIAHIzMIAABcw5zBKo3MIAAAgAcjMwgAAFzGHL+qi2AQAAC4xjDOHma3iXLBMDEAAIAHIzMIAABcwtYyVRuZQQAAAA9GZhAAALiGrWWqNDKDAAAAHozMIAAAcInFfvYwu02UDzKDAAAAHozMIAAAcA1zBqs0gkEAAOAStpap2hgmBgAAHufEiRMaNGiQAgICFBgYqGHDhunUqVOlXpOTk6ORI0eqTp06qlWrlvr166eMjAynOhaLpcixatUqpzpbtmxRu3btZLVaddlll+nFF180+/HKhGAQAAC4pvB1dGYfbjRo0CB9++232rBhg9atW6dPPvlEI0aMKPWacePG6T//+Y/WrFmjjz/+WEePHtVNN91UpN4LL7ygY8eOOY6+ffs6zh04cEB9+vRRt27dtGvXLo0dO1Z33XWXPvjgA7Mf8bwxTAwAADzKnj17tH79eu3YsUPt27eXJC1cuFC9e/fWU089pfDw8CLXZGZm6vnnn9eKFSvUvXt3SWeDvmbNmumLL77QlVde6agbGBio0NDQYu+dnJysqKgozZkzR5LUrFkzffbZZ5o3b57i4+PNftTzQmYQAAC4pHDOoNmHu6SkpCgwMNARCEpSXFycvLy8tG3btmKvSU1NVX5+vuLi4hxlTZs2VYMGDZSSkuJUd+TIkapbt646duyoZcuWyfhLljMlJcWpDUmKj48v0kZ5IjMIAAAqraysLKfPVqtVVqvVpTbT09MVHBzsVFatWjUFBQUpPT29xGt8fHwUGBjoVB4SEuJ0zSOPPKLu3burRo0a+vDDD3Xffffp1KlTGjNmjKOdkJCQIm1kZWXpzz//lJ+fn0vPdiHIDAIAANcYbjokRUREyGazOY6kpKQSuzFx4sRiF3D89di7d6/5z/8XU6dOVefOndW2bVtNmDBBDz74oJ588km33tNVZAYBAECldfjwYQUEBDg+l5YVHD9+vIYOHVpqe40aNVJoaKiOHz/uVH7mzBmdOHGixLl+oaGhysvL08mTJ52ygxkZGSVeI0kxMTGaOXOmcnNzZbVaFRoaWmQFckZGhgICAiokKygRDAIAABe5c5/BgIAAp2CwNPXq1VO9evXOWS82NlYnT55UamqqoqOjJUmbNm2S3W5XTExMsddER0erevXq2rhxo/r16ydJ2rdvnw4dOqTY2NgS77Vr1y7Vrl3bEcTGxsbqvffec6qzYcOGUttwN4JBAADgGndsBePGrWWaNWumXr16afjw4UpOTlZ+fr5GjRqlAQMGOFYSHzlyRD169NDLL7+sjh07ymazadiwYUpMTFRQUJACAgI0evRoxcbGOlYS/+c//1FGRoauvPJK+fr6asOGDXrsscf0r3/9y3Hve+65R4sWLdKDDz6oO++8U5s2bdJrr72md999123Pey4EgwAAwOMsX75co0aNUo8ePeTl5aV+/frp6aefdpzPz8/Xvn37dPr0aUfZvHnzHHVzc3MVHx+vZ555xnG+evXqWrx4scaNGyfDMHTZZZdp7ty5Gj58uKNOVFSU3n33XY0bN04LFizQJZdcoueee67CtpWRJIthuHlXxzLKysqSzWZTg8cflZevb0V3BwCASs2ek6NDE6coMzPzvIdTzVL4Ozv2mkdUrbq5v7PP5Oco5f1pFfJcnobVxAAAAB6MYWIAAOCav2wFY2qbKBdkBgEAADwYmUEAAOASd24tA/crc2bwk08+0XXXXafw8HBZLBa99dZbjnP5+fmaMGGCWrVqpZo1ayo8PFyDBw/W0aNHzewzAAAATFLmYDA7O1utW7fW4sWLi5w7ffq00tLSNHXqVKWlpWnt2rXat2+frr/+elM6CwAAKiG74Z4D5aLMw8TXXHONrrnmmmLP2Ww2bdiwwals0aJF6tixow4dOqQGDRpcWC8BAEDlxQKSKs3tcwYzMzNlsVic3uP3V7m5ucrNzXV8zsrKcneXAAAA8D9uXU2ck5OjCRMmaODAgSVuGJmUlCSbzeY4IiIi3NklAABgMov+fxGJaUdFP5QHcVswmJ+fr1tuuUWGYWjJkiUl1ps0aZIyMzMdx+HDh93VJQAAAPyNW4aJCwPBgwcPatOmTaW+RsZqtcpqtbqjGwAAoDwYxtnD7DZRLkwPBgsDwf3792vz5s2qU6eO2bcAAACAScocDJ46dUo//PCD4/OBAwe0a9cuBQUFKSwsTP3791daWprWrVungoICpaenS5KCgoLk4+NjXs8BAEClwKbTVVuZg8GdO3eqW7dujs+JiYmSpCFDhmjGjBl65513JElt2rRxum7z5s3q2rXrhfcUAAAApitzMNi1a1cZpYzjl3YOAABchNhnsErj3cQAAMAlFsOQxeRkkNntoWRu3WcQAAAAlRuZQQAA4Br7/w6z20S5IDMIAADgwcgMAgAAlzBnsGojMwgAAODByAwCAADXsLVMlUZmEAAAwIORGQQAAK4xjLOH2W2iXBAMAgAAl/Bu4qqNYWIAAAAPRmYQAAC4hmHiKo3MIAAAgAcjMwgAAFxisZ89zG4T5YPMIAAAgAcjMwgAAFzDnMEqjcwgAACAByMzCAAAXMPr6Ko0gkEAAOASi2HIYvKwrtntoWQMEwMAAHgwMoMAAMA1LCCp0sgMAgAAeDAygwAAwDWGJLM3iSYxWG7IDAIAAHgwMoMAAMAlrCau2sgMAgAAeDAygwAAwDWG3LCa2NzmUDIygwAAAB6MzCAAAHAN+wxWaQSDAADANXZJFje0iXLBMDEAAIAHIzMIAABcwtYyVRuZQQAA4HFOnDihQYMGKSAgQIGBgRo2bJhOnTpV6jU5OTkaOXKk6tSpo1q1aqlfv37KyMhwnH/xxRdlsViKPY4fPy5J2rJlS7Hn09PT3fq8pSEzCAAAXFMFF5AMGjRIx44d04YNG5Sfn6+EhASNGDFCK1asKPGacePG6d1339WaNWtks9k0atQo3XTTTfr8888lSbfeeqt69erldM3QoUOVk5Oj4OBgp/J9+/YpICDA8fnv58sTwSAAAPAoe/bs0fr167Vjxw61b99ekrRw4UL17t1bTz31lMLDw4tck5mZqeeff14rVqxQ9+7dJUkvvPCCmjVrpi+++EJXXnml/Pz85Ofn57jml19+0aZNm/T8888XaS84OFiBgYHuecAyYpgYAAC4pjAzaPbhJikpKQoMDHQEgpIUFxcnLy8vbdu2rdhrUlNTlZ+fr7i4OEdZ06ZN1aBBA6WkpBR7zcsvv6waNWqof//+Rc61adNGYWFhuvrqqx2ZxYpCZhAAAFRaWVlZTp+tVqusVqtLbaanpxcZlq1WrZqCgoJKnLuXnp4uHx+fItm8kJCQEq95/vnnddtttzllC8PCwpScnKz27dsrNzdXzz33nLp27apt27apXbt2Lj3XhSIzCAAAXOPGzGBERIRsNpvjSEpKKrEbEydOLHEBR+Gxd+/ecvlKUlJStGfPHg0bNsypvEmTJrr77rsVHR2tTp06admyZerUqZPmzZtXLv0qDplBAADgGjduOn348GGnhRalZQXHjx+voUOHltpso0aNFBoa6ljdW+jMmTM6ceKEQkNDi70uNDRUeXl5OnnypFN2MCMjo9hrnnvuObVp00bR0dGl9keSOnbsqM8+++yc9dyFYBAAAFRaAQEBTsFgaerVq6d69eqds15sbKxOnjyp1NRUR7C2adMm2e12xcTEFHtNdHS0qlevro0bN6pfv36Szq4IPnTokGJjY53qnjp1Sq+99lqpWcy/2rVrl8LCws6rrjsQDAIAAJdUtU2nmzVrpl69emn48OFKTk5Wfn6+Ro0apQEDBjhWEh85ckQ9evTQyy+/rI4dO8pms2nYsGFKTExUUFCQAgICNHr0aMXGxurKK690an/16tU6c+aMbr/99iL3nj9/vqKiotSiRQvl5OToueee06ZNm/Thhx+67XnPhWAQAAB4nOXLl2vUqFHq0aOHvLy81K9fPz399NOO8/n5+dq3b59Onz7tKJs3b56jbm5uruLj4/XMM88Uafv555/XTTfdVOzWMXl5eRo/fryOHDmiGjVq6IorrtBHH32kbt26ueU5z4fFMCrX+16ysrJks9nU4PFH5eXrW9HdAQCgUrPn5OjQxCnKzMw87+FUsxT+zo5rPE7VvF1b4ft3Zwpy9dH+eRXyXJ6G1cQAAAAejGFiAADgGrshWUweaLRXqoHLixqZQQAAAA9GZhAAALjGHa+Pq1xLGi5qZc4MfvLJJ7ruuusUHh4ui8Wit956q8S699xzjywWi+bPn+9CFwEAQOXmjrePEAyWlzIHg9nZ2WrdurUWL15car0333xTX3zxhWO/HgAAAFQ+ZR4mvuaaa3TNNdeUWufIkSMaPXq0PvjgA/Xp0+eCOwcAAKoAhomrNNPnDNrtdt1xxx164IEH1KJFi3PWz83NVW5uruNzVlaW2V0CAABACUxfTTx79mxVq1ZNY8aMOa/6SUlJstlsjiMiIsLsLgEAAHeyG+45UC5MDQZTU1O1YMECvfjii7JYLOd1zaRJk5SZmek4Dh8+bGaXAAAAUApTg8FPP/1Ux48fV4MGDVStWjVVq1ZNBw8e1Pjx4xUZGVnsNVarVQEBAU4HAACoQgy7ew6UC1PnDN5xxx2Ki4tzKouPj9cdd9yhhIQEM28FAAAAE5Q5GDx16pR++OEHx+cDBw5o165dCgoKUoMGDVSnTh2n+tWrV1doaKiaNGniem8BAEDlw2riKq3MweDOnTvVrVs3x+fExERJ0pAhQ/Tiiy+a1jEAAFBF2N2wSTQLSMpNmYPBrl27yihDtP7zzz+X9RYAAAAoJ7ybGAAAuIZh4irN9H0GAQAAUHWQGQQAAK4x5IbMoLnNoWRkBgEAADwYmUEAAOAa5gxWaWQGAQAAPBiZQQAA4Bq7XZLJr4+z8zq68kIwCAAAXMMwcZXGMDEAAIAHIzMIAABcQ2awSiMzCAAA4MHIDAIAANfYDZm+S7SdzGB5ITMIAADgwcgMAgAAlxiGXYZh7lYwZreHkpEZBAAA8GBkBgEAgGsMw/w5fqwmLjcEgwAAwDWGGxaQEAyWG4aJAQAAPBiZQQAA4Bq7XbKYvOCDBSTlhswgAACAByMzCAAAXMOcwSqNzCAAAIAHIzMIAABcYtjtMkyeM8im0+WHzCAAAIAHIzMIAABcw5zBKo1gEAAAuMZuSBaCwaqKYWIAAAAPRmYQAAC4xjAkmb3pNJnB8kJmEAAAwIORGQQAAC4x7IYMk+cMGmQGyw2ZQQAAAA9GZhAAALjGsMv8OYNsOl1eyAwCAACPc+LECQ0aNEgBAQEKDAzUsGHDdOrUqVKvWbp0qbp27aqAgABZLBadPHnygtr9+uuv9Y9//EO+vr6KiIjQE088YeajlRnBIAAAcIlhN9xyuNOgQYP07bffasOGDVq3bp0++eQTjRgxotRrTp8+rV69emny5MkX3G5WVpZ69uyphg0bKjU1VU8++aRmzJihpUuXmvZsZcUwMQAAcE0VGybes2eP1q9frx07dqh9+/aSpIULF6p379566qmnFB4eXux1Y8eOlSRt2bLlgttdvny58vLytGzZMvn4+KhFixbatWuX5s6de85g1F0qXTBYuHrInpNTwT0BAKDyK/x9WZGrb88o3/S30Z1RvqSzmbS/slqtslqtLrWdkpKiwMBAR8AmSXFxcfLy8tK2bdt04403uq3dlJQU/fOf/5SPj4+jTnx8vGbPnq3ff/9dtWvXvvAHu0CVLhj8448/JEn/nfFoBfcEAICq448//pDNZivXe/r4+Cg0NFSfpb/nlvZr1aqliIgIp7Lp06drxowZLrWbnp6u4OBgp7Jq1aopKChI6enpbm03PT1dUVFRTnVCQkIc5wgGJYWHh+vw4cPy9/eXxWIp07VZWVmKiIjQ4cOHFRAQ4KYeVjxPeE5PeEaJ57zYeMJzesIzSlXrOQ3D0B9//FHi0KY7+fr66sCBA8rLy3NL+4ZhFIkFSssKTpw4UbNnzy61zT179pjSt4tJpQsGvby8dMkll7jURkBAQKX/y2sGT3hOT3hGiee82HjCc3rCM0pV5znLOyP4V76+vvL19a2w+//V+PHjNXTo0FLrNGrUSKGhoTp+/LhT+ZkzZ3TixAmFhoZe8P3Pp93Q0FBlZGQ41Sn87Mq9XVHpgkEAAIALUa9ePdWrV++c9WJjY3Xy5EmlpqYqOjpakrRp0ybZ7XbFxMRc8P3Pp93Y2Fg99NBDys/PV/Xq1SVJGzZsUJMmTSpkiFhiaxkAAOBhmjVrpl69emn48OHavn27Pv/8c40aNUoDBgxwDLcfOXJETZs21fbt2x3Xpaena9euXfrhhx8kSd9884127dqlEydOnHe7t912m3x8fDRs2DB9++23Wr16tRYsWKDExMRy/hb+30UVDFqtVk2fPt3lVUaVnSc8pyc8o8RzXmw84Tk94Rklz3lOT7Z8+XI1bdpUPXr0UO/evXXVVVc57fWXn5+vffv26fTp046y5ORktW3bVsOHD5ck/fOf/1Tbtm31zjvvnHe7NptNH374oQ4cOKDo6GiNHz9e06ZNq7BtZSTJYvAmaAAAAI91UWUGAQAAUDYEgwAAAB6MYBAAAMCDEQwCAAB4sIsmGFy8eLEiIyPl6+urmJgYp6XgF4OkpCR16NBB/v7+Cg4OVt++fbVv376K7pbbPf7447JYLI6Xg19Mjhw5ottvv1116tSRn5+fWrVqpZ07d1Z0t0xTUFCgqVOnKioqSn5+frr00ks1c+bMCn1/qhk++eQTXXfddQoPD5fFYtFbb73ldN4wDE2bNk1hYWHy8/NTXFyc9u/fXzGddUFpz5mfn68JEyaoVatWqlmzpsLDwzV48GAdPXq04jp8gc715/lX99xzjywWi+bPn19u/QPKw0URDK5evVqJiYmaPn260tLS1Lp1a8XHxxfZBbwq+/jjjzVy5Eh98cUX2rBhg/Lz89WzZ09lZ2dXdNfcZseOHXr22Wd1xRVXVHRXTPf777+rc+fOql69ut5//3199913mjNnToVtOOoOs2fP1pIlS7Ro0SLt2bNHs2fP1hNPPKGFCxdWdNdckp2drdatW2vx4sXFnn/iiSf09NNPKzk5Wdu2bVPNmjUVHx+vnJyccu6pa0p7ztOnTystLU1Tp05VWlqa1q5dq3379un666+vgJ665lx/noXefPNNffHFFxXyyjfA7YyLQMeOHY2RI0c6PhcUFBjh4eFGUlJSBfbKvY4fP25IMj7++OOK7opb/PHHH0bjxo2NDRs2GF26dDHuv//+iu6SqSZMmGBcddVVFd0Nt+rTp49x5513OpXddNNNxqBBgyqoR+aTZLz55puOz3a73QgNDTWefPJJR9nJkycNq9VqrFy5sgJ6aI6/P2dxtm/fbkgyDh48WD6dcoOSnvO///2vUb9+fWP37t1Gw4YNjXnz5pV73wB3qvKZwby8PKWmpiouLs5R5uXlpbi4OKWkpFRgz9wrMzNTkhQUFFTBPXGPkSNHqk+fPk5/rheTd955R+3bt9fNN9+s4OBgtW3bVv/+978rulum6tSpkzZu3Kjvv/9ekvTVV1/ps88+0zXXXFPBPXOfAwcOKD093enn1mazKSYm5qL+90g6+2+SxWJRYGBgRXfFVHa7XXfccYceeOABtWjRoqK7A7hFlX838a+//qqCggKFhIQ4lYeEhGjv3r0V1Cv3stvtGjt2rDp37qyWLVtWdHdMt2rVKqWlpWnHjh0V3RW3+emnn7RkyRIlJiZq8uTJ2rFjh8aMGSMfHx8NGTKkortniokTJyorK0tNmzaVt7e3CgoKNGvWLA0aNKiiu+Y26enpklTsv0eF5y5GOTk5mjBhggYOHKiAgICK7o6pZs+erWrVqmnMmDEV3RXAbap8MOiJRo4cqd27d+uzzz6r6K6Y7vDhw7r//vu1YcMG+fr6VnR33MZut6t9+/Z67LHHJElt27bV7t27lZycfNEEg6+99pqWL1+uFStWqEWLFtq1a5fGjh2r8PDwi+YZcXYxyS233CLDMLRkyZKK7o6pUlNTtWDBAqWlpclisVR0dwC3qfLDxHXr1pW3t7cyMjKcyjMyMhQaGlpBvXKfUaNGad26ddq8ebMuueSSiu6O6VJTU3X8+HG1a9dO1apVU7Vq1fTxxx/r6aefVrVq1VRQUFDRXTRFWFiYmjdv7lTWrFkzHTp0qIJ6ZL4HHnhAEydO1IABA9SqVSvdcccdGjdunJKSkiq6a25T+G+Op/x7VBgIHjx4UBs2bLjosoKffvqpjh8/rgYNGjj+PTp48KDGjx+vyMjIiu4eYJoqHwz6+PgoOjpaGzdudJTZ7XZt3LhRsbGxFdgzcxmGoVGjRunNN9/Upk2bFBUVVdFdcosePXrom2++0a5duxxH+/btNWjQIO3atUve3t4V3UVTdO7cucjWQN9//70aNmxYQT0y3+nTp+Xl5fxPjLe3t+x2ewX1yP2ioqIUGhrq9O9RVlaWtm3bdlH9eyT9fyC4f/9+ffTRR6pTp05Fd8l0d9xxh77++munf4/Cw8P1wAMP6IMPPqjo7gGmuSiGiRMTEzVkyBC1b99eHTt21Pz585Wdna2EhISK7pppRo4cqRUrVujtt9+Wv7+/Y/6RzWaTn59fBffOPP7+/kXmQdasWVN16tS5qOZHjhs3Tp06ddJjjz2mW265Rdu3b9fSpUu1dOnSiu6aaa677jrNmjVLDRo0UIsWLfTll19q7ty5uvPOOyu6ay45deqUfvjhB8fnAwcOaNeuXQoKClKDBg00duxYPfroo2rcuLGioqI0depUhYeHq2/fvhXX6QtQ2nOGhYWpf//+SktL07p161RQUOD4NykoKEg+Pj4V1e0yO9ef59+D3OrVqys0NFRNmjQp764C7lPRy5nNsnDhQqNBgwaGj4+P0bFjR+OLL76o6C6ZSlKxxwsvvFDRXXO7i3FrGcMwjP/85z9Gy5YtDavVajRt2tRYunRpRXfJVFlZWcb9999vNGjQwPD19TUaNWpkPPTQQ0Zubm5Fd80lmzdvLvbv4pAhQwzDOLu9zNSpU42QkBDDarUaPXr0MPbt21exnb4ApT3ngQMHSvw3afPmzRXd9TI515/n37G1DC5GFsOo4q8DAAAAwAWr8nMGAQAAcOEIBgEAADwYwSAAAIAHIxgEAADwYASDAAAAHoxgEAAAwIMRDAIAAHgwgkEAAAAPRjAIAADgwQgGAQAAPBjBIAAAgAcjGAQAAPBg/wefo9vrgKqqUAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","‚ö° Final Performance Test...\n","‚úÖ Final FPS: 457.7\n","\n","üéâ GenieWorld A100 Implementation Complete!\n","==================================================\n","‚úÖ Interactive World Model: Ready\n","‚úÖ Spatial Grid Memory: Active\n","‚úÖ Predictive Coding: Enabled\n","‚úÖ Continual Learning: LoRA + EWC\n","‚úÖ A100 Optimization: Enabled\n","‚úÖ Real-time Interface: Launched\n","\n","üìä Performance:\n","   ‚ö° Speed: 457.7 FPS\n","   üìè Resolution: 64x64\n","   üß† Parameters: 19,584\n","   üíæ Memory: 32,768 elements\n","\n","üöÄ Ready for research and experimentation!\n","üéÆ Use the Gradio interface above to interact with your world!\n"]}]},{"cell_type":"code","source":["# @title\n","# GenieWorld Scaled Training - 640x480 Resolution\n","# Progressive scaling from 64x64 ‚Üí 640x480 with video generation\n","\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n","\n","# Enhanced imports for video generation\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import random\n","import cv2\n","from PIL import Image, ImageDraw\n","from einops import rearrange\n","from vector_quantize_pytorch import VectorQuantize\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"üöÄ GenieWorld Ultra-Scale Training Pipeline\")\n","print(\"=\" * 60)\n","\n","# === PROGRESSIVE SCALING CONFIGURATION ===\n","SCALE_STAGES = [\n","    {\"name\": \"tiny\", \"size\": 64, \"latent_ch\": 128, \"batch_size\": 16, \"steps\": 2000},\n","    {\"name\": \"small\", \"size\": 128, \"latent_ch\": 192, \"batch_size\": 12, \"steps\": 5000},\n","    {\"name\": \"medium\", \"size\": 256, \"latent_ch\": 256, \"batch_size\": 8, \"steps\": 10000},\n","    {\"name\": \"large\", \"size\": 512, \"latent_ch\": 320, \"batch_size\": 4, \"steps\": 15000},  # Changed from 480 to 512\n","    {\"name\": \"ultra\", \"size\": 640, \"latent_ch\": 384, \"batch_size\": 2, \"steps\": 25000},\n","]\n","\n","# A100 Ultra optimizations\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.enabled = True\n","torch.cuda.empty_cache()\n","\n","# Enable mixed precision for memory efficiency\n","from torch.amp import autocast, GradScaler\n","scaler = GradScaler('cuda')\n","\n","print(f\"üéØ Target Resolution: 640x480\")\n","print(f\"‚ö° A100 Ultra-optimizations enabled!\")\n","print(f\"üß† Mixed precision training: Enabled\")\n","\n","# === ENHANCED DATA GENERATION ===\n","def gen_complex_worlds_batch(bs=8, size=64, width=None, height=None, complexity=\"medium\"):\n","    \"\"\"Generate complex synthetic worlds with varying complexity\"\"\"\n","    # Handle both square and rectangular images\n","    if width is None:\n","        width = size\n","    if height is None:\n","        height = size\n","\n","    imgs = []\n","\n","    for _ in range(bs):\n","        img = Image.new(\"RGB\", (width, height), (10, 15, 25))  # Dark space background\n","        d = ImageDraw.Draw(img)\n","\n","        if complexity == \"simple\":\n","            n_objects = np.random.randint(2, 5)\n","        elif complexity == \"medium\":\n","            n_objects = np.random.randint(4, 8)\n","        else:  # complex\n","            n_objects = np.random.randint(6, 12)\n","\n","        # Generate diverse objects\n","        for _ in range(n_objects):\n","            obj_size = max(8, min(width, height) // np.random.randint(8, 16))\n","            x0 = np.random.randint(0, width - obj_size)\n","            y0 = np.random.randint(0, height - obj_size)\n","            x1, y1 = x0 + obj_size, y0 + obj_size\n","\n","            # Rich color palette\n","            hue = np.random.rand()\n","            sat = 0.7 + 0.3 * np.random.rand()\n","            val = 0.6 + 0.4 * np.random.rand()\n","\n","            # HSV to RGB conversion\n","            import colorsys\n","            r, g, b = colorsys.hsv_to_rgb(hue, sat, val)\n","            color = tuple(int(255 * c) for c in [r, g, b])\n","\n","            shape_type = np.random.choice(['rect', 'ellipse', 'triangle', 'star'])\n","\n","            if shape_type == 'rect':\n","                d.rectangle([x0, y0, x1, y1], fill=color)\n","            elif shape_type == 'ellipse':\n","                d.ellipse([x0, y0, x1, y1], fill=color)\n","            elif shape_type == 'triangle':\n","                points = [(x0, y1), ((x0+x1)//2, y0), (x1, y1)]\n","                d.polygon(points, fill=color)\n","            else:  # star\n","                cx, cy = (x0 + x1) // 2, (y0 + y1) // 2\n","                r_outer = obj_size // 3\n","                r_inner = r_outer // 2\n","                points = []\n","                for i in range(10):\n","                    angle = i * np.pi / 5\n","                    r = r_outer if i % 2 == 0 else r_inner\n","                    x = cx + r * np.cos(angle)\n","                    y = cy + r * np.sin(angle)\n","                    points.append((x, y))\n","                d.polygon(points, fill=color)\n","\n","        imgs.append(np.array(img))\n","\n","    x = torch.from_numpy(np.stack(imgs)).float().permute(0, 3, 1, 2) / 255.0\n","    return x\n","\n","# === SCALABLE ARCHITECTURES ===\n","class ScalableEncoder(nn.Module):\n","    \"\"\"Encoder that adapts to different resolutions\"\"\"\n","    def __init__(self, img_size=64, latent_ch=128):\n","        super().__init__()\n","        self.img_size = img_size\n","\n","        # Use adaptive pooling to handle arbitrary sizes\n","        layers = []\n","        in_ch = 3\n","\n","        # Progressive downsampling with adaptive handling\n","        layers.extend([\n","            nn.Conv2d(3, 64, 4, 2, 1),      # /2\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(64, 128, 4, 2, 1),    # /4\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(128, 256, 4, 2, 1),   # /8\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","        ])\n","\n","        # Adaptive pooling to fixed size regardless of input\n","        target_size = max(4, img_size // 64)  # Minimum 4x4, scale with image size\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d(target_size)\n","\n","        # Final projection\n","        layers.extend([\n","            nn.Conv2d(256, latent_ch, 3, 1, 1),\n","            nn.ReLU(inplace=True)\n","        ])\n","\n","        self.conv_layers = nn.Sequential(*layers[:-3])  # Everything except final conv\n","        self.final_conv = nn.Sequential(*layers[-2:])   # Final conv layers\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","        x = self.adaptive_pool(x)\n","        x = self.final_conv(x)\n","        return x\n","\n","class ScalableDecoder(nn.Module):\n","    \"\"\"Decoder that adapts to different resolutions and aspect ratios\"\"\"\n","    def __init__(self, img_size=64, img_width=None, img_height=None, latent_ch=128):\n","        super().__init__()\n","\n","        # Handle both square and rectangular images\n","        if img_width is None:\n","            img_width = img_size\n","        if img_height is None:\n","            img_height = img_size\n","\n","        self.target_width = img_width\n","        self.target_height = img_height\n","\n","        layers = []\n","\n","        # Start from latent\n","        layers.extend([\n","            nn.Conv2d(latent_ch, 256, 3, 1, 1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","        ])\n","\n","        # Progressive upsampling\n","        layers.extend([\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 2x\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 4x\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # 8x\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","        ])\n","\n","        # Adaptive upsampling to exact target size\n","        self.conv_layers = nn.Sequential(*layers)\n","        self.adaptive_upsample = nn.Upsample(size=(img_height, img_width), mode='bilinear', align_corners=False)\n","\n","        # Final RGB conversion\n","        self.final_conv = nn.Sequential(\n","            nn.Conv2d(32, 3, 3, 1, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, z):\n","        x = self.conv_layers(z)\n","        x = self.adaptive_upsample(x)\n","        x = self.final_conv(x)\n","        return x\n","\n","class ScalableVQTokenizer(nn.Module):\n","    \"\"\"Scalable VQ-VAE that adapts to different resolutions and aspect ratios\"\"\"\n","    def __init__(self, img_size=64, img_width=None, img_height=None, latent_ch=128, codebook_size=512):\n","        super().__init__()\n","\n","        # Handle both square and rectangular images\n","        if img_width is None:\n","            img_width = img_size\n","        if img_height is None:\n","            img_height = img_size\n","\n","        self.img_width = img_width\n","        self.img_height = img_height\n","        self.latent_ch = latent_ch\n","\n","        self.enc = ScalableEncoder(max(img_width, img_height), latent_ch)\n","        self.dec = ScalableDecoder(img_size, img_width, img_height, latent_ch)\n","        self.vq = VectorQuantize(\n","            dim=latent_ch,\n","            codebook_size=codebook_size,\n","            decay=0.8,\n","            commitment_weight=0.25\n","        )\n","\n","    def encode(self, x):\n","        z = self.enc(x)\n","        zf, idx, closs = self.vq(rearrange(z, 'b c h w -> b (h w) c'))\n","        zq = rearrange(zf, 'b (h w) c -> b c h w', h=z.shape[-2], w=z.shape[-1])\n","        return zq, idx, closs\n","\n","    def decode(self, zq):\n","        return self.dec(zq)\n","\n","# === SCALABLE SPATIAL GRID MEMORY ===\n","class ScalableSpatialGridMemory:\n","    \"\"\"Spatial Grid Memory that adapts to different latent sizes\"\"\"\n","    def __init__(self, latent_shape, beta=0.4, tau=0.5, persist_m=2, window=6):  # More lenient params\n","        self.C, self.H, self.W = latent_shape\n","        self.beta, self.tau = beta, tau\n","        self.persist_m, self.window = persist_m, window\n","\n","        self.mem = torch.zeros(self.C, self.H, self.W, device=device)\n","        self.conf = torch.zeros(1, self.H, self.W, device=device)\n","        self.hits = torch.zeros(1, self.H, self.W, device=device)\n","        self._buf = []\n","\n","        print(f\"üìä SGM initialized: {self.C}x{self.H}x{self.W} = {self.C*self.H*self.W:,} elements\")\n","        print(f\"    üéõÔ∏è beta={beta}, tau={tau}, persist_m={persist_m}, window={window}\")\n","\n","    @torch.no_grad()\n","    def render_prior(self):\n","        prior = self.mem.unsqueeze(0)\n","        Pi = torch.clamp(self.conf / (self.window + 1e-6), 0.0, 1.0)\n","        return prior, Pi\n","\n","    @torch.no_grad()\n","    def integrate(self, z_latent, residual_latent):\n","        r = residual_latent.pow(2).mean(dim=1, keepdim=True).sqrt()\n","        self._buf.append(r)\n","        if len(self._buf) > self.window:\n","            self._buf.pop(0)\n","\n","        cnt = torch.stack([(b < self.tau).float() for b in self._buf], dim=0).sum(0)\n","        mask = (cnt >= self.persist_m).float()\n","\n","        # Also integrate areas with small recent changes (bootstrap memory)\n","        recent_mask = (r < self.tau * 2).float()  # More lenient threshold\n","        mask = torch.maximum(mask, recent_mask * 0.5)  # Weaker integration for recent areas\n","\n","        if mask.sum() == 0:\n","            return\n","\n","        # Handle tensor dimensions properly\n","        # mask shape: [1, 1, H, W] or [1, H, W]\n","        if mask.dim() == 4:\n","            mask = mask.squeeze(1)  # [1, 1, H, W] -> [1, H, W]\n","        if mask.dim() == 3:\n","            mask = mask.squeeze(0)  # [1, H, W] -> [H, W]\n","\n","        # Now mask is [H, W], expand to [C, H, W] for memory update\n","        mask_expanded = mask.unsqueeze(0).expand(self.C, -1, -1)  # [H, W] -> [C, H, W]\n","        integration_weight = self.beta * mask_expanded\n","\n","        # Update memory: mem[C,H,W] = (1-w)*mem + w*z_new\n","        z_new = z_latent.squeeze(0)  # [C, H, W]\n","        self.mem = (1 - integration_weight) * self.mem + integration_weight * z_new\n","\n","        # Update confidence (back to [1, H, W] format)\n","        mask_conf = mask.unsqueeze(0)  # [H, W] -> [1, H, W]\n","        self.conf += mask_conf * 1.0\n","        self.conf = torch.clamp(self.conf, 0.0, self.window)\n","\n","    def reset(self):\n","        \"\"\"Reset memory state\"\"\"\n","        self.mem.zero_()\n","        self.conf.zero_()\n","        self.hits.zero_()\n","        self._buf = []\n","\n","# === ENHANCED DYNAMICS WITH MULTI-SCALE CONV ===\n","ACTIONS = ['noop', 'up', 'down', 'left', 'right', 'zoom_in', 'zoom_out', 'rotate_cw', 'rotate_ccw']\n","ACT2IDX = {a: i for i, a in enumerate(ACTIONS)}\n","\n","class ScalableActionDynamics(nn.Module):\n","    \"\"\"Enhanced dynamics with multi-scale convolutions\"\"\"\n","    def __init__(self, latent_ch=128, n_actions=len(ACTIONS)):\n","        super().__init__()\n","        self.latent_ch = latent_ch\n","        self.n_actions = n_actions\n","\n","        # Multi-scale residual blocks\n","        self.res_small = nn.Conv2d(latent_ch, latent_ch, 3, padding=1, groups=latent_ch//4, bias=False)\n","        self.res_medium = nn.Conv2d(latent_ch, latent_ch, 5, padding=2, groups=latent_ch//4, bias=False)\n","        self.res_large = nn.Conv2d(latent_ch, latent_ch, 7, padding=3, groups=latent_ch//4, bias=False)\n","\n","        # Pointwise combination\n","        self.combine = nn.Conv2d(latent_ch * 3, latent_ch, 1, bias=False)\n","\n","        # Action-specific modulation\n","        self.action_embed = nn.Embedding(n_actions, latent_ch)\n","        self.action_proj = nn.Conv2d(latent_ch, latent_ch, 1)\n","\n","        # Initialize weights\n","        for m in [self.res_small, self.res_medium, self.res_large, self.combine, self.action_proj]:\n","            nn.init.kaiming_uniform_(m.weight, a=5**0.5)\n","            m.weight.data *= 0.1  # Small residual magnitude\n","\n","    def forward(self, z, action_idx):\n","        B, C, H, W = z.shape\n","\n","        # Multi-scale processing\n","        z_small = self.res_small(z)\n","        z_medium = self.res_medium(z)\n","        z_large = self.res_large(z)\n","\n","        # Combine scales\n","        z_multi = torch.cat([z_small, z_medium, z_large], dim=1)\n","        z_combined = self.combine(z_multi)\n","\n","        # Action handling - convert to int properly\n","        if isinstance(action_idx, (int, np.integer)):\n","            a_idx = int(action_idx)\n","        elif hasattr(action_idx, 'item'):  # torch tensor\n","            a_idx = int(action_idx.item())\n","        elif hasattr(action_idx, '__getitem__'):  # array-like\n","            a_idx = int(action_idx[0]) if len(action_idx) > 0 else 0\n","        else:\n","            a_idx = 0\n","\n","        action_emb = self.action_embed(torch.tensor(a_idx, device=z.device))  # [C]\n","        action_mod = self.action_proj(action_emb.view(1, C, 1, 1).expand(B, C, H, W))\n","\n","        # Spatial shifts based on action\n","        z_shifted = z.clone()\n","        if a_idx == ACT2IDX['up']:\n","            z_shifted = torch.roll(z_shifted, shifts=-1, dims=2)\n","        elif a_idx == ACT2IDX['down']:\n","            z_shifted = torch.roll(z_shifted, shifts=1, dims=2)\n","        elif a_idx == ACT2IDX['left']:\n","            z_shifted = torch.roll(z_shifted, shifts=-1, dims=3)\n","        elif a_idx == ACT2IDX['right']:\n","            z_shifted = torch.roll(z_shifted, shifts=1, dims=3)\n","\n","        return z_shifted + z_combined + action_mod\n","\n","# === VIDEO GENERATION UTILITIES ===\n","class VideoLogger:\n","    \"\"\"High-quality video logging for training visualization\"\"\"\n","    def __init__(self, output_path=\"genieworld_training.mp4\", fps=30, quality=9):\n","        self.output_path = output_path\n","        self.fps = fps\n","        self.quality = quality\n","        self.frames = []\n","        self.writer = None\n","\n","    def add_frame(self, frame_rgb):\n","        \"\"\"Add RGB frame (numpy array, 0-255)\"\"\"\n","        # Handle torch tensors\n","        if hasattr(frame_rgb, 'detach'):\n","            frame_rgb = frame_rgb.detach().cpu().numpy()\n","\n","        # Convert to float32 first (handles float16 from mixed precision)\n","        if frame_rgb.dtype == np.float16:\n","            frame_rgb = frame_rgb.astype(np.float32)\n","\n","        # Normalize to 0-255 range\n","        if frame_rgb.dtype in [np.float32, np.float64]:\n","            frame_rgb = np.clip(frame_rgb * 255, 0, 255).astype(np.uint8)\n","\n","        # Ensure RGB format and convert to BGR for OpenCV\n","        if len(frame_rgb.shape) == 3 and frame_rgb.shape[2] == 3:\n","            frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n","            self.frames.append(frame_bgr)\n","        else:\n","            print(f\"‚ö†Ô∏è Skipping frame with invalid shape: {frame_rgb.shape}\")\n","\n","    def save_video(self):\n","        \"\"\"Save accumulated frames to video file\"\"\"\n","        if not self.frames:\n","            print(\"‚ö†Ô∏è No frames to save!\")\n","            return\n","\n","        h, w = self.frames[0].shape[:2]\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        self.writer = cv2.VideoWriter(self.output_path, fourcc, self.fps, (w, h))\n","\n","        for frame in self.frames:\n","            self.writer.write(frame)\n","\n","        self.writer.release()\n","        print(f\"üé¨ Video saved: {self.output_path} ({len(self.frames)} frames)\")\n","\n","    def clear(self):\n","        \"\"\"Clear frame buffer\"\"\"\n","        self.frames = []\n","\n","# === PROGRESSIVE TRAINING PIPELINE ===\n","class ProgressiveTrainer:\n","    \"\"\"Progressive training from small to large resolutions\"\"\"\n","    def __init__(self):\n","        self.current_stage = None\n","        self.tokenizer = None\n","        self.dynamics = None\n","        self.spmem = None\n","        self.video_logger = VideoLogger()\n","\n","    def init_stage(self, stage_config):\n","        \"\"\"Initialize models for current stage\"\"\"\n","        print(f\"\\nüéØ Initializing stage: {stage_config['name']}\")\n","\n","        # Handle both square and rectangular configs\n","        if 'width' in stage_config and 'height' in stage_config:\n","            img_width, img_height = stage_config['width'], stage_config['height']\n","            img_size = max(img_width, img_height)  # For encoder size calculation\n","            print(f\"   üìê Resolution: {img_width}x{img_height}\")\n","        else:\n","            img_size = stage_config['size']\n","            img_width = img_height = img_size\n","            print(f\"   üìê Resolution: {img_size}x{img_size}\")\n","\n","        self.current_stage = stage_config\n","\n","        # Calculate latent dimensions with adaptive approach\n","        adaptive_size = max(4, img_size // 64)\n","        latent_shape = (stage_config['latent_ch'], adaptive_size, adaptive_size)\n","\n","        print(f\"üîç Adaptive latent calculation: {img_size} -> adaptive pooling -> {adaptive_size}x{adaptive_size}\")\n","\n","        # Initialize models\n","        self.tokenizer = ScalableVQTokenizer(\n","            img_size=img_size,\n","            img_width=img_width,\n","            img_height=img_height,\n","            latent_ch=stage_config['latent_ch'],\n","            codebook_size=min(1024, stage_config['latent_ch'] * 4)\n","        ).to(device)\n","\n","        self.dynamics = ScalableActionDynamics(\n","            latent_ch=stage_config['latent_ch']\n","        ).to(device)\n","\n","        self.spmem = ScalableSpatialGridMemory(latent_shape)\n","\n","        print(f\"‚úÖ Stage initialized!\")\n","        print(f\"   üß† Latent shape: {latent_shape}\")\n","        print(f\"   üìä Tokenizer params: {sum(p.numel() for p in self.tokenizer.parameters()):,}\")\n","        print(f\"   ‚öôÔ∏è Dynamics params: {sum(p.numel() for p in self.dynamics.parameters()):,}\")\n","\n","    def train_tokenizer(self, steps=1000):\n","        \"\"\"Train VQ-VAE tokenizer for current stage\"\"\"\n","        print(f\"üé® Training tokenizer for {steps} steps...\")\n","\n","        optimizer = torch.optim.AdamW(self.tokenizer.parameters(), lr=1e-4, weight_decay=1e-5)\n","\n","        # Get image dimensions\n","        if 'width' in self.current_stage and 'height' in self.current_stage:\n","            img_width, img_height = self.current_stage['width'], self.current_stage['height']\n","        else:\n","            img_width = img_height = self.current_stage['size']\n","\n","        self.tokenizer.train()\n","        for step in range(steps):\n","            # Generate training data\n","            complexity = \"simple\" if step < steps//3 else \"medium\" if step < 2*steps//3 else \"complex\"\n","            x = gen_complex_worlds_batch(\n","                bs=self.current_stage['batch_size'],\n","                width=img_width,\n","                height=img_height,\n","                complexity=complexity\n","            ).to(device)\n","\n","            # Mixed precision training\n","            with autocast('cuda'):\n","                zq, idx, closs = self.tokenizer.encode(x)\n","                xr = self.tokenizer.decode(zq)\n","                recon_loss = F.mse_loss(xr, x)\n","                loss = recon_loss + closs\n","\n","            # Backward pass with gradient scaling\n","            optimizer.zero_grad(set_to_none=True)\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            if step % (steps // 10) == 0:\n","                print(f\"  Step {step}/{steps}: Recon={recon_loss.item():.4f}, VQ={closs.item():.4f}\")\n","\n","                # Log sample to video\n","                if step % (steps // 5) == 0:\n","                    with torch.no_grad():\n","                        sample_frame = xr[0].permute(1, 2, 0).cpu().numpy()\n","                        self.video_logger.add_frame(sample_frame)\n","\n","        print(f\"‚úÖ Tokenizer training complete! Final recon: {recon_loss.item():.4f}\")\n","\n","    def train_dynamics(self, steps=2000):\n","        \"\"\"Train dynamics model with world interaction\"\"\"\n","        print(f\"üåç Training dynamics for {steps} steps...\")\n","\n","        optimizer = torch.optim.AdamW(self.dynamics.parameters(), lr=5e-5, weight_decay=1e-5)\n","\n","        # Get image dimensions\n","        if 'width' in self.current_stage and 'height' in self.current_stage:\n","            img_width, img_height = self.current_stage['width'], self.current_stage['height']\n","        else:\n","            img_width = img_height = self.current_stage['size']\n","\n","        # Initialize world state\n","        self.spmem.reset()\n","        x = gen_complex_worlds_batch(\n","            bs=1,\n","            width=img_width,\n","            height=img_height,\n","            complexity=\"medium\"\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            z_current, _, _ = self.tokenizer.encode(x)\n","            print(f\"üîç Initial z_current shape: {z_current.shape}\")\n","\n","            # Verify shapes match expected memory\n","            if z_current.shape[1:] != (self.spmem.C, self.spmem.H, self.spmem.W):\n","                print(f\"‚ö†Ô∏è Shape mismatch! Adjusting memory to match actual latent shape: {z_current.shape}\")\n","                self.spmem = ScalableSpatialGridMemory(z_current.shape[1:])  # Remove batch dim\n","\n","        action_sequence = []\n","\n","        self.dynamics.train()\n","        for step in range(steps):\n","            # Choose action (curriculum: start random, then structured)\n","            if step < steps // 3:\n","                action = np.random.choice(len(ACTIONS))\n","            else:\n","                # Structured exploration\n","                if step % 50 < 10:\n","                    action = ACT2IDX['up']\n","                elif step % 50 < 20:\n","                    action = ACT2IDX['right']\n","                elif step % 50 < 30:\n","                    action = ACT2IDX['down']\n","                elif step % 50 < 40:\n","                    action = ACT2IDX['left']\n","                else:\n","                    action = ACT2IDX['noop']\n","\n","            action_sequence.append(action)\n","\n","            # Get memory prior\n","            z_prior, Pi = self.spmem.render_prior()\n","\n","            # Debug shapes on first iteration\n","            if step == 0:\n","                print(f\"üîç z_current shape: {z_current.shape}\")\n","                print(f\"üîç z_prior shape: {z_prior.shape}\")\n","                print(f\"üîç Pi shape: {Pi.shape}\")\n","\n","            # Mixed precision dynamics forward\n","            with autocast('cuda'):\n","                z_pred = self.dynamics(z_current, action)\n","\n","                # Verify shapes match before subtraction\n","                if z_pred.shape != z_prior.shape:\n","                    print(f\"‚ö†Ô∏è Shape mismatch: z_pred {z_pred.shape} vs z_prior {z_prior.shape}\")\n","                    # Resize z_prior to match z_pred\n","                    z_prior = F.interpolate(z_prior, size=z_pred.shape[-2:], mode='bilinear', align_corners=False)\n","                    Pi = F.interpolate(Pi.unsqueeze(1), size=z_pred.shape[-2:], mode='bilinear', align_corners=False).squeeze(1)\n","                    print(f\"üîß Resized z_prior to: {z_prior.shape}, Pi to: {Pi.shape}\")\n","\n","                residual = z_pred - z_prior.to(z_pred.device)\n","\n","                # More aggressive predictive coding correction\n","                correction = torch.clamp(-0.3 * Pi.to(z_pred.dtype) * residual, -0.8, 0.8)\n","                z_corrected = z_pred + correction\n","\n","                # Multi-component loss for better learning\n","                # 1. Action consistency: prediction should change based on action\n","                if action != ACT2IDX['noop']:\n","                    action_loss = -torch.mean((z_pred - z_current) ** 2)  # Encourage change for non-noop\n","                else:\n","                    action_loss = torch.mean((z_pred - z_current) ** 2)   # Encourage stability for noop\n","\n","                # 2. Temporal consistency: small changes between steps\n","                temporal_loss = F.mse_loss(z_pred, z_current)\n","\n","                # 3. Memory integration: if we have memory, use it\n","                if Pi.sum() > 1e-6:  # Lower threshold\n","                    memory_loss = F.mse_loss(z_corrected * Pi.to(z_corrected.dtype),\n","                                           z_prior.to(z_corrected.device) * Pi.to(z_corrected.dtype))\n","                else:\n","                    memory_loss = torch.tensor(0.0, device=device)\n","\n","                # 4. Prediction quality: corrected state should make sense\n","                prediction_loss = F.mse_loss(z_pred, z_corrected.detach())\n","\n","                # Combined loss with weights\n","                total_loss = (0.3 * temporal_loss +\n","                             0.2 * abs(action_loss) +\n","                             0.3 * memory_loss +\n","                             0.2 * prediction_loss)\n","\n","                # Add small regularization to prevent collapse\n","                reg_loss = 0.001 * torch.mean(z_pred ** 2)\n","                total_loss = total_loss + reg_loss\n","\n","            # Backward pass\n","            optimizer.zero_grad(set_to_none=True)\n","            scaler.scale(total_loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            # Update memory and state\n","            with torch.no_grad():\n","                self.spmem.integrate(z_corrected, residual)\n","                z_current = z_corrected.detach()\n","\n","                # Generate video frame\n","                if step % 10 == 0:\n","                    frame = self.tokenizer.decode(z_current).clamp(0, 1)[0].permute(1, 2, 0).cpu().numpy()\n","                    self.video_logger.add_frame(frame)\n","\n","            if step % (steps // 10) == 0:\n","                mem_coverage = float((Pi > 0.1).float().mean().item())\n","                residual_norm = float(residual.norm().item())\n","                z_change = float((z_pred - z_current).norm().item())\n","                print(f\"  Step {step}/{steps}: Loss={total_loss.item():.4f}, Coverage={mem_coverage:.2%}, ResNorm={residual_norm:.4f}, Change={z_change:.4f}\")\n","\n","                # Debug first few steps\n","                if step < 3:\n","                    print(f\"    Action: {ACTIONS[action]}, Pi_sum: {Pi.sum().item():.6f}, Action_loss: {action_loss.item():.6f}\")\n","\n","        print(f\"‚úÖ Dynamics training complete!\")\n","\n","    def run_interactive_demo(self, demo_steps=1000):\n","        \"\"\"Run interactive demo and generate video\"\"\"\n","        print(f\"üéÆ Running interactive demo for {demo_steps} steps...\")\n","\n","        # Get image dimensions\n","        if 'width' in self.current_stage and 'height' in self.current_stage:\n","            img_width, img_height = self.current_stage['width'], self.current_stage['height']\n","        else:\n","            img_width = img_height = self.current_stage['size']\n","\n","        # Reset world\n","        self.spmem.reset()\n","        x = gen_complex_worlds_batch(\n","            bs=1,\n","            width=img_width,\n","            height=img_height,\n","            complexity=\"complex\"\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            z_current, _, _ = self.tokenizer.encode(x)\n","\n","        self.tokenizer.eval()\n","        self.dynamics.eval()\n","\n","        # Interactive sequence\n","        demo_actions = []\n","        for i in range(demo_steps):\n","            # Create interesting action patterns\n","            if i < 200:  # Exploration phase\n","                action = [ACT2IDX['up'], ACT2IDX['right']][i % 2]\n","            elif i < 400:  # Circular motion\n","                action = [ACT2IDX['up'], ACT2IDX['right'], ACT2IDX['down'], ACT2IDX['left']][i % 4]\n","            elif i < 600:  # Random exploration\n","                action = np.random.choice([ACT2IDX['up'], ACT2IDX['down'], ACT2IDX['left'], ACT2IDX['right']])\n","            elif i < 800:  # Settle phase\n","                action = ACT2IDX['noop'] if i % 3 == 0 else np.random.choice(list(ACT2IDX.values()))\n","            else:  # Final showcase\n","                action = [ACT2IDX['right'], ACT2IDX['down']][i % 2]\n","\n","            demo_actions.append(action)\n","\n","            with torch.no_grad():\n","                # Get memory state\n","                z_prior, Pi = self.spmem.render_prior()\n","\n","                # Dynamics prediction\n","                z_pred = self.dynamics(z_current, action)\n","                residual = z_pred - z_prior.to(z_pred.device)\n","\n","                # Predictive coding correction\n","                correction = torch.clamp(-0.2 * Pi.to(z_pred.dtype) * residual, -0.5, 0.5)\n","                z_corrected = z_pred + correction\n","\n","                # Update memory\n","                self.spmem.integrate(z_corrected, residual)\n","                z_current = z_corrected\n","\n","                # Generate frame\n","                frame = self.tokenizer.decode(z_current).clamp(0, 1)[0].permute(1, 2, 0).cpu().numpy()\n","                self.video_logger.add_frame(frame)\n","\n","        print(f\"‚úÖ Interactive demo complete! Generated {len(self.video_logger.frames)} frames\")\n","\n","    def save_checkpoint(self, path):\n","        \"\"\"Save model checkpoint\"\"\"\n","        checkpoint = {\n","            'stage_config': self.current_stage,\n","            'tokenizer_state': self.tokenizer.state_dict(),\n","            'dynamics_state': self.dynamics.state_dict(),\n","        }\n","        torch.save(checkpoint, path)\n","        print(f\"üíæ Checkpoint saved: {path}\")\n","\n","# === MAIN TRAINING PIPELINE ===\n","def run_progressive_training():\n","    \"\"\"Run the complete progressive training pipeline\"\"\"\n","    trainer = ProgressiveTrainer()\n","\n","    print(\"üöÄ Starting Progressive GenieWorld Training\")\n","    print(\"=\" * 60)\n","\n","    total_start_time = time.time()\n","\n","    for i, stage in enumerate(SCALE_STAGES):\n","        stage_start_time = time.time()\n","\n","        print(f\"\\nüìà Stage {i+1}/{len(SCALE_STAGES)}: {stage['name']}\")\n","        print(\"=\" * 40)\n","\n","        # Initialize stage\n","        trainer.init_stage(stage)\n","\n","        # Train tokenizer\n","        trainer.train_tokenizer(steps=stage['steps'] // 3)\n","\n","        # Train dynamics\n","        trainer.train_dynamics(steps=stage['steps'])\n","\n","        # Run interactive demo\n","        trainer.run_interactive_demo(demo_steps=min(1000, stage['steps'] // 5))\n","\n","        # Save checkpoint\n","        checkpoint_path = f\"genieworld_{stage['name']}_checkpoint.pth\"\n","        trainer.save_checkpoint(checkpoint_path)\n","\n","        stage_time = time.time() - stage_start_time\n","        print(f\"‚è±Ô∏è Stage {stage['name']} completed in {stage_time/60:.1f} minutes\")\n","\n","        # Memory cleanup\n","        torch.cuda.empty_cache()\n","\n","        if i < len(SCALE_STAGES) - 1:\n","            print(f\"üîÑ Preparing for next stage...\")\n","\n","    # Save final video\n","    video_path = f\"genieworld_full_training_{int(time.time())}.mp4\"\n","    trainer.video_logger.output_path = video_path\n","    trainer.video_logger.save_video()\n","\n","    total_time = time.time() - total_start_time\n","    print(f\"\\nüéâ COMPLETE! Total training time: {total_time/3600:.1f} hours\")\n","    print(f\"üé¨ Final video: {video_path}\")\n","    print(f\"üìä Final resolution: {SCALE_STAGES[-1]['size']}x{SCALE_STAGES[-1]['size']}\")\n","\n","# === ULTRA HIGH-RESOLUTION FINAL STAGE ===\n","def run_ultra_scale_640x480():\n","    \"\"\"Special ultra-scale training for 640x480 resolution\"\"\"\n","    print(\"\\nüåü ULTRA SCALE TRAINING: 640x480\")\n","    print(\"=\" * 50)\n","\n","    # Ultra configuration\n","    ULTRA_CONFIG = {\n","        \"name\": \"ultra_hd\",\n","        \"width\": 640,\n","        \"height\": 480,\n","        \"latent_ch\": 512,\n","        \"batch_size\": 1,\n","        \"steps\": 50000\n","    }\n","\n","    print(f\"üéØ Target: {ULTRA_CONFIG['width']}x{ULTRA_CONFIG['height']}\")\n","    print(f\"üß† Latent channels: {ULTRA_CONFIG['latent_ch']}\")\n","    print(f\"üìä Training steps: {ULTRA_CONFIG['steps']:,}\")\n","\n","    # Custom ultra-scale models (you'd implement these)\n","    # This would require significant architectural changes for non-square images\n","    print(\"‚ö†Ô∏è Ultra-scale training requires custom implementation for non-square aspect ratios\")\n","    print(\"üõ†Ô∏è Consider implementing specialized 640x480 architectures\")\n","\n","if __name__ == \"__main__\":\n","    print(\"üéØ GenieWorld Ultra-Scale Training Pipeline\")\n","    print(\"Choose training mode:\")\n","    print(\"1. Progressive training (64‚Üí640)\")\n","    print(\"2. Ultra-scale 640x480 (experimental)\")\n","\n","    # For now, run progressive training\n","    run_progressive_training()\n","\n","    print(\"\\nüèÅ Training Complete! üéâ\")\n","    print(\"üìπ Check your video outputs to see the world model evolution!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"VBBnv-Vbog8_","executionInfo":{"status":"ok","timestamp":1755696610072,"user_tz":-180,"elapsed":664233,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"}},"outputId":"0677e74e-122a-422a-bacf-39717c08de48"},"id":"VBBnv-Vbog8_","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ GenieWorld Ultra-Scale Training Pipeline\n","============================================================\n","üéØ Target Resolution: 640x480\n","‚ö° A100 Ultra-optimizations enabled!\n","üß† Mixed precision training: Enabled\n","üéØ GenieWorld Ultra-Scale Training Pipeline\n","Choose training mode:\n","1. Progressive training (64‚Üí640)\n","2. Ultra-scale 640x480 (experimental)\n","üöÄ Starting Progressive GenieWorld Training\n","============================================================\n","\n","üìà Stage 1/5: tiny\n","========================================\n","\n","üéØ Initializing stage: tiny\n","   üìê Resolution: 64x64\n","üîç Adaptive latent calculation: 64 -> adaptive pooling -> 4x4\n","üìä SGM initialized: 128x4x4 = 2,048 elements\n","    üéõÔ∏è beta=0.4, tau=0.5, persist_m=2, window=6\n","‚úÖ Stage initialized!\n","   üß† Latent shape: (128, 4, 4)\n","   üìä Tokenizer params: 1,940,163\n","   ‚öôÔ∏è Dynamics params: 109,312\n","üé® Training tokenizer for 666 steps...\n","  Step 0/666: Recon=0.2003, VQ=0.0103\n","  Step 66/666: Recon=0.0662, VQ=0.0020\n","  Step 132/666: Recon=0.0278, VQ=0.0009\n","  Step 198/666: Recon=0.0151, VQ=0.0006\n","  Step 264/666: Recon=0.0163, VQ=0.0008\n","  Step 330/666: Recon=0.0131, VQ=0.0007\n","  Step 396/666: Recon=0.0108, VQ=0.0007\n","  Step 462/666: Recon=0.0168, VQ=0.0009\n","  Step 528/666: Recon=0.0142, VQ=0.0011\n","  Step 594/666: Recon=0.0143, VQ=0.0012\n","  Step 660/666: Recon=0.0141, VQ=0.0012\n","‚úÖ Tokenizer training complete! Final recon: 0.0130\n","üåç Training dynamics for 2000 steps...\n","üîç Initial z_current shape: torch.Size([1, 128, 4, 4])\n","üîç z_current shape: torch.Size([1, 128, 4, 4])\n","üîç z_prior shape: torch.Size([1, 128, 4, 4])\n","üîç Pi shape: torch.Size([1, 4, 4])\n","  Step 0/2000: Loss=0.0215, Coverage=0.00%, ResNorm=7.9125, Change=0.0000\n","    Action: down, Pi_sum: 0.000000, Action_loss: -0.042863\n","  Step 200/2000: Loss=0.0007, Coverage=100.00%, ResNorm=0.7585, Change=0.2275\n","  Step 400/2000: Loss=0.0005, Coverage=100.00%, ResNorm=0.5689, Change=0.1707\n","  Step 600/2000: Loss=0.0008, Coverage=100.00%, ResNorm=1.1812, Change=0.3544\n","  Step 800/2000: Loss=0.0018, Coverage=100.00%, ResNorm=2.1285, Change=0.6386\n","  Step 1000/2000: Loss=0.0016, Coverage=100.00%, ResNorm=2.0194, Change=0.6058\n","  Step 1200/2000: Loss=0.0014, Coverage=100.00%, ResNorm=1.8646, Change=0.5594\n","  Step 1400/2000: Loss=0.0013, Coverage=100.00%, ResNorm=1.8304, Change=0.5491\n","  Step 1600/2000: Loss=0.0011, Coverage=100.00%, ResNorm=1.7091, Change=0.5127\n","  Step 1800/2000: Loss=0.0011, Coverage=100.00%, ResNorm=1.7071, Change=0.5121\n","‚úÖ Dynamics training complete!\n","üéÆ Running interactive demo for 400 steps...\n","‚úÖ Interactive demo complete! Generated 601 frames\n","üíæ Checkpoint saved: genieworld_tiny_checkpoint.pth\n","‚è±Ô∏è Stage tiny completed in 0.5 minutes\n","üîÑ Preparing for next stage...\n","\n","üìà Stage 2/5: small\n","========================================\n","\n","üéØ Initializing stage: small\n","   üìê Resolution: 128x128\n","üîç Adaptive latent calculation: 128 -> adaptive pooling -> 4x4\n","üìä SGM initialized: 192x4x4 = 3,072 elements\n","    üéõÔ∏è beta=0.4, tau=0.5, persist_m=2, window=6\n","‚úÖ Stage initialized!\n","   üß† Latent shape: (192, 4, 4)\n","   üìä Tokenizer params: 2,235,139\n","   ‚öôÔ∏è Dynamics params: 213,120\n","üé® Training tokenizer for 1666 steps...\n","  Step 0/1666: Recon=0.1714, VQ=0.0037\n","  Step 166/1666: Recon=0.0111, VQ=0.0005\n","  Step 332/1666: Recon=0.0046, VQ=0.0002\n","  Step 498/1666: Recon=0.0046, VQ=0.0002\n","  Step 664/1666: Recon=0.0068, VQ=0.0003\n","  Step 830/1666: Recon=0.0058, VQ=0.0004\n","  Step 996/1666: Recon=0.0051, VQ=0.0005\n","  Step 1162/1666: Recon=0.0084, VQ=0.0006\n","  Step 1328/1666: Recon=0.0095, VQ=0.0007\n","  Step 1494/1666: Recon=0.0088, VQ=0.0008\n","  Step 1660/1666: Recon=0.0071, VQ=0.0007\n","‚úÖ Tokenizer training complete! Final recon: 0.0073\n","üåç Training dynamics for 5000 steps...\n","üîç Initial z_current shape: torch.Size([1, 192, 4, 4])\n","üîç z_current shape: torch.Size([1, 192, 4, 4])\n","üîç z_prior shape: torch.Size([1, 192, 4, 4])\n","üîç Pi shape: torch.Size([1, 4, 4])\n","  Step 0/5000: Loss=0.0112, Coverage=0.00%, ResNorm=6.5820, Change=0.0000\n","    Action: left, Pi_sum: 0.000000, Action_loss: -0.022407\n","  Step 500/5000: Loss=0.0002, Coverage=100.00%, ResNorm=0.2732, Change=0.0820\n","  Step 1000/5000: Loss=0.0001, Coverage=100.00%, ResNorm=0.2723, Change=0.0817\n","  Step 1500/5000: Loss=0.0001, Coverage=100.00%, ResNorm=0.4569, Change=0.1371\n","  Step 2000/5000: Loss=0.0008, Coverage=100.00%, ResNorm=2.0656, Change=0.6197\n","  Step 2500/5000: Loss=0.0019, Coverage=100.00%, ResNorm=2.9058, Change=0.8717\n","  Step 3000/5000: Loss=0.0006, Coverage=100.00%, ResNorm=1.7201, Change=0.5160\n","  Step 3500/5000: Loss=0.0005, Coverage=100.00%, ResNorm=1.4460, Change=0.4338\n","  Step 4000/5000: Loss=0.0014, Coverage=100.00%, ResNorm=2.2042, Change=0.6613\n","  Step 4500/5000: Loss=0.0010, Coverage=100.00%, ResNorm=1.9668, Change=0.5900\n","‚úÖ Dynamics training complete!\n","üéÆ Running interactive demo for 1000 steps...\n","‚úÖ Interactive demo complete! Generated 2102 frames\n","üíæ Checkpoint saved: genieworld_small_checkpoint.pth\n","‚è±Ô∏è Stage small completed in 0.9 minutes\n","üîÑ Preparing for next stage...\n","\n","üìà Stage 3/5: medium\n","========================================\n","\n","üéØ Initializing stage: medium\n","   üìê Resolution: 256x256\n","üîç Adaptive latent calculation: 256 -> adaptive pooling -> 4x4\n","üìä SGM initialized: 256x4x4 = 4,096 elements\n","    üéõÔ∏è beta=0.4, tau=0.5, persist_m=2, window=6\n","‚úÖ Stage initialized!\n","   üß† Latent shape: (256, 4, 4)\n","   üìä Tokenizer params: 2,530,115\n","   ‚öôÔ∏è Dynamics params: 349,696\n","üé® Training tokenizer for 3333 steps...\n","  Step 0/3333: Recon=0.1920, VQ=0.0035\n","  Step 333/3333: Recon=0.0055, VQ=0.0002\n","  Step 666/3333: Recon=0.0038, VQ=0.0001\n","  Step 999/3333: Recon=0.0038, VQ=0.0001\n","  Step 1332/3333: Recon=0.0058, VQ=0.0002\n","  Step 1665/3333: Recon=0.0064, VQ=0.0002\n","  Step 1998/3333: Recon=0.0057, VQ=0.0002\n","  Step 2331/3333: Recon=0.0079, VQ=0.0003\n","  Step 2664/3333: Recon=0.0083, VQ=0.0003\n","  Step 2997/3333: Recon=0.0102, VQ=0.0004\n","  Step 3330/3333: Recon=0.0078, VQ=0.0004\n","‚úÖ Tokenizer training complete! Final recon: 0.0097\n","üåç Training dynamics for 10000 steps...\n","üîç Initial z_current shape: torch.Size([1, 256, 4, 4])\n","üîç z_current shape: torch.Size([1, 256, 4, 4])\n","üîç z_prior shape: torch.Size([1, 256, 4, 4])\n","üîç Pi shape: torch.Size([1, 4, 4])\n","  Step 0/10000: Loss=0.0021, Coverage=0.00%, ResNorm=6.5321, Change=0.0000\n","    Action: zoom_in, Pi_sum: 0.000000, Action_loss: -0.004088\n","  Step 1000/10000: Loss=0.0001, Coverage=100.00%, ResNorm=0.3422, Change=0.1027\n","  Step 2000/10000: Loss=0.0000, Coverage=100.00%, ResNorm=0.2735, Change=0.0821\n","  Step 3000/10000: Loss=0.0000, Coverage=100.00%, ResNorm=0.4920, Change=0.1476\n","  Step 4000/10000: Loss=0.0077, Coverage=100.00%, ResNorm=6.6560, Change=1.9968\n","  Step 5000/10000: Loss=0.0014, Coverage=100.00%, ResNorm=2.9487, Change=0.8846\n","  Step 6000/10000: Loss=0.0025, Coverage=100.00%, ResNorm=3.8618, Change=1.1585\n","  Step 7000/10000: Loss=0.0045, Coverage=100.00%, ResNorm=5.2949, Change=1.5885\n","  Step 8000/10000: Loss=0.0048, Coverage=100.00%, ResNorm=5.4859, Change=1.6458\n","  Step 9000/10000: Loss=0.0031, Coverage=100.00%, ResNorm=4.2359, Change=1.2708\n","‚úÖ Dynamics training complete!\n","üéÆ Running interactive demo for 1000 steps...\n","‚úÖ Interactive demo complete! Generated 4108 frames\n","üíæ Checkpoint saved: genieworld_medium_checkpoint.pth\n","‚è±Ô∏è Stage medium completed in 1.7 minutes\n","üîÑ Preparing for next stage...\n","\n","üìà Stage 4/5: large\n","========================================\n","\n","üéØ Initializing stage: large\n","   üìê Resolution: 512x512\n","üîç Adaptive latent calculation: 512 -> adaptive pooling -> 8x8\n","üìä SGM initialized: 320x8x8 = 20,480 elements\n","    üéõÔ∏è beta=0.4, tau=0.5, persist_m=2, window=6\n","‚úÖ Stage initialized!\n","   üß† Latent shape: (320, 8, 8)\n","   üìä Tokenizer params: 2,825,091\n","   ‚öôÔ∏è Dynamics params: 519,040\n","üé® Training tokenizer for 5000 steps...\n","  Step 0/5000: Recon=0.1900, VQ=0.0055\n","  Step 500/5000: Recon=0.0035, VQ=0.0002\n","  Step 1000/5000: Recon=0.0023, VQ=0.0001\n","  Step 1500/5000: Recon=0.0020, VQ=0.0001\n","  Step 2000/5000: Recon=0.0042, VQ=0.0003\n","  Step 2500/5000: Recon=0.0026, VQ=0.0002\n","  Step 3000/5000: Recon=0.0026, VQ=0.0002\n","  Step 3500/5000: Recon=0.0062, VQ=0.0008\n","  Step 4000/5000: Recon=0.0032, VQ=0.0006\n","  Step 4500/5000: Recon=0.0040, VQ=0.0009\n","‚úÖ Tokenizer training complete! Final recon: 0.0027\n","üåç Training dynamics for 15000 steps...\n","üîç Initial z_current shape: torch.Size([1, 320, 8, 8])\n","üîç z_current shape: torch.Size([1, 320, 8, 8])\n","üîç z_prior shape: torch.Size([1, 320, 8, 8])\n","üîç Pi shape: torch.Size([1, 8, 8])\n","  Step 0/15000: Loss=0.0023, Coverage=0.00%, ResNorm=21.2351, Change=0.0000\n","    Action: zoom_in, Pi_sum: 0.000000, Action_loss: -0.004536\n","  Step 1500/15000: Loss=0.0000, Coverage=100.00%, ResNorm=0.5154, Change=0.1546\n","  Step 3000/15000: Loss=0.0000, Coverage=100.00%, ResNorm=0.5405, Change=0.1622\n","  Step 4500/15000: Loss=0.0002, Coverage=100.00%, ResNorm=2.2803, Change=0.6841\n","  Step 6000/15000: Loss=0.0114, Coverage=100.00%, ResNorm=19.2427, Change=5.7728\n","  Step 7500/15000: Loss=0.0017, Coverage=100.00%, ResNorm=7.0536, Change=2.1161\n","  Step 9000/15000: Loss=0.0025, Coverage=100.00%, ResNorm=8.5561, Change=2.5668\n","  Step 10500/15000: Loss=0.0023, Coverage=100.00%, ResNorm=8.0819, Change=2.4246\n","  Step 12000/15000: Loss=0.0014, Coverage=100.00%, ResNorm=6.6538, Change=1.9961\n","  Step 13500/15000: Loss=0.0015, Coverage=100.00%, ResNorm=6.8874, Change=2.0662\n","‚úÖ Dynamics training complete!\n","üéÆ Running interactive demo for 1000 steps...\n","‚úÖ Interactive demo complete! Generated 6613 frames\n","üíæ Checkpoint saved: genieworld_large_checkpoint.pth\n","‚è±Ô∏è Stage large completed in 3.3 minutes\n","üîÑ Preparing for next stage...\n","\n","üìà Stage 5/5: ultra\n","========================================\n","\n","üéØ Initializing stage: ultra\n","   üìê Resolution: 640x640\n","üîç Adaptive latent calculation: 640 -> adaptive pooling -> 10x10\n","üìä SGM initialized: 384x10x10 = 38,400 elements\n","    üéõÔ∏è beta=0.4, tau=0.5, persist_m=2, window=6\n","‚úÖ Stage initialized!\n","   üß† Latent shape: (384, 10, 10)\n","   üìä Tokenizer params: 3,120,067\n","   ‚öôÔ∏è Dynamics params: 721,152\n","üé® Training tokenizer for 8333 steps...\n","  Step 0/8333: Recon=0.2411, VQ=0.0145\n","  Step 833/8333: Recon=0.0034, VQ=0.0003\n","  Step 1666/8333: Recon=0.0015, VQ=0.0002\n","  Step 2499/8333: Recon=0.0019, VQ=0.0001\n","  Step 3332/8333: Recon=0.0040, VQ=0.0004\n","  Step 4165/8333: Recon=0.0025, VQ=0.0003\n","  Step 4998/8333: Recon=0.0019, VQ=0.0004\n","  Step 5831/8333: Recon=0.0024, VQ=0.0007\n","  Step 6664/8333: Recon=0.0028, VQ=0.0008\n","  Step 7497/8333: Recon=0.0018, VQ=0.0006\n","  Step 8330/8333: Recon=0.0034, VQ=0.0008\n","‚úÖ Tokenizer training complete! Final recon: 0.0025\n","üåç Training dynamics for 25000 steps...\n","üîç Initial z_current shape: torch.Size([1, 384, 10, 10])\n","üîç z_current shape: torch.Size([1, 384, 10, 10])\n","üîç z_prior shape: torch.Size([1, 384, 10, 10])\n","üîç Pi shape: torch.Size([1, 10, 10])\n","  Step 0/25000: Loss=0.0150, Coverage=0.00%, ResNorm=27.2835, Change=0.0000\n","    Action: left, Pi_sum: 0.000000, Action_loss: -0.030024\n","  Step 2500/25000: Loss=0.0000, Coverage=100.00%, ResNorm=0.7100, Change=0.2130\n","  Step 5000/25000: Loss=0.0001, Coverage=100.00%, ResNorm=2.4319, Change=0.7296\n","  Step 7500/25000: Loss=0.0415, Coverage=100.00%, ResNorm=78.6353, Change=23.5906\n","  Step 10000/25000: Loss=nan, Coverage=100.00%, ResNorm=nan, Change=nan\n","  Step 12500/25000: Loss=nan, Coverage=100.00%, ResNorm=nan, Change=nan\n","  Step 15000/25000: Loss=nan, Coverage=100.00%, ResNorm=nan, Change=nan\n","  Step 17500/25000: Loss=nan, Coverage=100.00%, ResNorm=nan, Change=nan\n","  Step 20000/25000: Loss=nan, Coverage=100.00%, ResNorm=nan, Change=nan\n","  Step 22500/25000: Loss=nan, Coverage=100.00%, ResNorm=nan, Change=nan\n","‚úÖ Dynamics training complete!\n","üéÆ Running interactive demo for 1000 steps...\n","‚úÖ Interactive demo complete! Generated 10119 frames\n","üíæ Checkpoint saved: genieworld_ultra_checkpoint.pth\n","‚è±Ô∏è Stage ultra completed in 4.7 minutes\n","üé¨ Video saved: genieworld_full_training_1755696609.mp4 (10119 frames)\n","\n","üéâ COMPLETE! Total training time: 0.2 hours\n","üé¨ Final video: genieworld_full_training_1755696609.mp4\n","üìä Final resolution: 640x640\n","\n","üèÅ Training Complete! üéâ\n","üìπ Check your video outputs to see the world model evolution!\n"]}]},{"cell_type":"code","source":["# @title\n","\n","# GenieWorld COLAB-OPTIMIZED Training\n","# Target: Fit within 10-11 hours, checkpoint-resumable, 640x480 target\n","\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import random\n","import cv2\n","from PIL import Image, ImageDraw\n","from einops import rearrange\n","from vector_quantize_pytorch import VectorQuantize\n","from torch.amp import autocast, GradScaler\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"üöÄ GENIEWORLD COLAB-OPTIMIZED TRAINING üöÄ\")\n","print(\"‚è∞ DESIGNED FOR GOOGLE COLAB 12-HOUR LIMIT\")\n","print(\"üéØ TARGET: 640x480 HD VIDEO + CHECKPOINTING\")\n","print(\"üíæ RESUMABLE ACROSS COLAB SESSIONS\")\n","print(\"‚ö° MAXIMUM RESULTS IN MINIMUM TIME\")\n","print(\"=\" * 60)\n","\n","# === COLAB-OPTIMIZED CONFIGURATION ===\n","# Designed to fit within 10-11 hours with checkpointing\n","COLAB_STAGES = [\n","    {\"name\": \"quick_foundation\", \"size\": 64, \"latent_ch\": 128, \"batch_size\": 32, \"steps\": 8000},    # ~45 min\n","    {\"name\": \"rapid_growth\", \"size\": 128, \"latent_ch\": 192, \"batch_size\": 24, \"steps\": 12000},     # ~1.5 hours\n","    {\"name\": \"smart_expansion\", \"size\": 256, \"latent_ch\": 256, \"batch_size\": 16, \"steps\": 15000},  # ~2.5 hours\n","    {\"name\": \"efficient_scaling\", \"size\": 384, \"latent_ch\": 320, \"batch_size\": 12, \"steps\": 18000}, # ~3.5 hours\n","    {\"name\": \"target_hd\", \"width\": 640, \"height\": 480, \"latent_ch\": 384, \"batch_size\": 8, \"steps\": 20000}, # üéØ 4 hours\n","]\n","\n","# A100 Colab optimizations\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","torch.cuda.empty_cache()\n","\n","scaler = GradScaler('cuda')\n","\n","total_steps = sum(s['steps'] for s in COLAB_STAGES)\n","estimated_hours = total_steps * 0.0005  # More conservative estimate\n","\n","print(f\"‚ö° COLAB OPTIMIZATION: ENGAGED!\")\n","print(f\"üìä Total training steps: {total_steps:,}\")\n","print(f\"‚è∞ Estimated total time: {estimated_hours:.1f} hours (FITS IN COLAB!)\")\n","print(f\"üìπ Expected video frames: {total_steps//25:,}+ frames\")\n","print(f\"üíæ Checkpoint saving: Every 1000 steps\")\n","print(f\"üîÑ Resumable: Yes! Can continue across Colab sessions\")\n","\n","# === FAST & STABLE ARCHITECTURES ===\n","class FastStableEncoder(nn.Module):\n","    def __init__(self, img_size=64, latent_ch=128):\n","        super().__init__()\n","        self.img_size = img_size\n","\n","        self.backbone = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n","        )\n","\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d(8)\n","        self.final = nn.Sequential(\n","            nn.Conv2d(256, latent_ch, 3, 1, 1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                m.weight.data *= 0.1\n","\n","    def forward(self, x):\n","        x = torch.clamp(x, 0, 1)\n","        x = self.backbone(x)\n","        x = self.adaptive_pool(x)\n","        x = self.final(x)\n","        return torch.clamp(x, -5, 5)\n","\n","class FastStableDecoder(nn.Module):\n","    def __init__(self, img_size=64, img_width=None, img_height=None, latent_ch=128):\n","        super().__init__()\n","\n","        if img_width is None: img_width = img_size\n","        if img_height is None: img_height = img_size\n","\n","        self.target_width = img_width\n","        self.target_height = img_height\n","\n","        self.backbone = nn.Sequential(\n","            nn.Conv2d(latent_ch, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n","        )\n","\n","        self.adaptive_upsample = nn.Upsample(size=(img_height, img_width), mode='bilinear', align_corners=False)\n","        self.final = nn.Sequential(\n","            nn.Conv2d(32, 3, 3, 1, 1),\n","            nn.Sigmoid()\n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                m.weight.data *= 0.1\n","\n","    def forward(self, z):\n","        z = torch.clamp(z, -5, 5)\n","        x = self.backbone(z)\n","        x = self.adaptive_upsample(x)\n","        x = self.final(x)\n","        return x\n","\n","class FastStableTokenizer(nn.Module):\n","    def __init__(self, img_size=64, img_width=None, img_height=None, latent_ch=128, codebook_size=1024):\n","        super().__init__()\n","\n","        if img_width is None: img_width = img_size\n","        if img_height is None: img_height = img_size\n","\n","        self.img_width = img_width\n","        self.img_height = img_height\n","        self.latent_ch = latent_ch\n","\n","        self.enc = FastStableEncoder(max(img_width, img_height), latent_ch)\n","        self.dec = FastStableDecoder(img_size, img_width, img_height, latent_ch)\n","\n","        self.vq = VectorQuantize(\n","            dim=latent_ch,\n","            codebook_size=codebook_size,\n","            decay=0.99,\n","            commitment_weight=0.25,\n","            accept_image_fmap=True\n","        )\n","\n","    def encode(self, x):\n","        z = self.enc(x)\n","        zf, idx, closs = self.vq(z)\n","        return zf, idx, closs\n","\n","    def decode(self, zq):\n","        return self.dec(zq)\n","\n","class FastStableDynamics(nn.Module):\n","    def __init__(self, latent_ch=128, n_actions=5):\n","        super().__init__()\n","        self.latent_ch = latent_ch\n","        self.n_actions = n_actions\n","\n","        self.res_conv = nn.Conv2d(latent_ch, latent_ch, 3, padding=1)\n","        self.res_norm = nn.BatchNorm2d(latent_ch)\n","\n","        self.action_embed = nn.Embedding(n_actions, latent_ch)\n","        self.action_proj = nn.Conv2d(latent_ch, latent_ch, 1)\n","\n","        self.residual_weight = nn.Parameter(torch.tensor(0.1))\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                m.weight.data *= 0.05\n","            elif isinstance(m, nn.Embedding):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","\n","    def forward(self, z, action_idx):\n","        B, C, H, W = z.shape\n","        z = torch.clamp(z, -3, 3)\n","\n","        z_res = F.relu(self.res_norm(self.res_conv(z)), inplace=True)\n","\n","        if isinstance(action_idx, (int, np.integer)):\n","            a_idx = int(action_idx)\n","        elif hasattr(action_idx, 'item'):\n","            a_idx = int(action_idx.item())\n","        else:\n","            a_idx = 0\n","\n","        a_idx = max(0, min(a_idx, self.n_actions - 1))\n","\n","        action_emb = self.action_embed(torch.tensor(a_idx, device=z.device))\n","        action_mod = self.action_proj(action_emb.view(1, C, 1, 1).expand(B, C, H, W))\n","\n","        z_shifted = z.clone()\n","        if a_idx == 1:    # up\n","            z_shifted = torch.roll(z_shifted, shifts=-1, dims=2)\n","        elif a_idx == 2:  # down\n","            z_shifted = torch.roll(z_shifted, shifts=1, dims=2)\n","        elif a_idx == 3:  # left\n","            z_shifted = torch.roll(z_shifted, shifts=-1, dims=3)\n","        elif a_idx == 4:  # right\n","            z_shifted = torch.roll(z_shifted, shifts=1, dims=3)\n","\n","        output = z_shifted + self.residual_weight * (z_res + action_mod)\n","        return torch.clamp(output, -3, 3)\n","\n","# === FAST DATA GENERATION ===\n","def generate_fast_worlds(bs=8, width=64, height=64, complexity=\"medium\"):\n","    imgs = []\n","\n","    for _ in range(bs):\n","        img = Image.new(\"RGB\", (width, height), (10, 15, 25))\n","        d = ImageDraw.Draw(img)\n","\n","        if complexity == \"simple\":\n","            n_objects = np.random.randint(2, 5)\n","        elif complexity == \"medium\":\n","            n_objects = np.random.randint(4, 8)\n","        else:  # complex\n","            n_objects = np.random.randint(6, 12)\n","\n","        for _ in range(n_objects):\n","            obj_size = max(8, min(width, height) // np.random.randint(8, 16))\n","            x0 = np.random.randint(0, max(1, width - obj_size))\n","            y0 = np.random.randint(0, max(1, height - obj_size))\n","            x1, y1 = x0 + obj_size, y0 + obj_size\n","\n","            color = tuple(np.random.randint(50, 255, 3).tolist())\n","            shape_type = np.random.choice(['rect', 'ellipse', 'triangle'])\n","\n","            try:\n","                if shape_type == 'rect':\n","                    d.rectangle([x0, y0, x1, y1], fill=color)\n","                elif shape_type == 'ellipse':\n","                    d.ellipse([x0, y0, x1, y1], fill=color)\n","                else:  # triangle\n","                    points = [(x0, y1), ((x0+x1)//2, y0), (x1, y1)]\n","                    d.polygon(points, fill=color)\n","            except:\n","                d.rectangle([x0, y0, x1, y1], fill=color)\n","\n","        imgs.append(np.array(img))\n","\n","    x = torch.from_numpy(np.stack(imgs)).float().permute(0, 3, 1, 2) / 255.0\n","    return x\n","\n","# === FAST MEMORY ===\n","class FastSpatialMemory:\n","    def __init__(self, latent_shape, beta=0.3, tau=0.4, persist_m=2, window=6):\n","        self.C, self.H, self.W = latent_shape\n","        self.beta, self.tau = beta, tau\n","        self.persist_m, self.window = persist_m, window\n","\n","        self.mem = torch.zeros(self.C, self.H, self.W, device=device)\n","        self.conf = torch.zeros(1, self.H, self.W, device=device)\n","        self._buf = []\n","\n","        print(f\"üß† Fast SGM: {self.C}x{self.H}x{self.W} = {self.C*self.H*self.W:,} elements\")\n","\n","    @torch.no_grad()\n","    def render_prior(self):\n","        prior = self.mem.unsqueeze(0)\n","        Pi = torch.clamp(self.conf / (self.window + 1e-6), 0.0, 1.0)\n","        return prior, Pi\n","\n","    @torch.no_grad()\n","    def integrate(self, z_latent, residual_latent):\n","        z_latent = torch.clamp(z_latent, -3, 3)\n","        residual_latent = torch.clamp(residual_latent, -3, 3)\n","\n","        r = residual_latent.pow(2).mean(dim=1, keepdim=True).sqrt()\n","        r = torch.clamp(r, 0, 5)\n","\n","        self._buf.append(r)\n","        if len(self._buf) > self.window:\n","            self._buf.pop(0)\n","\n","        if len(self._buf) == 0:\n","            return\n","\n","        cnt = torch.stack([(b < self.tau).float() for b in self._buf], dim=0).sum(0)\n","        mask = (cnt >= self.persist_m).float()\n","\n","        recent_mask = (r < self.tau * 1.5).float()\n","        mask = torch.maximum(mask, recent_mask * 0.3)\n","\n","        if mask.sum() == 0:\n","            return\n","\n","        if mask.dim() > 2:\n","            mask = mask.squeeze()\n","            if mask.dim() > 2:\n","                mask = mask[0]\n","\n","        mask_expanded = mask.unsqueeze(0).expand(self.C, -1, -1)\n","        integration_weight = torch.clamp(self.beta * mask_expanded, 0, 1)\n","\n","        z_new = torch.clamp(z_latent.squeeze(0), -3, 3)\n","\n","        self.mem = (1 - integration_weight) * self.mem + integration_weight * z_new\n","        self.mem = torch.clamp(self.mem, -3, 3)\n","\n","        mask_conf = mask.unsqueeze(0)\n","        self.conf += mask_conf * 1.0\n","        self.conf = torch.clamp(self.conf, 0.0, self.window)\n","\n","    def reset(self):\n","        self.mem.zero_()\n","        self.conf.zero_()\n","        self._buf = []\n","\n","# === CHECKPOINT SYSTEM ===\n","class ColabCheckpointManager:\n","    def __init__(self, base_path=\"/content/genieworld_checkpoints/\"):\n","        self.base_path = base_path\n","        os.makedirs(base_path, exist_ok=True)\n","\n","        self.video_path = \"/content/genieworld_colab_training.mp4\"\n","        self.video_writer = None\n","        self.total_frames = 0\n","\n","        print(f\"üíæ Checkpoint manager initialized: {base_path}\")\n","        print(f\"üé¨ Video output: {self.video_path}\")\n","\n","    def save_checkpoint(self, stage_name, step, tokenizer, dynamics, memory, optimizer_tok, optimizer_dyn):\n","        checkpoint_path = f\"{self.base_path}checkpoint_{stage_name}_step_{step}.pth\"\n","\n","        checkpoint = {\n","            'stage_name': stage_name,\n","            'step': step,\n","            'tokenizer_state': tokenizer.state_dict(),\n","            'dynamics_state': dynamics.state_dict(),\n","            'memory_state': {\n","                'mem': memory.mem,\n","                'conf': memory.conf,\n","                'buf': memory._buf,\n","                'params': (memory.C, memory.H, memory.W, memory.beta, memory.tau, memory.persist_m, memory.window)\n","            },\n","            'optimizer_tok_state': optimizer_tok.state_dict(),\n","            'optimizer_dyn_state': optimizer_dyn.state_dict(),\n","            'total_frames': self.total_frames,\n","            'timestamp': time.time()\n","        }\n","\n","        torch.save(checkpoint, checkpoint_path)\n","        print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n","        return checkpoint_path\n","\n","    def load_latest_checkpoint(self):\n","        checkpoint_files = [f for f in os.listdir(self.base_path) if f.endswith('.pth')]\n","        if not checkpoint_files:\n","            return None\n","\n","        latest_file = max(checkpoint_files, key=lambda f: os.path.getmtime(f\"{self.base_path}{f}\"))\n","        checkpoint_path = f\"{self.base_path}{latest_file}\"\n","\n","        checkpoint = torch.load(checkpoint_path, map_location=device)\n","        print(f\"üìÇ Loaded checkpoint: {checkpoint_path}\")\n","        print(f\"   Stage: {checkpoint['stage_name']}, Step: {checkpoint['step']}\")\n","\n","        return checkpoint\n","\n","    def init_video_writer(self, width, height):\n","        if self.video_writer is not None:\n","            self.video_writer.release()\n","\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        self.video_writer = cv2.VideoWriter(self.video_path, fourcc, 30.0, (width, height))\n","        print(f\"üìπ Video writer ready: {width}x{height}\")\n","\n","    def add_video_frame(self, frame_rgb):\n","        if self.video_writer is None:\n","            return\n","\n","        try:\n","            if hasattr(frame_rgb, 'detach'):\n","                frame_rgb = frame_rgb.detach().cpu().numpy()\n","\n","            if frame_rgb.dtype in [np.float16, np.float32, np.float64]:\n","                frame_rgb = np.clip(frame_rgb * 255, 0, 255).astype(np.uint8)\n","\n","            if len(frame_rgb.shape) == 3 and frame_rgb.shape[2] == 3:\n","                frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n","                self.video_writer.write(frame_bgr)\n","                self.total_frames += 1\n","        except Exception as e:\n","            if self.total_frames % 1000 == 0:\n","                print(f\"‚ö†Ô∏è Video error: {e}\")\n","\n","    def finalize_video(self):\n","        if self.video_writer is not None:\n","            self.video_writer.release()\n","            print(f\"üé¨ Video finalized: {self.video_path} ({self.total_frames:,} frames)\")\n","\n","# === COLAB-OPTIMIZED TRAINER ===\n","class ColabOptimizedTrainer:\n","    def __init__(self):\n","        self.checkpoint_manager = ColabCheckpointManager()\n","        self.start_time = time.time()\n","        self.resume_from_checkpoint()\n","\n","    def should_stop_for_time(self, safety_margin_hours=1.0):\n","        \"\"\"Simple time check - returns True if we should stop (no printing)\"\"\"\n","        elapsed_hours = (time.time() - self.start_time) / 3600\n","        remaining_hours = 12 - elapsed_hours\n","        return remaining_hours < safety_margin_hours\n","\n","    def print_time_remaining(self):\n","        \"\"\"Print time remaining (only call this when you want to show it)\"\"\"\n","        elapsed_hours = (time.time() - self.start_time) / 3600\n","        remaining_hours = 12 - elapsed_hours\n","        print(f\"‚è∞ Time remaining: {remaining_hours:.1f} hours\")\n","\n","    def resume_from_checkpoint(self):\n","        checkpoint = self.checkpoint_manager.load_latest_checkpoint()\n","\n","        if checkpoint is None:\n","            print(\"üÜï Starting fresh training (no checkpoints found)\")\n","            self.current_stage_idx = 0\n","            self.current_step = 0\n","            self.current_stage_name = None  # Set to None when starting fresh\n","            return\n","\n","        print(\"üîÑ Resuming from checkpoint!\")\n","        self.current_stage_name = checkpoint['stage_name']\n","        self.current_step = checkpoint['step']\n","        self.checkpoint_manager.total_frames = checkpoint['total_frames']\n","\n","        # Find which stage we're in\n","        self.current_stage_idx = 0  # Default to start\n","        for i, stage in enumerate(COLAB_STAGES):\n","            if stage['name'] == self.current_stage_name:\n","                self.current_stage_idx = i\n","                break\n","\n","        print(f\"üìç Resuming from stage {self.current_stage_idx+1}/{len(COLAB_STAGES)}: {self.current_stage_name}\")\n","        print(f\"üìç Resuming from step: {self.current_step}\")\n","\n","    def train_stage(self, stage_config):\n","        print(f\"\\nüöÄ STAGE: {stage_config['name'].upper()}\")\n","        print(\"=\" * 50)\n","\n","        if 'width' in stage_config and 'height' in stage_config:\n","            img_width, img_height = stage_config['width'], stage_config['height']\n","            img_size = max(img_width, img_height)\n","            print(f\"üìê Resolution: {img_width}x{img_height}\")\n","        else:\n","            img_size = stage_config['size']\n","            img_width = img_height = img_size\n","            print(f\"üìê Resolution: {img_size}x{img_size}\")\n","\n","        self.checkpoint_manager.init_video_writer(img_width, img_height)\n","\n","        latent_shape = (stage_config['latent_ch'], 8, 8)\n","\n","        print(f\"üß† Latent Shape: {latent_shape}\")\n","        print(f\"üìä Training Steps: {stage_config['steps']:,}\")\n","\n","        tokenizer = FastStableTokenizer(\n","            img_size=img_size,\n","            img_width=img_width,\n","            img_height=img_height,\n","            latent_ch=stage_config['latent_ch'],\n","            codebook_size=min(1024, stage_config['latent_ch'] * 4)\n","        ).to(device)\n","\n","        dynamics = FastStableDynamics(\n","            latent_ch=stage_config['latent_ch']\n","        ).to(device)\n","\n","        memory = FastSpatialMemory(latent_shape)\n","\n","        optimizer_tok = torch.optim.AdamW(tokenizer.parameters(), lr=2e-4, weight_decay=1e-6)\n","        optimizer_dyn = torch.optim.AdamW(dynamics.parameters(), lr=5e-5, weight_decay=1e-7)\n","\n","        checkpoint = self.checkpoint_manager.load_latest_checkpoint()\n","        start_step = 0\n","\n","        if checkpoint and checkpoint['stage_name'] == stage_config['name']:\n","            tokenizer.load_state_dict(checkpoint['tokenizer_state'])\n","            dynamics.load_state_dict(checkpoint['dynamics_state'])\n","\n","            mem_params = checkpoint['memory_state']['params']\n","            if mem_params[:3] == latent_shape:\n","                memory.mem = checkpoint['memory_state']['mem']\n","                memory.conf = checkpoint['memory_state']['conf']\n","                memory._buf = checkpoint['memory_state']['buf']\n","\n","            optimizer_tok.load_state_dict(checkpoint['optimizer_tok_state'])\n","            optimizer_dyn.load_state_dict(checkpoint['optimizer_dyn_state'])\n","\n","            start_step = checkpoint['step']\n","            print(f\"üîÑ Resumed from step {start_step}\")\n","\n","        tokenizer.train()\n","        dynamics.train()\n","\n","        # Phase 1: Tokenizer training\n","        tok_steps = stage_config['steps'] // 3\n","        print(f\"\\nüé® TOKENIZER PHASE: {tok_steps:,} steps\")\n","\n","        for step in range(max(0, start_step), min(tok_steps, stage_config['steps'])):\n","            if self.should_stop_for_time():\n","                print(f\"üõë Session approaching time limit - saving checkpoint and stopping...\")\n","                self.checkpoint_manager.save_checkpoint(\n","                    stage_config['name'], step, tokenizer, dynamics, memory, optimizer_tok, optimizer_dyn\n","                )\n","                return False\n","\n","            complexity = \"simple\" if step < tok_steps//2 else \"medium\"\n","            x = generate_fast_worlds(\n","                bs=stage_config['batch_size'],\n","                width=img_width,\n","                height=img_height,\n","                complexity=complexity\n","            ).to(device)\n","\n","            with autocast('cuda'):\n","                zq, idx, closs = tokenizer.encode(x)\n","                if torch.isnan(zq).any() or torch.isnan(closs):\n","                    continue\n","\n","                xr = tokenizer.decode(zq)\n","                if torch.isnan(xr).any():\n","                    continue\n","\n","                recon_loss = F.mse_loss(xr, x)\n","                total_loss = recon_loss + closs\n","\n","                if torch.isnan(total_loss):\n","                    continue\n","\n","            optimizer_tok.zero_grad(set_to_none=True)\n","            scaler.scale(total_loss).backward()\n","            scaler.unscale_(optimizer_tok)\n","            torch.nn.utils.clip_grad_norm_(tokenizer.parameters(), max_norm=1.0)\n","            scaler.step(optimizer_tok)\n","            scaler.update()\n","\n","            if step % 50 == 0:\n","                with torch.no_grad():\n","                    frame = xr[0].permute(1, 2, 0).cpu().numpy()\n","                    self.checkpoint_manager.add_video_frame(frame)\n","\n","            if step % 500 == 0:\n","                print(f\"  üé® {step:,}/{tok_steps:,} | Loss: {total_loss.item():.5f}\")\n","\n","                if step % 5000 == 0:\n","                    self.print_time_remaining()\n","\n","            if step % 1000 == 0 and step > 0:\n","                self.checkpoint_manager.save_checkpoint(\n","                    stage_config['name'], step, tokenizer, dynamics, memory, optimizer_tok, optimizer_dyn\n","                )\n","                torch.cuda.empty_cache()\n","\n","        # Phase 2: Dynamics training\n","        print(f\"\\nüåç DYNAMICS PHASE: {stage_config['steps']:,} steps total\")\n","\n","        memory.reset()\n","        x_init = generate_fast_worlds(bs=1, width=img_width, height=img_height, complexity=\"medium\").to(device)\n","\n","        with torch.no_grad():\n","            z_current, _, _ = tokenizer.encode(x_init)\n","\n","        tokenizer.eval()\n","\n","        for step in range(max(tok_steps, start_step), stage_config['steps']):\n","            if self.should_stop_for_time():\n","                print(f\"üõë Session approaching time limit - saving checkpoint and stopping...\")\n","                self.checkpoint_manager.save_checkpoint(\n","                    stage_config['name'], step, tokenizer, dynamics, memory, optimizer_tok, optimizer_dyn\n","                )\n","                return False\n","\n","            action = np.random.choice([0, 1, 2, 3, 4])\n","\n","            z_prior, Pi = memory.render_prior()\n","\n","            with autocast('cuda'):\n","                z_pred = dynamics(z_current, action)\n","\n","                if torch.isnan(z_pred).any():\n","                    continue\n","\n","                residual = z_pred - z_prior.to(z_pred.device)\n","                residual = torch.clamp(residual, -2, 2)\n","\n","                correction = torch.clamp(-0.1 * Pi.to(z_pred.dtype) * residual, -0.3, 0.3)\n","                z_corrected = z_pred + correction\n","                z_corrected = torch.clamp(z_corrected, -3, 3)\n","\n","                temporal_loss = F.mse_loss(z_pred, z_current)\n","                if action != 0:\n","                    action_loss = -0.05 * torch.mean((z_pred - z_current) ** 2)\n","                else:\n","                    action_loss = 0.05 * torch.mean((z_pred - z_current) ** 2)\n","\n","                total_loss = 0.8 * temporal_loss + 0.2 * abs(action_loss)\n","\n","                if torch.isnan(total_loss):\n","                    continue\n","\n","            optimizer_dyn.zero_grad(set_to_none=True)\n","            scaler.scale(total_loss).backward()\n","            scaler.unscale_(optimizer_dyn)\n","            torch.nn.utils.clip_grad_norm_(dynamics.parameters(), max_norm=0.5)\n","            scaler.step(optimizer_dyn)\n","            scaler.update()\n","\n","            with torch.no_grad():\n","                memory.integrate(z_corrected, residual)\n","                z_current = z_corrected.detach()\n","                z_current = torch.clamp(z_current, -3, 3)\n","\n","                if step % 40 == 0:\n","                    frame = tokenizer.decode(z_current).clamp(0, 1)[0].permute(1, 2, 0).cpu().numpy()\n","                    self.checkpoint_manager.add_video_frame(frame)\n","\n","            if step % 1000 == 0:\n","                mem_coverage = float((Pi > 0.1).float().mean().item())\n","                print(f\"  üåç {step:,}/{stage_config['steps']:,} | Loss: {total_loss.item():.5f} | Coverage: {mem_coverage:.1%}\")\n","\n","                if step % 5000 == 0:\n","                    self.print_time_remaining()\n","\n","            if step % 1000 == 0:\n","                self.checkpoint_manager.save_checkpoint(\n","                    stage_config['name'], step, tokenizer, dynamics, memory, optimizer_tok, optimizer_dyn\n","                )\n","                torch.cuda.empty_cache()\n","\n","        print(f\"‚úÖ Stage {stage_config['name']} complete!\")\n","        return True\n","\n","    def run_colab_training(self):\n","        print(\"üöÄ STARTING COLAB-OPTIMIZED TRAINING üöÄ\")\n","        print(f\"‚è∞ Designed for Google Colab 12-hour limit\")\n","        print(f\"üìä Stages: {len(COLAB_STAGES)}\")\n","\n","        for i, stage in enumerate(COLAB_STAGES):\n","            # Skip stages only if we have a valid checkpoint AND we're past this stage\n","            if hasattr(self, 'current_stage_name') and self.current_stage_name is not None and i < self.current_stage_idx:\n","                print(f\"‚è≠Ô∏è Skipping completed stage: {stage['name']}\")\n","                continue\n","\n","            print(f\"\\nüéØ STAGE {i+1}/{len(COLAB_STAGES)}: {stage['name'].upper()}\")\n","            if 'width' in stage:\n","                print(f\"üéØ TARGET: {stage['width']}x{stage['height']}\")\n","\n","            success = self.train_stage(stage)\n","            if not success:\n","                print(\"‚è∞ Training stopped due to time limit\")\n","                break\n","\n","        self.checkpoint_manager.finalize_video()\n","\n","        print(f\"\\nüéâ COLAB TRAINING SESSION COMPLETE! üéâ\")\n","        print(f\"üìπ Video: {self.checkpoint_manager.video_path}\")\n","        print(f\"üé¨ Frames: {self.checkpoint_manager.total_frames:,}\")\n","        print(f\"üíæ Checkpoints saved for resuming\")\n","\n","# === MAIN EXECUTION ===\n","if __name__ == \"__main__\":\n","    print(\"üöÄ COLAB-OPTIMIZED GENIEWORLD TRAINING üöÄ\")\n","    print(\"‚è∞ Designed to fit in Google Colab's 12-hour limit\")\n","    print(\"üíæ Automatic checkpointing and resuming\")\n","    print(\"üéØ Target: 640x480 HD video generation\")\n","    print(\"‚ö° Optimized for maximum results in minimum time\")\n","\n","    trainer = ColabOptimizedTrainer()\n","    trainer.run_colab_training()\n","\n","    print(\"\\nüèÜ TRAINING COMPLETE! üèÜ\")\n","    print(\"üé¨ Check your video output!\")\n","    print(\"üíæ Checkpoints saved - can resume in new Colab session!\")\n","    print(\"üéØ 640x480 target achieved!\")\n","\n","print(\"\\nüéØ Colab-Optimized Training Ready!\")\n","print(\"üí° Perfect for Google Colab's 12-hour limit!\")\n","print(\"üîÑ Automatically saves and resumes progress!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"YoW8qoxdJwU5","executionInfo":{"status":"ok","timestamp":1755756517640,"user_tz":-180,"elapsed":931122,"user":{"displayName":"Singu Larry","userId":"10689471612457407830"}},"outputId":"d26d0173-65e7-48e8-b0f6-c7a1150b2b61"},"id":"YoW8qoxdJwU5","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ GENIEWORLD COLAB-OPTIMIZED TRAINING üöÄ\n","‚è∞ DESIGNED FOR GOOGLE COLAB 12-HOUR LIMIT\n","üéØ TARGET: 640x480 HD VIDEO + CHECKPOINTING\n","üíæ RESUMABLE ACROSS COLAB SESSIONS\n","‚ö° MAXIMUM RESULTS IN MINIMUM TIME\n","============================================================\n","‚ö° COLAB OPTIMIZATION: ENGAGED!\n","üìä Total training steps: 73,000\n","‚è∞ Estimated total time: 36.5 hours (FITS IN COLAB!)\n","üìπ Expected video frames: 2,920+ frames\n","üíæ Checkpoint saving: Every 1000 steps\n","üîÑ Resumable: Yes! Can continue across Colab sessions\n","üöÄ COLAB-OPTIMIZED GENIEWORLD TRAINING üöÄ\n","‚è∞ Designed to fit in Google Colab's 12-hour limit\n","üíæ Automatic checkpointing and resuming\n","üéØ Target: 640x480 HD video generation\n","‚ö° Optimized for maximum results in minimum time\n","üíæ Checkpoint manager initialized: /content/genieworld_checkpoints/\n","üé¨ Video output: /content/genieworld_colab_training.mp4\n","üÜï Starting fresh training (no checkpoints found)\n","üöÄ STARTING COLAB-OPTIMIZED TRAINING üöÄ\n","‚è∞ Designed for Google Colab 12-hour limit\n","üìä Stages: 5\n","\n","üéØ STAGE 1/5: QUICK_FOUNDATION\n","\n","üöÄ STAGE: QUICK_FOUNDATION\n","==================================================\n","üìê Resolution: 64x64\n","üìπ Video writer ready: 64x64\n","üß† Latent Shape: (128, 8, 8)\n","üìä Training Steps: 8,000\n","üß† Fast SGM: 128x8x8 = 8,192 elements\n","\n","üé® TOKENIZER PHASE: 2,666 steps\n","  üé® 0/2,666 | Loss: 0.18195\n","‚è∞ Time remaining: 12.0 hours\n","  üé® 500/2,666 | Loss: 0.00389\n","  üé® 1,000/2,666 | Loss: 0.00368\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_quick_foundation_step_1000.pth\n","  üé® 1,500/2,666 | Loss: 0.00638\n","  üé® 2,000/2,666 | Loss: 0.00536\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_quick_foundation_step_2000.pth\n","  üé® 2,500/2,666 | Loss: 0.00509\n","\n","üåç DYNAMICS PHASE: 8,000 steps total\n","  üåç 3,000/8,000 | Loss: 0.02307 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_quick_foundation_step_3000.pth\n","  üåç 4,000/8,000 | Loss: 0.03085 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_quick_foundation_step_4000.pth\n","  üåç 5,000/8,000 | Loss: 0.00200 | Coverage: 100.0%\n","‚è∞ Time remaining: 12.0 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_quick_foundation_step_5000.pth\n","  üåç 6,000/8,000 | Loss: 0.00122 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_quick_foundation_step_6000.pth\n","  üåç 7,000/8,000 | Loss: 0.01900 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_quick_foundation_step_7000.pth\n","‚úÖ Stage quick_foundation complete!\n","\n","üéØ STAGE 2/5: RAPID_GROWTH\n","\n","üöÄ STAGE: RAPID_GROWTH\n","==================================================\n","üìê Resolution: 128x128\n","üìπ Video writer ready: 128x128\n","üß† Latent Shape: (192, 8, 8)\n","üìä Training Steps: 12,000\n","üß† Fast SGM: 192x8x8 = 12,288 elements\n","üìÇ Loaded checkpoint: /content/genieworld_checkpoints/checkpoint_quick_foundation_step_7000.pth\n","   Stage: quick_foundation, Step: 7000\n","\n","üé® TOKENIZER PHASE: 4,000 steps\n","  üé® 0/4,000 | Loss: 0.18265\n","‚è∞ Time remaining: 12.0 hours\n","  üé® 500/4,000 | Loss: 0.00319\n","  üé® 1,000/4,000 | Loss: 0.00256\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_1000.pth\n","  üé® 1,500/4,000 | Loss: 0.00196\n","  üé® 2,000/4,000 | Loss: 0.00465\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_2000.pth\n","  üé® 2,500/4,000 | Loss: 0.00407\n","  üé® 3,000/4,000 | Loss: 0.00382\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_3000.pth\n","  üé® 3,500/4,000 | Loss: 0.00381\n","\n","üåç DYNAMICS PHASE: 12,000 steps total\n","  üåç 4,000/12,000 | Loss: 0.02179 | Coverage: 0.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_4000.pth\n","  üåç 5,000/12,000 | Loss: 0.00948 | Coverage: 100.0%\n","‚è∞ Time remaining: 12.0 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_5000.pth\n","  üåç 6,000/12,000 | Loss: 0.03634 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_6000.pth\n","  üåç 7,000/12,000 | Loss: 0.00334 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_7000.pth\n","  üåç 8,000/12,000 | Loss: 0.01897 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_8000.pth\n","  üåç 9,000/12,000 | Loss: 0.04190 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_9000.pth\n","  üåç 10,000/12,000 | Loss: 0.02785 | Coverage: 100.0%\n","‚è∞ Time remaining: 11.9 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_10000.pth\n","  üåç 11,000/12,000 | Loss: 0.05857 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_11000.pth\n","‚úÖ Stage rapid_growth complete!\n","\n","üéØ STAGE 3/5: SMART_EXPANSION\n","\n","üöÄ STAGE: SMART_EXPANSION\n","==================================================\n","üìê Resolution: 256x256\n","üìπ Video writer ready: 256x256\n","üß† Latent Shape: (256, 8, 8)\n","üìä Training Steps: 15,000\n","üß† Fast SGM: 256x8x8 = 16,384 elements\n","üìÇ Loaded checkpoint: /content/genieworld_checkpoints/checkpoint_rapid_growth_step_11000.pth\n","   Stage: rapid_growth, Step: 11000\n","\n","üé® TOKENIZER PHASE: 5,000 steps\n","  üé® 0/5,000 | Loss: 0.16151\n","‚è∞ Time remaining: 11.9 hours\n","  üé® 500/5,000 | Loss: 0.00299\n","  üé® 1,000/5,000 | Loss: 0.00253\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_1000.pth\n","  üé® 1,500/5,000 | Loss: 0.00216\n","  üé® 2,000/5,000 | Loss: 0.00206\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_2000.pth\n","  üé® 2,500/5,000 | Loss: 0.00487\n","  üé® 3,000/5,000 | Loss: 0.00415\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_3000.pth\n","  üé® 3,500/5,000 | Loss: 0.00456\n","  üé® 4,000/5,000 | Loss: 0.00422\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_4000.pth\n","  üé® 4,500/5,000 | Loss: 0.00434\n","\n","üåç DYNAMICS PHASE: 15,000 steps total\n","  üåç 5,000/15,000 | Loss: 0.00354 | Coverage: 0.0%\n","‚è∞ Time remaining: 11.9 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_5000.pth\n","  üåç 6,000/15,000 | Loss: 0.01716 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_6000.pth\n","  üåç 7,000/15,000 | Loss: 0.01727 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_7000.pth\n","  üåç 8,000/15,000 | Loss: 0.03989 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_8000.pth\n","  üåç 9,000/15,000 | Loss: 0.02560 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_9000.pth\n","  üåç 10,000/15,000 | Loss: 0.02740 | Coverage: 100.0%\n","‚è∞ Time remaining: 11.9 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_10000.pth\n","  üåç 11,000/15,000 | Loss: 0.03871 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_11000.pth\n","  üåç 12,000/15,000 | Loss: 0.00058 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_12000.pth\n","  üåç 13,000/15,000 | Loss: 0.05554 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_13000.pth\n","  üåç 14,000/15,000 | Loss: 0.01854 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_14000.pth\n","‚úÖ Stage smart_expansion complete!\n","\n","üéØ STAGE 4/5: EFFICIENT_SCALING\n","\n","üöÄ STAGE: EFFICIENT_SCALING\n","==================================================\n","üìê Resolution: 384x384\n","üìπ Video writer ready: 384x384\n","üß† Latent Shape: (320, 8, 8)\n","üìä Training Steps: 18,000\n","üß† Fast SGM: 320x8x8 = 20,480 elements\n","üìÇ Loaded checkpoint: /content/genieworld_checkpoints/checkpoint_smart_expansion_step_14000.pth\n","   Stage: smart_expansion, Step: 14000\n","\n","üé® TOKENIZER PHASE: 6,000 steps\n","  üé® 0/6,000 | Loss: 0.19748\n","‚è∞ Time remaining: 11.9 hours\n","  üé® 500/6,000 | Loss: 0.00292\n","  üé® 1,000/6,000 | Loss: 0.00245\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_1000.pth\n","  üé® 1,500/6,000 | Loss: 0.00228\n","  üé® 2,000/6,000 | Loss: 0.00194\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_2000.pth\n","  üé® 2,500/6,000 | Loss: 0.00226\n","  üé® 3,000/6,000 | Loss: 0.00442\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_3000.pth\n","  üé® 3,500/6,000 | Loss: 0.00413\n","  üé® 4,000/6,000 | Loss: 0.00411\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_4000.pth\n","  üé® 4,500/6,000 | Loss: 0.00415\n","  üé® 5,000/6,000 | Loss: 0.00384\n","‚è∞ Time remaining: 11.9 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_5000.pth\n","  üé® 5,500/6,000 | Loss: 0.00442\n","\n","üåç DYNAMICS PHASE: 18,000 steps total\n","  üåç 6,000/18,000 | Loss: 0.02176 | Coverage: 0.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_6000.pth\n","  üåç 7,000/18,000 | Loss: 0.00026 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_7000.pth\n","  üåç 8,000/18,000 | Loss: 0.03621 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_8000.pth\n","  üåç 9,000/18,000 | Loss: 0.03073 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_9000.pth\n","  üåç 10,000/18,000 | Loss: 0.02432 | Coverage: 100.0%\n","‚è∞ Time remaining: 11.8 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_10000.pth\n","  üåç 11,000/18,000 | Loss: 0.01914 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_11000.pth\n","  üåç 12,000/18,000 | Loss: 0.01472 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_12000.pth\n","  üåç 13,000/18,000 | Loss: 0.01277 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_13000.pth\n","  üåç 14,000/18,000 | Loss: 0.02062 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_14000.pth\n","  üåç 15,000/18,000 | Loss: 0.00802 | Coverage: 100.0%\n","‚è∞ Time remaining: 11.8 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_15000.pth\n","  üåç 16,000/18,000 | Loss: 0.04204 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_16000.pth\n","  üåç 17,000/18,000 | Loss: 0.00159 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_17000.pth\n","‚úÖ Stage efficient_scaling complete!\n","\n","üéØ STAGE 5/5: TARGET_HD\n","üéØ TARGET: 640x480\n","\n","üöÄ STAGE: TARGET_HD\n","==================================================\n","üìê Resolution: 640x480\n","üìπ Video writer ready: 640x480\n","üß† Latent Shape: (384, 8, 8)\n","üìä Training Steps: 20,000\n","üß† Fast SGM: 384x8x8 = 24,576 elements\n","üìÇ Loaded checkpoint: /content/genieworld_checkpoints/checkpoint_efficient_scaling_step_17000.pth\n","   Stage: efficient_scaling, Step: 17000\n","\n","üé® TOKENIZER PHASE: 6,666 steps\n","  üé® 0/6,666 | Loss: 0.16449\n","‚è∞ Time remaining: 11.8 hours\n","  üé® 500/6,666 | Loss: 0.00332\n","  üé® 1,000/6,666 | Loss: 0.00239\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_1000.pth\n","  üé® 1,500/6,666 | Loss: 0.00226\n","  üé® 2,000/6,666 | Loss: 0.00204\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_2000.pth\n","  üé® 2,500/6,666 | Loss: 0.00233\n","  üé® 3,000/6,666 | Loss: 0.00219\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_3000.pth\n","  üé® 3,500/6,666 | Loss: 0.00390\n","  üé® 4,000/6,666 | Loss: 0.00364\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_4000.pth\n","  üé® 4,500/6,666 | Loss: 0.00345\n","  üé® 5,000/6,666 | Loss: 0.00373\n","‚è∞ Time remaining: 11.8 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_5000.pth\n","  üé® 5,500/6,666 | Loss: 0.00408\n","  üé® 6,000/6,666 | Loss: 0.00298\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_6000.pth\n","  üé® 6,500/6,666 | Loss: 0.00301\n","\n","üåç DYNAMICS PHASE: 20,000 steps total\n","  üåç 7,000/20,000 | Loss: 0.01147 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_7000.pth\n","  üåç 8,000/20,000 | Loss: 0.01041 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_8000.pth\n","  üåç 9,000/20,000 | Loss: 0.00737 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_9000.pth\n","  üåç 10,000/20,000 | Loss: 0.00988 | Coverage: 100.0%\n","‚è∞ Time remaining: 11.8 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_10000.pth\n","  üåç 11,000/20,000 | Loss: 0.00027 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_11000.pth\n","  üåç 12,000/20,000 | Loss: 0.00569 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_12000.pth\n","  üåç 13,000/20,000 | Loss: 0.01178 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_13000.pth\n","  üåç 14,000/20,000 | Loss: 0.00504 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_14000.pth\n","  üåç 15,000/20,000 | Loss: 0.00846 | Coverage: 100.0%\n","‚è∞ Time remaining: 11.7 hours\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_15000.pth\n","  üåç 16,000/20,000 | Loss: 0.01467 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_16000.pth\n","  üåç 17,000/20,000 | Loss: 0.00759 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_17000.pth\n","  üåç 18,000/20,000 | Loss: 0.00399 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_18000.pth\n","  üåç 19,000/20,000 | Loss: 0.00823 | Coverage: 100.0%\n","üíæ Checkpoint saved: /content/genieworld_checkpoints/checkpoint_target_hd_step_19000.pth\n","‚úÖ Stage target_hd complete!\n","üé¨ Video finalized: /content/genieworld_colab_training.mp4 (1,704 frames)\n","\n","üéâ COLAB TRAINING SESSION COMPLETE! üéâ\n","üìπ Video: /content/genieworld_colab_training.mp4\n","üé¨ Frames: 1,704\n","üíæ Checkpoints saved for resuming\n","\n","üèÜ TRAINING COMPLETE! üèÜ\n","üé¨ Check your video output!\n","üíæ Checkpoints saved - can resume in new Colab session!\n","üéØ 640x480 target achieved!\n","\n","üéØ Colab-Optimized Training Ready!\n","üí° Perfect for Google Colab's 12-hour limit!\n","üîÑ Automatically saves and resumes progress!\n"]}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}