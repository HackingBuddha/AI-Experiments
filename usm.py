# -*- coding: utf-8 -*-
"""USM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5h3CLeqYgqlmUszKtahGR7i18_X-Dea
"""

# @title
# -*- coding: utf-8 -*-
"""USM_v12_7_FULL_ARC_AGI_2_Safe_TTT.ipynb

Automatically generated by Colab.

USM v12.7: FULL ARC-AGI 2 (Adaptive TTT Sampling)
-------------------------------------------------
Fixes OOM by capping N_Support per inference step.
Instead of processing ALL support examples in one forward pass,
we sample a mini-batch of supports (e.g. 3) per optimization step.

Architecture (Preserved):
-   **CTM Core**: Spatial Power Attention + Conditioned ConvLTC.
-   **Discrete LoT**: VQ-VAE codebook.
-   **Evolution**: Active Inference + Genetic Search.
"""

import os
import json
import random
import math
import time
import copy
import gc
import numpy as np
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Any, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Running USM v12.7 (Safe TTT) on: {device}")

# =============================================================================
# 1. CONFIGURATION
# =============================================================================

@dataclass
class USMConfig:
    seed: int = 42
    arc_repo_url: str = "https://github.com/fchollet/ARC-AGI/archive/refs/heads/master.zip"

    # --- Constraints ---
    max_grid_size: int = 30   # Official max size
    pad_val: int = 10

    # --- Meta-Training ---
    meta_epochs: int = 100
    meta_lr: float = 3e-4
    tasks_per_epoch: int = 50

    # --- VQ ---
    vocab_size: int = 512
    commitment_cost: float = 0.25

    # --- Inference ---
    pop_size: int = 128
    inference_steps: int = 50
    inference_lr: float = 0.1
    mutation_scale: float = 0.5

    # Batching Safety
    inference_batch_size: int = 4 # Pop chunk size
    max_support_per_step: int = 3 # Limit support examples per step to avoid OOM
    use_mixed_precision: bool = True # Enable AMP during inference to reduce memory

    # --- Memory ---
    memory_capacity: int = 2000
    retrieval_k: int = 8

    # --- Architecture ---
    dim: int = 128
    z_dim: int = 128
    n_heads: int = 4
    ctm_ticks: int = 4

    DEV_ONLY_DEBUG_RUN: bool = False

cfg = USMConfig()

def set_seed(s):
    random.seed(s)
    np.random.seed(s)
    torch.manual_seed(s)
    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)

set_seed(cfg.seed)

# =============================================================================
# 2. DATA PIPELINE
# =============================================================================

def download_arc():
    if not os.path.exists("ARC-AGI-master"):
        print("Downloading ARC-AGI...")
        import requests, zipfile, io
        r = requests.get(cfg.arc_repo_url)
        z = zipfile.ZipFile(io.BytesIO(r.content))
        z.extractall()

class ARCTask:
    def __init__(self, task_id, data):
        self.task_id = task_id
        self.train = data['train']
        self.test = data['test']

    def get_train_batch(self, device):
        sx, sy = [], []
        for p in self.train:
            sx.append(self._proc(p['input']))
            sy.append(self._proc(p['output']))
        return torch.stack(sx).to(device), torch.stack(sy).to(device)

    def get_test_batch(self, device):
        qx, qy = [], []
        for p in self.test:
            qx.append(self._proc(p['input']))
            if 'output' in p:
                qy.append(self._proc(p['output']))
            else:
                qy.append(torch.zeros_like(self._proc(p['input'])))
        return torch.stack(qx).to(device), torch.stack(qy).to(device)

    def _proc(self, grid):
        t = torch.full((cfg.max_grid_size, cfg.max_grid_size), cfg.pad_val, dtype=torch.long)
        h = len(grid)
        w = len(grid[0])
        h = min(h, cfg.max_grid_size)
        w = min(w, cfg.max_grid_size)
        for r in range(h):
            for c in range(w):
                t[r, c] = grid[r][c]
        return t

class ARCDataset(Dataset):
    def __init__(self, split="training", limit=None):
        self.tasks = []
        path = f"ARC-AGI-master/data/{split}"
        if not os.path.exists(path):
            print(f"Path not found: {path}")
            return
        files = sorted(os.listdir(path))
        if limit: files = files[:limit]
        for f in files:
            if not f.endswith(".json"): continue
            with open(os.path.join(path, f)) as fp: data = json.load(fp)
            self.tasks.append(ARCTask(f, data))
        print(f"[{split}] Loaded {len(self.tasks)} tasks.")

    def __len__(self): return len(self.tasks)
    def __getitem__(self, idx): return self.tasks[idx]

# =============================================================================
# 3. VECTOR QUANTIZER
# =============================================================================

class VectorQuantizer(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, commitment_cost):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.commitment_cost = commitment_cost
        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)
        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)

    def forward(self, inputs):
        flat_input = inputs.view(-1, self.embedding_dim)
        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)
                    + torch.sum(self.embedding.weight**2, dim=1)
                    - 2 * torch.matmul(flat_input, self.embedding.weight.t()))
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)
        quantized = torch.matmul(encodings, self.embedding.weight).view(inputs.shape)
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        q_latent_loss = F.mse_loss(quantized, inputs.detach())
        loss = q_latent_loss * self.commitment_cost + e_latent_loss
        quantized = inputs + (quantized - inputs).detach()
        return loss, quantized

# =============================================================================
# 4. CTM CORE
# =============================================================================

class ConditionedConvLTC(nn.Module):
    def __init__(self, dim, z_dim):
        super().__init__()
        self.conv_f = nn.Sequential(
            nn.Conv2d(dim, dim, 3, padding=1, bias=False),
            nn.GroupNorm(8, dim), nn.SiLU(),
            nn.Conv2d(dim, dim, 3, padding=1, bias=False)
        )
        self.conv_g = nn.Sequential(nn.Conv2d(dim, dim, 3, padding=1), nn.Sigmoid())
        self.film = nn.Linear(z_dim, dim * 2)

    def forward(self, x, z_task):
        style = self.film(z_task).unsqueeze(-1).unsqueeze(-1)
        gamma, beta = style.chunk(2, dim=1)
        x_mod = x * (1 + gamma) + beta
        target = self.conv_f(x_mod)
        gate = self.conv_g(x_mod)
        return (1 - gate) * x + gate * target

class SpatialAttention(nn.Module):
    def __init__(self, dim, n_heads=4):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        self.q = nn.Conv2d(dim, dim, 1); self.k = nn.Conv2d(dim, dim, 1); self.v = nn.Conv2d(dim, dim, 1); self.o = nn.Conv2d(dim, dim, 1)
        self.norm = nn.GroupNorm(8, dim)

    def forward(self, x):
        B, C, H, W = x.shape
        x_norm = self.norm(x)
        q = self.q(x_norm).view(B, self.n_heads, self.head_dim, -1).transpose(-1, -2)
        k = self.k(x_norm).view(B, self.n_heads, self.head_dim, -1)
        v = self.v(x_norm).view(B, self.n_heads, self.head_dim, -1).transpose(-1, -2)
        scores = (q @ k) / math.sqrt(self.head_dim)
        attn = torch.softmax(scores, dim=-1)
        out = (attn @ v).transpose(-1, -2).reshape(B, C, H, W)
        return self.o(out)

class UniversalCTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.emb = nn.Embedding(12, cfg.dim)
        self.project = nn.Conv2d(cfg.dim, cfg.dim, 1)
        self.attn = SpatialAttention(cfg.dim, cfg.n_heads)
        self.ltc = ConditionedConvLTC(cfg.dim, cfg.z_dim)
        self.time_emb = nn.Embedding(16, cfg.dim)
        self.head = nn.Conv2d(cfg.dim, 11, 1)
        self.vq = VectorQuantizer(cfg.vocab_size, cfg.z_dim, cfg.commitment_cost)
        self.retrieval_pool = nn.AdaptiveAvgPool2d(1)

    def get_task_key(self, x):
        z = self.emb(x).permute(0, 3, 1, 2)
        z = self.project(z)
        key = self.retrieval_pool(z).flatten(1)
        return F.normalize(key, dim=-1)

    def forward(self, x, z_task, ticks=None):
        if ticks is None: ticks = cfg.ctm_ticks
        vq_loss, z_q = self.vq(z_task)
        z = self.emb(x).permute(0, 3, 1, 2)
        z = self.project(z)
        for t in range(ticks):
            t_emb = self.time_emb(torch.tensor(t, device=x.device)).view(1, -1, 1, 1)
            z = z + t_emb
            z = z + self.attn(z)
            z = self.ltc(z, z_q)
        return self.head(z), vq_loss

# =============================================================================
# 5. ETERNAL HYPERGRAPH
# =============================================================================

class EternalHypergraph:
    def __init__(self, dim):
        self.keys = []
        self.values = []
        self.dim = dim

    def add(self, key, value):
        if len(self.keys) < cfg.memory_capacity:
            self.keys.append(key.detach().cpu())
            self.values.append(value.detach().cpu())
        else:
            idx = random.randint(0, cfg.memory_capacity-1)
            self.keys[idx] = key.detach().cpu()
            self.values[idx] = value.detach().cpu()

    def retrieve(self, query_key, k=4):
        if not self.keys: return None
        keys = torch.cat(self.keys, dim=0).to(query_key.device)
        values = torch.stack(self.values).to(query_key.device)
        sim = (keys @ query_key.T).squeeze(1)
        k = min(k, len(keys))
        _, indices = torch.topk(sim, k)
        return values[indices]

# =============================================================================
# 6. DARWIN-GODEL CONTROLLER (Safe TTT)
# =============================================================================

class DarwinGodelController:
    def __init__(self, model, hypergraph):
        self.model = model
        self.hypergraph = hypergraph

    def solve(self, task: ARCTask):
        self.model.eval()
        sup_x_all, sup_y_all = task.get_train_batch(device)
        N_sup_total = sup_x_all.shape[0]

        use_amp = cfg.use_mixed_precision and device.type == "cuda"

        # 1. Init Population
        z_pop = torch.randn(cfg.pop_size, cfg.z_dim, device=device) * 0.1
        task_key = self.model.get_task_key(sup_x_all[0].unsqueeze(0))
        mem_z = self.hypergraph.retrieve(task_key, k=cfg.retrieval_k)
        if mem_z is not None:
            k = mem_z.shape[0]
            z_pop[:k] = mem_z
            for i in range(k):
                if k+i < cfg.pop_size:
                    z_pop[k+i] = mem_z[i] + torch.randn_like(mem_z[i]) * 0.05

        z_pop.requires_grad_(True)
        opt = optim.Adam([z_pop], lr=cfg.inference_lr)

        best_loss = float('inf')
        best_z = None

        # 2. Evolution Loop
        for gen in range(cfg.inference_steps):
            opt.zero_grad()
            fitness_chunks = []

            # --- Sample Support Subset for TTT Step ---
            # To save memory, we don't process ALL support examples in every gradient step if N is huge.
            if N_sup_total > cfg.max_support_per_step:
                indices = torch.randperm(N_sup_total)[:cfg.max_support_per_step]
                sup_x = sup_x_all[indices]
                sup_y = sup_y_all[indices]
            else:
                sup_x, sup_y = sup_x_all, sup_y_all

            N_sup_curr = sup_x.shape[0]

            # --- Memory Safe Population Batching ---
            for i in range(0, cfg.pop_size, cfg.inference_batch_size):
                end = min(i + cfg.inference_batch_size, cfg.pop_size)
                pop_batch = z_pop[i:end]
                curr_bs = end - i

                # Expand
                sup_x_exp = sup_x.repeat(curr_bs, 1, 1)
                sup_y_exp = sup_y.repeat(curr_bs, 1, 1)
                z_batch = pop_batch.repeat_interleave(N_sup_curr, dim=0)

                with autocast(device_type=device.type, dtype=torch.float16, enabled=use_amp):
                    logits, vq_loss = self.model(sup_x_exp, z_batch)

                    raw_loss = F.cross_entropy(logits, sup_y_exp, reduction='none')
                    loss_per_hyp = raw_loss.view(curr_bs, N_sup_curr, -1).mean(dim=2).sum(dim=1)
                    fitness = loss_per_hyp + vq_loss

                fitness_chunks.append(fitness)

                del logits, raw_loss, sup_x_exp, sup_y_exp, z_batch

            total_fitness = torch.cat(fitness_chunks, dim=0)
            loss = total_fitness.mean()
            loss.backward()
            opt.step()

            if device.type == "cuda" and (gen + 1) % 5 == 0:
                torch.cuda.empty_cache()

            # Genetic Step
            if gen % 5 == 0 and gen < cfg.inference_steps - 1:
                with torch.no_grad():
                    fitness = total_fitness.detach()
                    sorted_idx = torch.argsort(fitness)
                    elites = z_pop[sorted_idx[:cfg.pop_size//4]].clone()
                    new_pop = [elites]
                    n_mutants = cfg.pop_size - len(elites)
                    src_idxs = torch.randint(0, len(elites), (n_mutants,))
                    mutants = elites[src_idxs] + torch.randn_like(elites[src_idxs]) * cfg.mutation_scale
                    new_pop.append(mutants)
                    z_pop.data = torch.cat(new_pop, dim=0)

            # Tracking
            current_best_val = total_fitness.min().item()
            if current_best_val < best_loss:
                best_loss = current_best_val
                best_idx = total_fitness.argmin().item()
                best_z = z_pop[best_idx].detach().clone()

                # Check Perfect (Approx)
                if best_loss < 0.001:
                    break

        if best_loss < 0.01:
            self.hypergraph.add(task_key, best_z)

        # 4. Predict
        q_x, q_y = task.get_test_batch(device)
        N_test = q_x.shape[0]
        with torch.no_grad():
            if best_z is None:
                best_z = z_pop.detach()[0].clone()
            z_test = best_z.unsqueeze(0).expand(N_test, -1)
            q_logits, _ = self.model(q_x, z_test)
            q_pred = q_logits.argmax(1)

        return q_pred, q_y, best_loss

# =============================================================================
# 7. RUNNER
# =============================================================================

def ascii_preview(pred, tgt):
    p = pred.cpu().numpy()
    t = tgt.cpu().numpy()
    rows, cols = t.shape
    print("Pred:")
    for r in range(rows): print(" ".join([str(x) if x < 10 else '.' for x in p[r]]))
    print("Tgt:")
    for r in range(rows): print(" ".join([str(x) if x < 10 else '.' for x in t[r]]))

def run_usm_full():
    download_arc()
    ctm = UniversalCTM().to(device)
    mem = EternalHypergraph(cfg.dim)
    dgm = DarwinGodelController(ctm, mem)

    print(f"Model Params: {sum(p.numel() for p in ctm.parameters()):,}")

    # 1. Meta-Training
    print("\n>>> Phase 1: Meta-Training (Full Training Split)...")
    limit = cfg.train_limit if cfg.DEV_ONLY_DEBUG_RUN else None
    ds = ARCDataset("training", limit=limit)
    opt = optim.AdamW(ctm.parameters(), lr=cfg.meta_lr)

    tasks = ds.tasks
    for ep in range(cfg.meta_epochs):
        ctm.train()
        batch = random.sample(tasks, min(len(tasks), cfg.tasks_per_epoch))
        ep_loss = 0

        for task in batch:
            sup_x, sup_y = task.get_train_batch(device)
            if sup_x.shape[0] > 1: sx, sy = sup_x[:1], sup_y[:1]
            else: sx, sy = sup_x, sup_y

            z = torch.randn(1, cfg.z_dim, device=device, requires_grad=True)
            z_opt = optim.SGD([z], lr=0.1)
            for _ in range(5):
                out, vq_l = ctm(sx, z)
                l = F.cross_entropy(out, sy) + vq_l
                z_opt.zero_grad(); l.backward(); z_opt.step()

            z_final = z.detach()
            if ep > cfg.meta_epochs // 2:
                with torch.no_grad():
                    key = ctm.get_task_key(sx)
                    mem.add(key, z_final.squeeze(0))

            q_out, vq_l = ctm(sx, z_final)
            loss = F.cross_entropy(q_out, sy) + vq_l
            opt.zero_grad(); loss.backward(); opt.step()
            ep_loss += loss.item()

        if (ep+1) % 10 == 0:
            print(f"Ep {ep+1} | Loss: {ep_loss/len(batch):.4f} | Mem Size: {len(mem.keys)}")

    # 2. Evaluation
    print("\n>>> Phase 2: Full Benchmark Evaluation (Evaluation Split)...")
    eval_limit = cfg.train_limit if cfg.DEV_ONLY_DEBUG_RUN else None
    eval_ds = ARCDataset("evaluation", limit=eval_limit)

    solved_count = 0
    total_count = 0

    for i, task in enumerate(eval_ds.tasks):
        torch.cuda.empty_cache()
        gc.collect()

        pred_y, true_y, loss = dgm.solve(task)

        match_per_ex = (pred_y == true_y).view(pred_y.size(0), -1).all(dim=1)
        task_solved = match_per_ex.all().item()

        status = "SOLVED" if task_solved else "Fail"
        print(f"Task {task.task_id} ({i+1}/{len(eval_ds)}) | Fit: {loss:.4f} | {status}")

        if task_solved: solved_count += 1
        total_count += 1

        if i < 3: ascii_preview(pred_y[0], true_y[0])

    print("\n" + "="*40)
    print("USM v12.7 FULL ARC-AGI 2 SUMMARY")
    print(f"Model Params: {sum(p.numel() for p in ctm.parameters()):,}")
    print(f"Codebook Size: {cfg.vocab_size} | Dim: {cfg.z_dim}")
    print(f"Training tasks used: {len(ds)} (sampled)")
    print(f"Evaluation tasks used: {total_count}")
    print("")
    print("Eval Results (strict 'all test pairs correct'):")
    print(f"  Solved: {solved_count} / {total_count}")
    print(f"  Solved%: {solved_count/total_count:.1%}")
    print("========================================")

if __name__ == "__main__":
    run_usm_full()